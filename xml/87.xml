<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:58:24Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|86001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08106</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08106</id><created>2015-10-27</created><updated>2016-01-15</updated><authors><author><keyname>Marshak</keyname><forenames>Charles Z.</forenames></author><author><keyname>Rombach</keyname><forenames>M. Puck</forenames></author><author><keyname>Bertozzi</keyname><forenames>Andrea L.</forenames></author><author><keyname>D'Orsogna</keyname><forenames>Maria R.</forenames></author></authors><title>Growth and Containment of a Hierarchical Criminal Network</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages, 11 Figures; New title; Updated figures with color scheme
  better suited for colorblind readers and for gray scale printing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model the hierarchical evolution of an organized criminal network via
antagonistic recruitment and pursuit processes. Within the recruitment phase, a
criminal kingpin enlists new members into the network, who in turn seek out
other affiliates. New recruits are linked to established criminals according to
a probability distribution that depends on the current network structure. At
the same time, law enforcement agents attempt to dismantle the growing
organization using pursuit strategies that initiate on the lower level nodes
and that unfold as self-avoiding random walks. The global details of the
organization are unknown to law enforcement, who must explore the hierarchy
node by node. We halt the pursuit when certain local criteria of the network
are uncovered, encoding if and when an arrest is made; the criminal network is
assumed to be eradicated if the kingpin is arrested. We first analyze
recruitment and study the large scale properties of the growing network; later
we add pursuit and use numerical simulations to study the eradication
probability in the case of three pursuit strategies, the time to first
eradication and related costs. Within the context of this model, we find that
eradication becomes increasingly costly as the network increases in size and
that the optimal way of arresting the kingpin is to intervene at the early
stages of network formation. We discuss our results in the context of dark
network disruption and their implications on possible law enforcement
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08108</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08108</id><created>2015-10-27</created><authors><author><keyname>Wu</keyname><forenames>Yifan</forenames></author><author><keyname>Gy&#xf6;rgy</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Szepesv&#xe1;ri</keyname><forenames>Csaba</forenames></author></authors><title>Online Learning with Gaussian Payoffs and Side Observations</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a sequential learning problem with Gaussian payoffs and side
information: after selecting an action $i$, the learner receives information
about the payoff of every action $j$ in the form of Gaussian observations whose
mean is the same as the mean payoff, but the variance depends on the pair
$(i,j)$ (and may be infinite). The setup allows a more refined information
transfer from one action to another than previous partial monitoring setups,
including the recently introduced graph-structured feedback case. For the first
time in the literature, we provide non-asymptotic problem-dependent lower
bounds on the regret of any algorithm, which recover existing asymptotic
problem-dependent lower bounds and finite-time minimax lower bounds available
in the literature. We also provide algorithms that achieve the
problem-dependent lower bound (up to some universal constant factor) or the
minimax lower bounds (up to logarithmic factors).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08114</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08114</id><created>2015-10-27</created><authors><author><keyname>Frid</keyname><forenames>Anna E.</forenames></author></authors><title>Words containing all permutations of a family of factors</title><categories>math.CO cs.FL</categories><comments>Short 3-page note generalizing a lemma by de Luca and Zamboni</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that if a uniformly recurrent infinite word contains as a factor any
finite permutation of words from an infinite family, then either this word is
periodic, or its complexity (that is, the number of factors) grows faster than
linearly. This result generalizes one of the lemmas of a recent paper by de
Luca and Zamboni, where it was proved that such an infinite word cannot be
Sturmian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08119</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08119</id><created>2015-10-27</created><authors><author><keyname>Gjoka</keyname><forenames>Minas</forenames></author><author><keyname>Smith</keyname><forenames>Emily</forenames></author><author><keyname>Butts</keyname><forenames>Carter T.</forenames></author></authors><title>Estimating Subgraph Frequencies with or without Attributes from
  Egocentrically Sampled Data</title><categories>cs.SI</categories><comments>This article generalizes the methods we introduced in arxiv:1308.3297
  to count any directed or undirected subgraph that is containable within an
  egonet</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how to efficiently produce unbiased estimates of
subgraph frequencies from a probability sample of egocentric networks (i.e.,
focal nodes, their neighbors, and the induced subgraphs of ties among their
neighbors). A key feature of our proposed method that differentiates it from
prior methods is the use of egocentric data. Because of this, our method is
suitable for estimation in large unknown graphs, is easily parallelizable,
handles privacy sensitive network data (e.g. egonets with no neighbor labels),
and supports counting of large subgraphs (e.g. maximal clique of size 205 in
Section 6) by building on top of existing exact subgraph counting algorithms
that may not support sampling. It gracefully handles a variety of sampling
designs such as uniform or weighted independence or random walk sampling. Our
method can be used for subgraphs that are: (i) undirected or directed; (ii)
induced or non-induced; (iii) maximal or non-maximal; and (iv) potentially
annotated with attributes. We compare our estimators on a variety of real-world
graphs and sampling methods and provide suggestions for their use. Simulation
shows that our method outperforms the state-of-the-art approach for relative
subgraph frequencies by up to an order of magnitude for the same sample size.
Finally, we apply our methodology to a rare sample of Facebook users across the
social graph to estimate and interpret the clique size distribution and gender
composition of cliques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08121</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08121</id><created>2015-10-27</created><authors><author><keyname>Frankle</keyname><forenames>Jonathan</forenames></author></authors><title>Type-Directed Synthesis of Products</title><categories>cs.PL</categories><comments>Master's Thesis at Princeton University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software synthesis - the process of generating complete, general-purpose
programs from specifications - has become a hot research topic in the past few
years. For decades the problem was thought to be insurmountable: the search
space of possible programs is far too massive to efficiently traverse. Advances
in efficient constraint solving have overcome this barrier, enabling a new
generation of effective synthesis systems. Most existing systems compile
synthesis tasks down to low-level SMT instances, sacrificing high-level
semantic information while solving only first-order problems (i.e., filling
integer holes). Recent work takes an alternative approach, using the
Curry-Howard isomorphism and techniques from automated theorem proving to
construct higher-order programs with algebraic datatypes.
  My thesis involved extending this type-directed synthesis engine to handle
product types, which required significant modifications to both the underlying
theory and the tool itself. Product types streamline other language features,
eliminating variable-arity constructors among other workarounds employed in the
original synthesis system. A form of logical conjunction, products are
invertible, making it possible to equip the synthesis system with an efficient
theorem-proving technique called focusing that eliminates many of the
nondeterministic choices inherent in proof search. These theoretical
enhancements informed a new version of the type-directed synthesis prototype
implementation, which remained performance-competitive with the original
synthesizer. A significant advantage of the type-directed synthesis framework
is its extensibility; this thesis is a roadmap for future such efforts to
increase the expressive power of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08142</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08142</id><created>2015-10-27</created><authors><author><keyname>Lu</keyname><forenames>Xin-Yi</forenames></author><author><keyname>Lin</keyname><forenames>Jian-Hong</forenames></author><author><keyname>Guo</keyname><forenames>Qiang</forenames></author><author><keyname>Liu</keyname><forenames>Jian-Guo</forenames></author></authors><title>Empirical Analysis of the Online Rating Systems</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is to analyze the properties of evolving bipartite networks from
four aspects, the growth of networks, the degree distribution, the popularity
of objects and the diversity of user behaviours, leading a deep understanding
on the empirical data. By empirical studies of data from the online bookstore
Amazon and a question and answer site Stack Overflow, which are both rating
bipartite networks, we could reveal the rules for the evolution of bipartite
networks. These rules have significant meanings in practice for maintaining the
operation of real systems and preparing for their future development. We find
that the degree distribution of users follows a power law with an exponential
cutoff. Also, according to the evolution of popularity for objects, we find
that the large-degree objects tend to receive more new ratings than expected
depending on their current degrees while the small-degree objects receive less
ratings in terms of their degrees. Moreover, the user behaviours show such a
trend that the larger degree the users have, the stronger purposes are with
their behaviours except the initial periods when users choose a diversity of
products to learn about what they want. Finally, we conclude with a discussion
on how the bipartite network evolves, which provides guideline for meeting
challenges brought by the growth of network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08149</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08149</id><created>2015-10-27</created><authors><author><keyname>Guillot</keyname><forenames>Pierre</forenames></author></authors><title>Cayley graphs and automatic sequences</title><categories>math.CO cs.FL math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study those automatic sequences which are produced by an automaton whose
underlying graph is the Cayley graph of a finite group. For $2$-automatic
sequences, we find a characterization in terms of what we call homogeneity, and
among homogeneous sequences, we single out those enjoying what we call
self-similarity. It turns out that self-similar $2$-automatic sequences (viewed
up to a permutation of their alphabet) are in bijection with many interesting
objects, for example dessins d'enfants (covers of the Riemann sphere with three
points removed).
  For any $p$ we show that, in the case of an automatic sequence produced &quot;by a
Cayley graph&quot;, the group and indeed the automaton can be recovered canonically
from the sequence.
  Further, we show that a rational fraction may be associated to any automatic
sequence. To compute this fraction explicitly, knowledge of a certain graph is
required. We prove that for the sequences studied in the first part, the graph
is simply the Cayley graph that we start from, and so calculations are
possible.
  We give applications to the study of the frequencies of letters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08154</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08154</id><created>2015-10-27</created><authors><author><keyname>Agrawal</keyname><forenames>Akanksha</forenames></author><author><keyname>Kolay</keyname><forenames>Sudeshna</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>A faster FPT Algorithm and a smaller Kernel for Block Graph Vertex
  Deletion</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ is called a \emph{block graph} if each maximal $2$-connected
component of $G$ is a clique. In this paper we study the Block Graph Vertex
Deletion from the perspective of fixed parameter tractable (FPT) and
kernelization algorithm. In particular, an input to Block Graph Vertex Deletion
consists of a graph $G$ and a positive integer $k$ and the objective to check
whether there exists a subset $S \subseteq V(G)$ of size at most $k$ such that
the graph induced on $V(G)\setminus S$ is a block graph. In this paper we give
an FPT algorithm with running time $4^kn^{O(1)}$ and a polynomial kernel of
size $O(k^4)$ for Block Graph Vertex Deletion. The running time of our FPT
algorithm improves over the previous best algorithm for the problem that ran in
time $10^kn^{O(1)}$ and the size of our kernel reduces over the previously
known kernel of size $O(k^9)$. Our results are based on a novel connection
between Block Graph Vertex Deletion and the classical {\sc Feedback Vertex Set}
problem in graphs without induced $C_4$ and $K_4-e$. To achieve our results we
also obtain an algorithm for {\sc Weighted Feedback Vertex Set} running in time
$3.618^kn^{O(1)}$ and improving over the running time of previously known
algorithm with running time $5^kn^{O(1)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08160</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08160</id><created>2015-10-27</created><updated>2015-11-09</updated><authors><author><keyname>Li</keyname><forenames>Jianan</forenames></author><author><keyname>Liang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Shen</keyname><forenames>ShengMei</forenames></author><author><keyname>Xu</keyname><forenames>Tingfa</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Scale-aware Fast R-CNN for Pedestrian Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intuitively, instances of the same object category with different spatial
scales may exhibit dramatically different features. Thus, large variance in
instance scales, which results in undesirable large intra-category variance in
features, may severely hurt the performance of modern object instance detection
methods. We argue that this issue can be substantially alleviated by the
divide-and-conquer philosophy. Taking pedestrian detection as an example, we
illustrate how we can leverage this philosophy to develop a Scale-Aware Fast
R-CNN (SAF R-CNN) framework. The model introduces multiple built-in
sub-networks which detect pedestrians with scales from disjoint ranges. Outputs
from all the sub-networks are then adaptively combined to generate the final
detection results that are shown to be robust to large variance in instance
scales, via a gate function defined over the sizes of object proposals.
Extensive evaluations on the challenging Caltech pedestrian detection
dataset~\cite{dollar2012pedestrian} well demonstrate the superiority of the
proposed SAF R-CNN over the state-of-the-arts. Particularly, the miss rate is
reduced to $9.68\%$, which is significantly smaller than $11.75\%$ by
CompACT-Deep~\cite{compact}, $20.86\%$ by TA-CNN~\cite{ta_cnn} and $12.86\%$ by
the vanilla Fast R-CNN model~\cite{girshick2015fast}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08172</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08172</id><created>2015-10-27</created><authors><author><keyname>Lam</keyname><forenames>Emily</forenames></author><author><keyname>Wilson</keyname><forenames>Sarah Kate</forenames></author><author><keyname>Elgala</keyname><forenames>Hany</forenames></author><author><keyname>Little</keyname><forenames>Thomas D. C.</forenames></author></authors><title>Spectrally and Energy Efficient OFDM (SEE-OFDM) for Intensity Modulated
  Optical Wireless Systems</title><categories>cs.IT math.IT</categories><comments>26 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrally and energy efficient orthogonal frequency division multiplexing
(SEE-OFDM) is an optical OFDM technique based on combining multiple
asymmetrically clipped optical OFDM (ACO-OFDM) signals into one OFDM signal. By
summing different components together, SEE-OFDM can achieve the same spectral
efficiency as DC-biased optical OFDM (DCO-OFDM) without an energy-inefficient
DC-bias. This paper introduces multiple methods for decoding a SEE-OFDM symbol
and shows that an iterative decoder with hard decisions gives the best
performance. Being a multi-component format, different energy allocation
amongst the different components of SEE-OFDM is possible. However, equal energy
allocation performs 1.5 dB better than unequal energy allocation. A
hard-decision, iterative subtraction receiver can further increase performance
by another 1.5 dB over soft-decision subtraction and reconstruction receivers.
SEE-OFDM consistently performs 3 dB or better and with higher spectral
efficiency than ACO-OFDM at the same bit-error-rate (BER). Comparing other
combination methods at the same BER, SEE-OFDM performs up to 3 dB better than
hybrid asymmetrically clipped optical (OFDM) (HACO-OFDM) and up to 1.5 dB
better than asymmetrically and symmetrically clipped optical OFDM (ASCO-OFDM)
and enhanced unipolar OFDM (eU-OFDM) when using hard decisions at the receiver.
Additionally, SEE-OFDM has the best peak-to-average-power rate (PAPR) as
compared to the other combination OFDM formats and ACO-OFDM, which makes it
excellent for any range limited optical source, such as laser diodes and
light-emitting diodes (LEDs). In summary, SEE-OFDM is shown to have excellent
properties to glean additional capacity from an intensity modulation and direct
detection (IM/DD) optical wireless communications system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08174</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08174</id><created>2015-10-27</created><authors><author><keyname>Mandal</keyname><forenames>Subhamoy</forenames></author><author><keyname>De&#xe1;n-Ben</keyname><forenames>Xos&#xe9; Lu&#xed;s</forenames></author><author><keyname>Razansky</keyname><forenames>Daniel</forenames></author></authors><title>Visual Quality Enhancement in Multispectral Optoacoustic Tomography
  using Active Contour Segmentation Priors</title><categories>physics.med-ph cs.CV physics.optics</categories><comments>Manuscript under review</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Segmentation of biomedical images is essential for studying and
characterizing anatomical structures, detection and evaluation of pathological
tissues. Segmentation has been further shown to enhance the reconstruction
performance in many tomographic imaging modalities by accounting for
heterogeneities of the excitation field and tissue properties in the imaged
region. This is particularly relevant in optoacoustic tomography, where
discontinuities in the optical and acoustic tissue properties, if not properly
accounted for, may result in deterioration of the imaging performance.
Efficient segmentation of optoacoustic images is often hampered by the
relatively low intrinsic contrast of large anatomical structures, which is
further impaired by the limited angular coverage of some commonly employed
tomographic imaging configurations. Herein, we analyze the performance of
active contour models for boundary segmentation in cross-sectional optoacoustic
tomography. The segmented mask is employed to construct a two compartment model
for the acoustic and optical parameters of the imaged tissues, which is
subsequently used to improve accuracy of the image reconstruction routines. The
performance of the suggested segmentation and modeling approach are showcased
in tissue-mimicking phantoms and small animal imaging experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08183</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08183</id><created>2015-10-27</created><authors><author><keyname>Shirvanimoghaddam</keyname><forenames>Mahyar</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>Raptor Codes in the Low SNR Regime</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Communications. arXiv admin
  note: text overlap with arXiv:1510.07728</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the design of Raptor codes for binary input
additive white Gaussian noise (BIAWGN) channels, where we are interested in
very low signal to noise ratios (SNRs). A linear programming degree
distribution optimization problem is defined for Raptor codes in the low SNR
regime through several approximations. We also provide an exact expression for
the polynomial representation of the degree distribution with infinite maximum
degree in the low SNR regime, which enables us to calculate the exact value of
the fractions of output nodes of small degrees. A more practical degree
distribution design is also proposed for Raptor codes in the low SNR regime,
where we include the rate efficiency and the decoding complexity in the
optimization problem, and an upper bound on the maximum rate efficiency is
derived for given design parameters. Simulation results show that the Raptor
code with the designed degree distributions can approach rate efficiencies
larger than 0.95 in the low SNR regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08186</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08186</id><created>2015-10-28</created><authors><author><keyname>Riechers</keyname><forenames>Paul M.</forenames></author><author><keyname>Mahoney</keyname><forenames>John R.</forenames></author><author><keyname>Aghamohammadi</keyname><forenames>Cina</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>A Closed-Form Shave from Occam's Quantum Razor: Exact Results for
  Quantum Compression</title><categories>quant-ph cond-mat.stat-mech cs.IT math.DS math.IT</categories><comments>21 pages, 13 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/eqc.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The causal structure of a stochastic process can be more efficiently
transmitted via a quantum channel than a classical one, an advantage that
increases with codeword length. While previously difficult to compute, we
express the quantum advantage in closed form using spectral decomposition,
leading to direct computation of the quantum communication cost at all encoding
lengths, including infinite. This makes clear how finite-codeword compression
is controlled by the classical process' cryptic order and allows us to analyze
structure within the length-asymptotic regime of infinite-cryptic order (and
infinite Markov order) processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08202</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08202</id><created>2015-10-28</created><authors><author><keyname>Homri</keyname><forenames>Adi</forenames></author><author><keyname>Peleg</keyname><forenames>Michael</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>Oblivious Processing in a Fronthaul Constrained Gaussian Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider systems for which the transmitter conveys messages to the
receiver through a capacity-limited relay station. The channel between the
transmitter and the relay-station is assumed to be frequency selective additive
Gaussian. It is assumed that the transmitter can shape the spectrum and adapt
the coding technique as to optimize performance.The relay operation is
oblivious, that is, the specific codebooks used are unknown, while the spectral
shape of the transmitted signal is available. We find the reliable information
rate that can be achieved in this setting, and to that end employ Gaussian
bottleneck results combined with Shannon's incremental frequency approach
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08204</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08204</id><created>2015-10-28</created><authors><author><keyname>Mahdavifar</keyname><forenames>Hessam</forenames></author><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Touri</keyname><forenames>Behrouz</forenames></author><author><keyname>Shamma</keyname><forenames>Jeff S.</forenames></author></authors><title>Global Games with Noisy Information Sharing</title><categories>cs.SI cs.GT math.OC</categories><comments>Submitted to the IEEE Transactions on Control and Network Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global games form a subclass of games with incomplete information where a set
of agents decide actions against a regime with an underlying fundamental
$\theta$ representing its power. Each agent has access to an independent noisy
observation of $\theta$. In order to capture the behavior of agents in a social
network of information exchange we assume that agents share their observation
in a noisy environment prior to making their decision. We show that global
games with noisy sharing of information do not admit an intuitive type of
threshold policies which only depends on agents' belief about the underlying
$\theta$. This is in contrast to the existing results on the threshold policies
for the conventional set-up of global games. Motivated by this result, we
investigate the existence of equilibrium strategies in a more general
collection of threshold-type policies and show that such equilibrium strategies
exist and are unique if the sharing of information happens over a sufficiently
noisy environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08210</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08210</id><created>2015-10-28</created><authors><author><keyname>Topkara</keyname><forenames>Mercan</forenames></author><author><keyname>Erickson</keyname><forenames>Thomas</forenames></author><author><keyname>Topkara</keyname><forenames>Umut</forenames></author><author><keyname>Narayanaswami</keyname><forenames>Chandrasekhar</forenames></author></authors><title>Enabling Multiple QR Codes in Close Proximity</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quick response codes - 2D patterns that can be scanned to access online
resources - are being used in a variety of industrial and consumer
applications. However, it is problematic to use multiple QR codes in close
proximity: scans can fail or result in access to the wrong resource. While this
problem is, strictly speaking, due to the design of the scanning software, the
very large number of extant scanning applications makes changing the software a
difficult logistical challenge. Instead, we describe the design of a new type
of QR code that not only enables the use of multiple QR codes in close
proximity, but also is compatible with existing scanning solutions. In an
evaluation with 20 users, it was found that the new QR codes were as usable as
traditional ones, and that they were superior for selecting one code from many.
Users did have initial difficulty in discovering how to use the new QR code, so
further work is required on that front. We conclude with a discussion of the
pros and cons of pQR codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08213</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08213</id><created>2015-10-28</created><authors><author><keyname>Bustin</keyname><forenames>Ronit</forenames><affiliation>Shitz</affiliation></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Optimal Point-to-Point Codes in Interference Channels: An Incremental
  I-MMSE approach</title><categories>cs.IT math.IT</categories><comments>submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent result of the authors shows a so-called I-MMSE-like relationship
that, for the two-user Gaussian interference channel, an I-MMSE relationship
holds in the limit, as n $\to \infty$, between the interference and the
interfered-with receiver, assuming that the interfered-with transmission is an
optimal point-to-point sequence (achieves the point-to-point capacity). This
result was further used to provide a proof of the &quot;missing corner points&quot; of
the two-user Gaussian interference channel. This paper provides an information
theoretic proof of the above-mentioned I-MMSE-like relationship which follows
the incremental channel approach, an approach which was used by Guo, Shamai and
Verd\'u to provide an insightful proof of the original I-MMSE relationship for
point-to-point channels. Finally, some additional applications of this result
are shown for other multi-user settings: the Gaussian multiple-access channel
with interference and specific K-user Gaussian Z-interference channel settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08231</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08231</id><created>2015-10-28</created><updated>2015-11-04</updated><authors><author><keyname>Kadri</keyname><forenames>Hachem</forenames><affiliation>LIF</affiliation></author><author><keyname>Duflos</keyname><forenames>Emmanuel</forenames><affiliation>CRIStAL</affiliation></author><author><keyname>Preux</keyname><forenames>Philippe</forenames><affiliation>CRIStAL, SEQUEL</affiliation></author><author><keyname>Canu</keyname><forenames>St&#xe9;phane</forenames><affiliation>LITIS</affiliation></author><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author><author><keyname>Audiffren</keyname><forenames>Julien</forenames><affiliation>CMLA</affiliation></author></authors><title>Operator-valued Kernels for Learning from Functional Response Data</title><categories>cs.LG stat.ML</categories><comments>in Journal of Machine Learning Research (JMLR), 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problems of supervised classification and
regression in the case where attributes and labels are functions: a data is
represented by a set of functions, and the label is also a function. We focus
on the use of reproducing kernel Hilbert space theory to learn from such
functional data. Basic concepts and properties of kernel-based learning are
extended to include the estimation of function-valued functions. In this
setting, the representer theorem is restated, a set of rigorously defined
infinite-dimensional operator-valued kernels that can be valuably applied when
the data are functions is described, and a learning algorithm for nonlinear
functional data analysis is introduced. The methodology is illustrated through
speech and audio signal processing experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08233</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08233</id><created>2015-10-28</created><authors><author><keyname>Palmieri</keyname><forenames>Luigi</forenames></author><author><keyname>Rudenko</keyname><forenames>Andrey</forenames></author><author><keyname>Arras</keyname><forenames>Kai O.</forenames></author></authors><title>A Fast Randomized Method to Find Homotopy Classes for Socially-Aware
  Navigation</title><categories>cs.RO</categories><comments>In Proceedings of the IROS 2015 Workshop on Assistance and Service
  Robotics in a Human Environment Workshop, Hamburg, Germany, 2015</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce and show preliminary results of a fast randomized method that
finds a set of K paths lying in distinct homotopy classes. We frame the path
planning task as a graph search problem, where the navigation graph is based on
a Voronoi diagram. The search is biased by a cost function derived from the
social force model that is used to generate and select the paths. We compare
our method to Yen's algorithm, and empirically show that our approach is faster
to find a subset of homotopy classes. Furthermore our approach computes a set
of more diverse paths with respect to the baseline while obtaining a negligible
loss in path quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08237</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08237</id><created>2015-10-28</created><authors><author><keyname>Alaimo</keyname><forenames>Salvatore</forenames></author><author><keyname>Giugno</keyname><forenames>Rosalba</forenames></author><author><keyname>Acunzo</keyname><forenames>Mario</forenames></author><author><keyname>Veneziano</keyname><forenames>Dario</forenames></author><author><keyname>Ferro</keyname><forenames>Alfredo</forenames></author><author><keyname>Pulvirenti</keyname><forenames>Alfredo</forenames></author></authors><title>Post-transcriptional knowledge in pathway analysis increases the
  accuracy of phenotypes classification</title><categories>q-bio.MN cs.CE</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Motivation: Prediction of phenotypes from high-dimensional data is a crucial
task in precision biology and medicine. Many technologies employ genomic
biomarkers to characterize phenotypes. However, such elements are not
sufficient to explain the underlying biology. To improve this, pathway analysis
techniques have been proposed. Nevertheless, such methods have shown lack of
precision in phenotypes classification. Results: Here we propose a novel
methodology called MITHrIL (Mirna enrIched paTHway Impact anaLysis) for the
analysis of signaling pathways. MITHrIL extends pathways by adding missing
regulatory elements, such as microRNAs, and their interactions with genes. The
method takes as input the expression values of genes and/or microRNAs and
returns a list of pathways sorted according to their deregulation degee,
together with the corresponding statistical significance (p-values). Our
analysis shows that MITHrIL outperforms its competitors even in the worst case.
In addition, our method is able to correctly classify sets of tumor samples
drawn from TCGA (overall error rate equal to 0.90%). Availability: MITHrIL is
freely available at the following URL: http://alpha.dmi.unict.it/mithril/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08240</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08240</id><created>2015-10-28</created><authors><author><keyname>Omer</keyname><forenames>H</forenames><affiliation>I2M</affiliation></author><author><keyname>Torr&#xe9;sani</keyname><forenames>B</forenames><affiliation>I2M</affiliation></author></authors><title>Time-frequency and time-scale analysis of deformed stationary processes,
  with application to non-stationary sound modeling</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Applied and Computational Harmonic Analysis, Elsevier, 2016,
  \&amp;lt;10.1016/j.acha.2015.10.002\&amp;gt</comments><proxy>ccsd</proxy><doi>10.1016/j.acha.2015.10.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of random non-stationary signals termed timbre x dynamics is
introduced and studied. These signals are obtained by non-linear
transformations of sta-tionary random gaussian signals, in such a way that the
transformation can be approximated by translations in an appropriate
representation domain. In such situations, approximate maximum likelihood
estimation techniques can be de-rived, which yield simultaneous estimation of
the transformation and the power spectrum of the underlying stationary signal.
This paper focuses on the case of modulation and time warping of station-ary
signals, and proposes and studies estimation algorithms (based on
time-frequency and time-scale representations respectively) for these
quantities of interest. The proposed approach is validated on numerical
simulations on synthetic signals, and examples on real life car engine sounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08259</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08259</id><created>2015-10-28</created><authors><author><keyname>Vassilaras</keyname><forenames>Spyridon</forenames></author><author><keyname>Alexandropoulos</keyname><forenames>George C.</forenames></author><author><keyname>Kalis</keyname><forenames>Antonis A.</forenames></author></authors><title>Bit Error Rate Analysis of Cooperative Beamforming for Transmitting
  Individual Data Streams</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, conference in IEEE ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative beamforming (CB) has been proposed as a special case of
coordinated multi-point techniques in wireless communications. In wireless
sensor networks, CB can enable low power communication by allowing a collection
of sensor nodes to transmit data simultaneously to a distant fusion center in
one hop. Besides the traditional CB approach where all nodes need to share and
transmit the same data, a more recent technique allows each node to transmit
its own data while still achieving the benefits of cooperation. However, the
intricacies of varying beamforming gains in the direct sequence spread spectrum
with binary frequency shift keying multiple access scheme used in this context
need to be taken into account when evaluating the performance of this
beamforming technique. In this paper, we take the first step towards a more
comprehensive understanding of this individual-data CB technique by proposing a
best suited decoding scheme and analyzing its bit error rate (BER) performance
over an additive white Gaussian noise channel. Through analytical expressions
and simulation results BER curves are drawn and the achieved performance
improvement offered by the CB gain is quantified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08266</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08266</id><created>2015-10-28</created><updated>2015-11-01</updated><authors><author><keyname>Codish</keyname><forenames>Michael</forenames></author><author><keyname>Frank</keyname><forenames>Michael</forenames></author><author><keyname>Itzhakov</keyname><forenames>Avraham</forenames></author><author><keyname>Miller</keyname><forenames>Alice</forenames></author></authors><title>Computing the Ramsey Number R(4,3,3) using Abstraction and Symmetry
  breaking</title><categories>cs.AI cs.DM</categories><comments>arXiv admin note: text overlap with arXiv:1409.5189</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number $R(4,3,3)$ is often presented as the unknown Ramsey number with
the best chances of being found &quot;soon&quot;. Yet, its precise value has remained
unknown for almost 50 years. This paper presents a methodology based on
\emph{abstraction} and \emph{symmetry breaking} that applies to solve hard
graph edge-coloring problems. The utility of this methodology is demonstrated
by using it to compute the value $R(4,3,3)=30$. Along the way it is required to
first compute the previously unknown set ${\cal R}(3,3,3;13)$ consisting of
78{,}892 Ramsey colorings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08267</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08267</id><created>2015-10-28</created><authors><author><keyname>Balaji</keyname><forenames>Nikhil</forenames></author><author><keyname>Datta</keyname><forenames>Samir</forenames></author><author><keyname>Kulkarni</keyname><forenames>Raghav</forenames></author><author><keyname>Podder</keyname><forenames>Supartha</forenames></author></authors><title>Graph properties in node-query setting: effect of breaking symmetry</title><categories>cs.CC cs.DM math.CO</categories><comments>26 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The query complexity of graph properties is well-studied when queries are on
edges. We investigate the same when queries are on nodes. In this setting a
graph $G = (V, E)$ on $n$ vertices and a property $\mathcal{P}$ are given. A
black-box access to an unknown subset $S \subseteq V$ is provided via queries
of the form `Does $i \in S$?'. We are interested in the minimum number of
queries needed in worst case in order to determine whether $G[S]$, the subgraph
of $G$ induced on $S$, satisfies $\mathcal{P}$.
  Apart from being combinatorially rich, this setting allows us to initiate a
systematic study of breaking symmetry in the context of query complexity of
graph properties. In particular, we focus on hereditary graph properties. The
monotone functions in the node-query setting translate precisely to the
hereditary graph properties. The famous Evasiveness Conjecture asserts that
even with a minimal symmetry assumption on $G$, namely that of
vertex-transitivity, the query complexity for any hereditary graph property in
our setting is the worst possible, i.e., $n$.
  We show that in the absence of any symmetry on $G$ it can fall as low as
$O(n^{1/(d + 1) })$ where $d$ denotes the minimum possible degree of a minimal
forbidden sub-graph for $\mathcal{P}$. In particular, every hereditary property
benefits at least quadratically. The main question left open is: can it go
exponentially low for some hereditary property?
  We show that the answer is no for any hereditary property with {finitely
many} forbidden subgraphs by exhibiting a bound of $\Omega(n^{1/k})$ for some
constant $k$ depending only on the property. For general ones we rule out the
possibility of the query complexity falling down to constant by showing
$\Omega(\log n/ \log \log n)$ bound. Interestingly, our lower bound proofs rely
on the famous Sunflower Lemma due to Erd\&quot;os and Rado.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08271</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08271</id><created>2015-10-28</created><authors><author><keyname>Meng</keyname><forenames>Fan-Lin</forenames></author><author><keyname>Zeng</keyname><forenames>Xiao-Jun</forenames></author></authors><title>A Hybrid Optimization Approach to Demand Response Management for the
  Smart Grid</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a hybrid approach to optimal day-ahead pricing for demand
response management. At the customer-side, compared with the existing work, a
detailed, comprehensive and complete energy management system, which includes
all possible types of appliances, all possible applications, and an effective
waiting time cost model is proposed to manage the energy usages in households
(lower level problem). At the retailer-side, the best retail prices are
determined to maximize the retailer's profit (upper level problem). The
interactions between the electricity retailer and its customers can be cast as
a bilevel optimization problem. To overcome the weakness and infeasibility of
conventional Karush--Kuhn--Tucker (KKT) approach for this particular type of
bilevel problem, a hybrid pricing optimization approach, which adopts the
multi-population genetic algorithms for the upper level problem and distributed
individual optimization algorithms for the lower level problem, is proposed.
Numerical results show the applicability and effectiveness of the proposed
approach and its benefit to the retailer and its customers by improving the
retailer's profit and reducing the customers' bills.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08274</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08274</id><created>2015-10-28</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author><author><keyname>Di Battista</keyname><forenames>Giuseppe</forenames></author><author><keyname>Frati</keyname><forenames>Fabrizio</forenames></author><author><keyname>Patrignani</keyname><forenames>Maurizio</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Testing Cyclic Level and Simultaneous Level Planarity</title><categories>cs.DS cs.CG</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove that testing the $\textit{cyclic level planarity}$ of
a cyclic level graph is a polynomial-time solvable problem. This is achieved by
introducing and studying a generalization of this problem, which we call
$\textit{cyclic }{\cal T}\textit{-level planarity}$. Moreover, we show a
complexity dichotomy for testing the $\textit{simultaneous level planarity}$ of
a set of level graphs, with respect to both the number of level graphs and the
number of levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08276</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08276</id><created>2015-10-28</created><authors><author><keyname>You</keyname><forenames>Jie</forenames></author><author><keyname>Wang</keyname><forenames>Jianxin</forenames></author><author><keyname>Cao</keyname><forenames>Yixin</forenames></author></authors><title>Approximate Association via Dissociation</title><categories>cs.DS</categories><msc-class>05C75, 68R10</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A vertex set $X$ of a graph $G$ is an association set if each component of $G
- X$ is a clique, or a dissociation set if each component of $G - X$ is a
single vertex or a single edge. Interestingly, $G - X$ is then precisely a
graph containing no induced $P_3$'s or containing no $P_3$'s, respectively. We
observe some special structures and show that if none of them exists, then the
minimum association set problem can be reduced to the minimum (weighted)
dissociation set problem. This yields the first nontrivial approximation
algorithm for association set, and its approximation ratio is 2.5, matching the
best result of the closely related cluster editing problem. The reduction is
based on a combinatorial study of modular decomposition of graphs free of these
special structures. Further, a novel algorithmic use of modular decomposition
enables us to implement this approach in $O(m n + n^2)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08282</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08282</id><created>2015-10-28</created><authors><author><keyname>Chhabra</keyname><forenames>Anamika</forenames></author><author><keyname>Iyengar</keyname><forenames>S. R. S.</forenames></author><author><keyname>Saini</keyname><forenames>Jaspal Singh</forenames></author></authors><title>Skillset Distribution for Accelerated Knowledge Building in Crowdsourced
  Environments</title><categories>cs.HC cs.CY</categories><comments>11 pages, 8 figures, Submitted to WWW 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing has revolutionized the process of knowledge building on the
web. Wikipedia and StackOverflow are witness to this uprising development.
However, the dynamics behind the success of crowdsourcing in the domain of
knowledge building is an area relatively unexplored. It has been observed that
ecosystem exists in the collaborative knowledge building environments (KBE),
which divides the people in a KBE into various categories based on their
skills. In this work, we provide a detailed investigation of the process,
explaining the reason behind fast and efficient knowledge building in such
settings. We follow on Luhmann's theory of autopoietic systems and hypothesize
that the existence of categories leads to triggering, which makes a knowledge
building system an autopoietic system. This triggering process helps bring a
substantial amount of extra knowledge to the system, which would have remained
undiscovered otherwise. We quantitatively analyze the contribution of triggered
knowledge and find it to be a significant part of the total knowledge
generated. We demonstrate that different distribution of users across
categories leads to varied amount of knowledge in the system. We further
discuss on the ideal distribution of users for accelerated knowledge building.
The study will help the portal designers to accordingly build suitable
crowdsourced environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08291</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08291</id><created>2015-10-28</created><authors><author><keyname>Bernard</keyname><forenames>Florian</forenames></author><author><keyname>Gemmar</keyname><forenames>Peter</forenames></author><author><keyname>Hertel</keyname><forenames>Frank</forenames></author><author><keyname>Goncalves</keyname><forenames>Jorge</forenames></author><author><keyname>Thunberg</keyname><forenames>Johan</forenames></author></authors><title>Linear Shape Deformation Models with Local Support Using Graph-based
  Structured Matrix Factorisation</title><categories>cs.CV math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representing 3D shape deformations by linear models in high-dimensional space
has many applications in computer vision and medical imaging, such as
shape-based interpolation or segmentation. Commonly, using Principal Components
Analysis a low-dimensional (affine) subspace of the high-dimensional shape
space is determined. However, the resulting factors (the most dominant
eigenvectors of the covariance matrix) have global support, i.e. changing the
coefficient of a single factor deforms the entire shape. In this paper, a
method to obtain deformation factors with local support is presented. The
benefits of such models include better flexibility and interpretability as well
as the possibility of interactively deforming shapes locally. For that, based
on a well-grounded theoretical motivation, we formulate a matrix factorisation
problem employing sparsity and graph-based regularisation terms. We demonstrate
that for brain shapes our method outperforms the state of the art in local
support models with respect to generalisation ability and sparse shape
reconstruction, whereas for human body shapes our method gives more realistic
deformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08294</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08294</id><created>2015-10-28</created><authors><author><keyname>Sadamoto</keyname><forenames>Tomonori</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Besselink</keyname><forenames>Bart</forenames></author><author><keyname>Ishizaki</keyname><forenames>Takayuki</forenames></author><author><keyname>Imura</keyname><forenames>Jun-ichi</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Weak Resilience of Networked Control Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method to establish a networked control system
that maintains its stability in the presence of certain undesirable incidents
on local controllers. We call such networked control systems weakly resilient.
We first derive a necessary and sufficient condition for the weak resilience of
networked systems. Networked systems do not generally satisfy this condition.
Therefore, we provide a method for designing a compensator which ensures the
weak resilience of the compensated system. Finally, we illustrate the
efficiency of the proposed method by a power system example based on the IEEE
14-bus test system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08297</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08297</id><created>2015-10-28</created><authors><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>Numerical solving unsteady space-fractional problems with the square
  root of an elliptic operator</title><categories>math.NA cs.NA</categories><comments>21 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:1412.5706</comments><msc-class>26A33, 35R11, 65F60, 65M06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An unsteady problem is considered for a space-fractional equation in a
bounded domain. A first-order evolutionary equation involves the square root of
an elliptic operator of second order. Finite element approximation in space is
employed. To construct approximation in time, regularized two-level schemes are
used. The numerical implementation is based on solving the equation with the
square root of the elliptic operator using an auxiliary Cauchy problem for a
pseudo-parabolic equation. The scheme of the second-order accuracy in time is
based on a regularization of the three-level explicit Adams scheme. More
general problems for the equation with convective terms are considered, too.
The results of numerical experiments are presented for a model two-dimensional
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08301</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08301</id><created>2015-10-28</created><authors><author><keyname>Lee</keyname><forenames>Wonju</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Kang</keyname><forenames>Joonhyuk</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Multivariate Fronthaul Quantization for Downlink C-RAN</title><categories>cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Cloud-Radio Access Network (C-RAN) cellular architecture relies on the
transfer of complex baseband signals to and from a central unit (CU) over
digital fronthaul links to enable the virtualization of the baseband processing
functionalities of distributed radio units (RUs). The standard design of
digital fronthauling is based on either scalar quantization or on more
sophisticated point-to-point compression techniques operating on baseband
signals. Motivated by network-information theoretic results, techniques for
fronthaul quantization and compression that improve over point-to-point
solutions by allowing for joint processing across multiple fronthaul links at
the CU have been recently proposed for both the uplink and the downlink. For
the downlink, a form of joint compression, known in network information theory
as multivariate compression, was shown to be advantageous under a
non-constructive asymptotic information-theoretic framework. In this paper,
instead, the design of a practical symbol-by-symbol fronthaul quantization
algorithm that implements the idea of multivariate compression is investigated
for the C-RAN downlink. As compared to current standards, the proposed
multivariate quantization (MQ) only requires changes in the CU processing while
no modification is needed at the RUs. The algorithm is extended to enable the
joint optimization of downlink precoding and quantization, and also
variable-length compression. Numerical results demonstrate the advantages of MQ
and the merits of a joint optimization with precoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08331</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08331</id><created>2015-10-28</created><updated>2015-12-20</updated><authors><author><keyname>Frank</keyname><forenames>Drewes</forenames><affiliation>Dept. of Computing Science, Ume&#xe5; University, Ume&#xe5;, Sweden</affiliation></author><author><keyname>J&#xe9;r&#xf4;me</keyname><forenames>Leroux</forenames><affiliation>LaBRI, CNRS</affiliation></author></authors><title>Structurally Cyclic Petri Nets</title><categories>cs.LO</categories><comments>9 pages. Key words: Petri net, vector addition system, structural
  cyclicity, reachability</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:15) 2015</journal-ref><doi>10.2168/LMCS-11(4:15)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Petri net is structurally cyclic if every configuration is reachable from
itself in one or more steps. We show that structural cyclicity is decidable in
deterministic polynomial time. For this, we adapt the Kosaraju's approach for
the general reachability problem for Petri nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08332</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08332</id><created>2015-10-28</created><authors><author><keyname>Franceschet</keyname><forenames>Massimo</forenames></author><author><keyname>Bozzo</keyname><forenames>Enrico</forenames></author></authors><title>A theory on power in networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The eigenvector centrality equation $\lambda x = A \, x$ is a successful
compromise between simplicity and expressivity. It claims that central actors
are those connected with central others. For at least 70 years, this equation
has been explored in disparate contexts, including econometrics, sociometry,
bibliometrics, Web information retrieval, and network science. We propose an
equally elegant counterpart: the power equation $x = A x^{\div}$, where
$x^{\div}$ is the vector whose entries are the reciprocal of those of $x$. It
asserts that power is in the hands of those connected with powerless others. It
is meaningful, for instance, in bargaining situations, where it is advantageous
to be connected to those who have few options. We tell the parallel, mostly
unexplored story of this intriguing equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08334</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08334</id><created>2015-10-28</created><authors><author><keyname>Speck</keyname><forenames>Robert</forenames></author><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author></authors><title>Fault-tolerant parallel-in-time integration with PFASST</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and analyze different strategies for the parallel-in-time
integration method PFASST to recover from hard faults and subsequent data loss.
Since PFASST stores solutions at multiple time steps on different processors,
information from adjacent steps can be used to recover after a processor has
failed. PFASST's multi-level hierarchy allows to use the coarse level for
correcting the reconstructed solution, which can help to minimize overhead. A
theoretical model is devised linking overhead to the number of additional
PFASST iterations required for convergence after a fault. The efficiency of
different strategies is assessed for examples of diffusive and advective type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08341</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08341</id><created>2015-10-28</created><authors><author><keyname>Chung</keyname><forenames>Hye Won</forenames></author><author><keyname>Sadler</keyname><forenames>Brian M.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames></author></authors><title>Bounds on Variance for Symmetric Unimodal Distributions</title><categories>cs.IT math.IT</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a direct relationship between the variance and the differential
entropy for the general class of symmetric unimodal distributions by providing
an upper bound on variance in terms of entropy power. Combining this bound with
the well-known entropy power lower bound on variance, we prove that the
variance of the general class of symmetric unimodal distributions can be
bounded below and above by the scaled entropy power. As differential entropy
decreases, the variance is sandwiched between two exponentially decreasing
functions in the differential entropy. This establishes that for the general
class of symmetric unimodal distributions, the differential entropy can be used
as a surrogate for concentration of the distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08345</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08345</id><created>2015-10-28</created><updated>2016-01-26</updated><authors><author><keyname>Hynes</keyname><forenames>Michael B</forenames></author><author><keyname>De Sterck</keyname><forenames>Hans</forenames></author></authors><title>A polynomial expansion line search for large-scale unconstrained
  minimization of smooth L2-regularized loss functions, with implementation in
  Apache Spark</title><categories>math.NA cs.DC cs.NA</categories><comments>9 pages, 8 figures, 2 tables. Preprint appearing in SIAM Conf on Data
  Mining, Miami, FL, 2016</comments><msc-class>65K05</msc-class><acm-class>G.1.6; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In large-scale unconstrained optimization algorithms such as limited memory
BFGS (LBFGS), a common subproblem is a line search minimizing the loss function
along a descent direction. Commonly used line searches iteratively find an
approximate solution for which the Wolfe conditions are satisfied, typically
requiring multiple function and gradient evaluations per line search, which is
expensive in parallel due to communication requirements. In this paper we
propose a new line search approach for cases where the loss function is
analytic, as in least squares regression, logistic regression, or low rank
matrix factorization. We approximate the loss function by a truncated Taylor
polynomial, whose coefficients may be computed efficiently in parallel with
less communication than evaluating the gradient, after which this polynomial
may be minimized with high accuracy in a neighbourhood of the expansion point.
Our Polynomial Expansion Line Search (PELS) was implemented in the Apache Spark
framework and used to accelerate the training of a logistic regression model on
binary classification datasets from the LIBSVM repository with LBFGS and the
Nonlinear Conjugate Gradient (NCG) method. In large-scale numerical experiments
in parallel on a 16-node cluster with 256 cores using the URL, KDDA, and KDDB
datasets, the PELS approach produced significant convergence improvements
compared to the use of classical Wolfe line searches. For example, to reach the
final training label prediction accuracies, LBFGS using PELS had speedup
factors of 1.8--2 over LBFGS using a Wolfe line search, measured by both the
number of iterations and the time required, due to the better accuracy of step
sizes computed in the line search. PELS has the potential to significantly
accelerate large-scale regression and factorization computations, and is
applicable to continuous optimization problems with smooth loss functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08352</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08352</id><created>2015-10-28</created><authors><author><keyname>Zhandry</keyname><forenames>Mark</forenames></author></authors><title>Quantum Oracle Classification - The Case of Group Structure</title><categories>cs.CC cs.CR quant-ph</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quantum Oracle Classification (QOC) problem is to classify a function,
given only quantum black box access, into one of several classes without
necessarily determining the entire function. Generally, QOC captures a very
wide range of problems in quantum query complexity. However, relatively little
is known about many of these problems.
  In this work, we analyze the a subclass of the QOC problems where there is a
group structure. That is, suppose the range of the unknown function A is a
commutative group G, which induces a commutative group law over the entire
function space. Then we consider the case where A is drawn uniformly at random
from some subgroup A of the function space. Moreover, there is a homomorpism f
on A, and the goal is to determine f(A). This class of problems is very
general, and covers several interesting cases, such as oracle evaluation;
polynomial interpolation, evaluation, and extrapolation; and parity. These
problems are important in the study of message authentication codes in the
quantum setting, and may have other applications.
  We exactly characterize the quantum query complexity of every instance of QOC
with group structure in terms of a particular counting problem. That is, we
provide an algorithm for this general class of problems whose success
probability is determined by the solution to the counting problem, and prove
its exact optimality. Unfortunately, solving this counting problem in general
is a non-trivial task, and we resort to analyzing special cases. Our bounds
unify some existing results, such as the existing oracle evaluation and parity
bounds. In the case of polynomial interpolation and evaluation, our bounds give
new results for secret sharing and information theoretic message authentication
codes in the quantum setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08364</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08364</id><created>2015-10-28</created><authors><author><keyname>L&#xe1;zaro</keyname><forenames>Francisco</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author><author><keyname>Bauch</keyname><forenames>Gerhard</forenames></author></authors><title>Inactivation Decoding Analysis for LT Codes</title><categories>cs.IT math.IT</categories><comments>Presented at the 53rd Annual Allerton Conference on Communication,
  Control, and Computing, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide two analytical tools to model the inactivation decoding process of
LT codes. First, a model is presented which derives the expected number of
inactivations occurring in the decoding process of an LT code. This analysis is
then extended allowing the derivation of the distribution of the number of
inactivations. The accuracy of the method is verified by Monte Carlo
simulations. The proposed analysis opens the door to the design of LT codes
optimized for inactivation decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08368</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08368</id><created>2015-10-28</created><authors><author><keyname>di Bernardo</keyname><forenames>Mario</forenames></author><author><keyname>Fiore</keyname><forenames>Davide</forenames></author></authors><title>Switching control for incremental stabilization of nonlinear systems via
  contraction theory</title><categories>cs.SY</categories><comments>Preprint submitted to ECC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a switching control strategy to incrementally
stabilize a class of nonlinear dynamical systems. Exploiting recent results on
contraction analysis of switched Filippov systems derived using regularization,
sufficient conditions are presented to prove incremental stability of the
closed-loop system. Furthermore, based on these sufficient conditions, a design
procedure is proposed to design a switched control action that is active only
where the open-loop system is not sufficiently incrementally stable in order to
reduce the required control effort. The design procedure to either locally or
globally incrementally stabilize a dynamical system is then illustrated by
means of a representative example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08370</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08370</id><created>2015-10-28</created><authors><author><keyname>Nguyen</keyname><forenames>Hoang-Vu</forenames></author><author><keyname>Vreeken</keyname><forenames>Jilles</forenames></author></authors><title>Canonical Divergence Analysis</title><categories>stat.ML cs.LG</categories><comments>Submission to AISTATS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We aim to analyze the relation between two random vectors that may
potentially have both different number of attributes as well as realizations,
and which may even not have a joint distribution. This problem arises in many
practical domains, including biology and architecture. Existing techniques
assume the vectors to have the same domain or to be jointly distributed, and
hence are not applicable. To address this, we propose Canonical Divergence
Analysis (CDA). We introduce three instantiations, each of which permits
practical implementation. Extensive empirical evaluation shows the potential of
our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08371</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08371</id><created>2015-10-28</created><authors><author><keyname>Avgustinovich</keyname><forenames>Sergey</forenames><affiliation>I2M</affiliation></author><author><keyname>Frid</keyname><forenames>Anna</forenames><affiliation>I2M</affiliation></author><author><keyname>Puzynina</keyname><forenames>Svetlana</forenames><affiliation>ENS Lyon</affiliation></author></authors><title>Canonical Representatives of Morphic Permutations</title><categories>cs.DM math.CO</categories><comments>Springer. WORDS 2015, Sep 2015, Kiel, Germany. Combinatorics on
  Words: 10th International Conference. arXiv admin note: text overlap with
  arXiv:1503.06188</comments><proxy>ccsd</proxy><journal-ref>WORDS 2015, 9304, pp.59-72, 2015, Lecture Notes in Computer
  Science</journal-ref><doi>10.1007/978-3-319-23660-5_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An infinite permutation can be defined as a linear ordering of the set of
natural numbers. In particular, an infinite permutation can be constructed with
an aperiodic infinite word over $\{0,\ldots,q-1\}$ as the lexicographic order
of the shifts of the word. In this paper, we discuss the question if an
infinite permutation defined this way admits a canonical representative, that
is, can be defined by a sequence of numbers from [0, 1], such that the
frequency of its elements in any interval is equal to the length of that
interval. We show that a canonical representative exists if and only if the
word is uniquely ergodic, and that is why we use the term ergodic permutations.
We also discuss ways to construct the canonical representative of a permutation
defined by a morphic word and generalize the construction of Makarov, 2009, for
the Thue-Morse permutation to a wider class of infinite words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08380</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08380</id><created>2015-10-28</created><authors><author><keyname>Sturaro</keyname><forenames>Agostino</forenames></author><author><keyname>Silvestri</keyname><forenames>Simone</forenames></author><author><keyname>Conti</keyname><forenames>Mauro</forenames></author><author><keyname>Das</keyname><forenames>Sajal K.</forenames></author></authors><title>Towards a Realistic Model for Failure Propagation in Interdependent
  Networks</title><categories>cs.NI cs.SI physics.soc-ph</categories><comments>7 pages, 6 figures, to be published in conference proceedings of IEEE
  International Conference on Computing, Networking and Communications (ICNC
  2016), Kauai, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern networks are becoming increasingly interdependent. As a prominent
example, the smart grid is an electrical grid controlled through a
communications network, which in turn is powered by the electrical grid. Such
interdependencies create new vulnerabilities and make these networks more
susceptible to failures. In particular, failures can easily spread across these
networks due to their interdependencies, possibly causing cascade effects with
a devastating impact on their functionalities.
  In this paper we focus on the interdependence between the power grid and the
communications network, and propose a novel realistic model, HINT
(Heterogeneous Interdependent NeTworks), to study the evolution of cascading
failures. Our model takes into account the heterogeneity of such networks as
well as their complex interdependencies. We compare HINT with previously
proposed models both on synthetic and real network topologies. Experimental
results show that existing models oversimplify the failure evolution and
network functionality requirements, resulting in severe underestimations of the
cascading failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08382</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08382</id><created>2015-10-28</created><authors><author><keyname>Nguyen</keyname><forenames>Hoang-Vu</forenames></author><author><keyname>Vreeken</keyname><forenames>Jilles</forenames></author></authors><title>Flexibly Mining Better Subgroups</title><categories>stat.ML cs.LG</categories><comments>Submission to SDM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In subgroup discovery, also known as supervised pattern mining, discovering
high quality one-dimensional subgroups and refinements of these is a crucial
task. For nominal attributes, this is relatively straightforward, as we can
consider individual attribute values as binary features. For numerical
attributes, the task is more challenging as individual numeric values are not
reliable statistics. Instead, we can consider combinations of adjacent values,
i.e. bins. Existing binning strategies, however, are not tailored for subgroup
discovery. That is, they do not directly optimize for the quality of subgroups,
therewith potentially degrading the mining result.
  To address this issue, we propose FLEXI. In short, with FLEXI we propose to
use optimal binning to find high quality binary features for both numeric and
ordinal attributes. We instantiate FLEXI with various quality measures and show
how to achieve efficiency accordingly. Experiments on both synthetic and
real-world data sets show that FLEXI outperforms state of the art with up to 25
times improvement in subgroup quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08385</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08385</id><created>2015-10-28</created><authors><author><keyname>Nguyen</keyname><forenames>Hoang-Vu</forenames></author><author><keyname>Vreeken</keyname><forenames>Jilles</forenames></author></authors><title>Linear-time Detection of Non-linear Changes in Massively High
  Dimensional Time Series</title><categories>stat.ML cs.LG</categories><comments>Submission to SDM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Change detection in multivariate time series has applications in many
domains, including health care and network monitoring. A common approach to
detect changes is to compare the divergence between the distributions of a
reference window and a test window. When the number of dimensions is very
large, however, the naive approach has both quality and efficiency issues: to
ensure robustness the window size needs to be large, which not only leads to
missed alarms but also increases runtime.
  To this end, we propose LIGHT, a linear-time algorithm for robustly detecting
non-linear changes in massively high dimensional time series. Importantly,
LIGHT provides high flexibility in choosing the window size, allowing the
domain expert to fit the level of details required. To do such, we 1) perform
scalable PCA to reduce dimensionality, 2) perform scalable factorization of the
joint distribution, and 3) scalably compute divergences between these lower
dimensional distributions. Extensive empirical evaluation on both synthetic and
real-world data show that LIGHT outperforms state of the art with up to 100%
improvement in both quality and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08389</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08389</id><created>2015-10-28</created><authors><author><keyname>Nguyen</keyname><forenames>Hoang-Vu</forenames></author><author><keyname>Vreeken</keyname><forenames>Jilles</forenames></author></authors><title>Universal Dependency Analysis</title><categories>stat.ML cs.LG</categories><comments>Submission to SDM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most data is multi-dimensional. Discovering whether any subset of dimensions,
or subspaces, of such data is significantly correlated is a core task in data
mining. To do so, we require a measure that quantifies how correlated a
subspace is. For practical use, such a measure should be universal in the sense
that it captures correlation in subspaces of any dimensionality and allows to
meaningfully compare correlation scores across different subspaces, regardless
how many dimensions they have and what specific statistical properties their
dimensions possess. Further, it would be nice if the measure can
non-parametrically and efficiently capture both linear and non-linear
correlations.
  In this paper, we propose UDS, a multivariate correlation measure that
fulfills all of these desiderata. In short, we define \uds based on cumulative
entropy and propose a principled normalization scheme to bring its scores
across different subspaces to the same domain, enabling universal correlation
assessment. UDS is purely non-parametric as we make no assumption on data
distributions nor types of correlation. To compute it on empirical data, we
introduce an efficient and non-parametric method. Extensive experiments show
that UDS outperforms state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08393</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08393</id><created>2015-10-28</created><authors><author><keyname>Caulfield</keyname><forenames>Benjamin</forenames></author><author><keyname>Rabe</keyname><forenames>Markus N.</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author><author><keyname>Tripakis</keyname><forenames>Stavros</forenames></author></authors><title>What's Decidable about Syntax-Guided Synthesis?</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Syntax-guided synthesis (SyGuS) is a recently proposed framework for program
synthesis problems. The SyGuS problem is to find an expression or program
generated by a given grammar that meets a correctness specification.
Correctness specifications are given as formulas in suitable logical theories,
typically amongst those studied in satisfiability modulo theories (SMT)
solving.
  In this work, we analyze the decidability of the SyGuS problem for different
classes of grammars and correctness specifications. For theories with finite
domains, we give a general algorithm to solve the SyGuS problem. Finite-domain
theories include the bit-vector theory without concatenation. We prove SyGuS
undecidable for a very simple bit-vector theory with concatenation, both for
context-free grammars and for tree grammars. Additionally, we show that for the
theory of uninterpreted functions with equality, SyGuS is undecidable even for
a very restricted class of grammars. Finally, we give some additional results
for linear arithmetic and bit-vector arithmetic along with a discussion of the
implication of these results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08417</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08417</id><created>2015-10-28</created><updated>2015-11-03</updated><authors><author><keyname>Grochow</keyname><forenames>Joshua A.</forenames></author></authors><title>Monotone projection lower bounds from extended formulation lower bounds</title><categories>cs.CC</categories><comments>7 pages (proofs are only 2 pages). New connection relating bipartite
  and general perfect matchings. Clarified that results hold over many
  semi-rings, including the Boolean and-or semi-ring. Significantly updated
  discussion and open questions</comments><msc-class>68Q15, 68Q17, 90C05, 15A15, 05C70</msc-class><acm-class>F.1.3; F.2.1; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note, we show that the Hamilton Cycle polynomial is not a
monotone sub-exponential-size projection of the permanent; this both rules out
a natural attempt at a monotone lower bound on the Boolean permanent, and shows
that the permanent is not complete for non-negative polynomials in
$VNP_{\mathbb{R}}$ under monotone p-projections. We also show that the cut
polynomials and the perfect matching polynomial (or ``unsigned Pfaffian'') are
not monotone p-projections of the permanent. The latter can be interpreted as
saying that there is no monotone projection reduction from counting perfect
matchings in general graphs to counting perfect matchings in bipartite graphs,
putting at least one theorem behind the well-established intuition. To prove
these results we introduce a new connection between monotone projections of
polynomials and extended formulations of linear programs that may have further
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08418</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08418</id><created>2015-10-28</created><authors><author><keyname>Filippova</keyname><forenames>Katja</forenames></author><author><keyname>Alfonseca</keyname><forenames>Enrique</forenames></author></authors><title>Fast k-best Sentence Compression</title><categories>cs.CL</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A popular approach to sentence compression is to formulate the task as a
constrained optimization problem and solve it with integer linear programming
(ILP) tools. Unfortunately, dependence on ILP may make the compressor
prohibitively slow, and thus approximation techniques have been proposed which
are often complex and offer a moderate gain in speed. As an alternative
solution, we introduce a novel compression algorithm which generates k-best
compressions relying on local deletion decisions. Our algorithm is two orders
of magnitude faster than a recent ILP-based method while producing better
compressions. Moreover, an extensive evaluation demonstrates that the quality
of compressions does not degrade much as we move from single best to top-five
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08419</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08419</id><created>2015-10-28</created><updated>2016-02-02</updated><authors><author><keyname>Polikarpova</keyname><forenames>Nadia</forenames></author><author><keyname>Kuraj</keyname><forenames>Ivan</forenames></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames></author></authors><title>Program Synthesis from Polymorphic Refinement Types</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for synthesizing recursive functions that provably
satisfy a given specification in the form of a refinement type. We observe that
such specifications are particularly suitable for program synthesis for two
reasons. First, they support automatic inference of rich universal invariants,
which enables synthesis of nontrivial programs with no additional hints from
the user. Second, refinement types can be decomposed more effectively than
other kinds of specifications, which is the key to pruning the search space of
candidate programs. To support such decomposition, we propose a new algorithm
for refinement type inference, which is applicable to partial programs.
  We have evaluated our prototype implementation on a large set of synthesis
problems and found that it exceeds the state of the art in terms of both
scalability and usability. The tool was able to synthesize more complex
programs than those reported in prior work (several sorting algorithms,
binary-search tree manipulations, red-black tree rotation), as well as most of
the benchmarks tackled by existing synthesizers, often starting from a more
concise and intuitive user input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08435</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08435</id><created>2015-10-28</created><updated>2015-10-30</updated><authors><author><keyname>Ruoti</keyname><forenames>Scott</forenames></author><author><keyname>Andersen</keyname><forenames>Jeff</forenames></author><author><keyname>Hendershot</keyname><forenames>Travis</forenames></author><author><keyname>Zappala</keyname><forenames>Daniel</forenames></author><author><keyname>Seamons</keyname><forenames>Kent</forenames></author></authors><title>Helping Johnny Understand and Avoid Mistakes: A Comparison of Automatic
  and Manual Encryption in Email</title><categories>cs.CR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usable, secure email remains an open problem. Recent research compared two
separate secure email systems that support manual and automatic encryption,
respectively. The system supporting manual encryption helped users better
understand secure email and avoid mistakes. However, the use of two distinct
systems casts doubt on whether the improvements can indeed be attributed to
manual encryption. In this paper, we report on the results of a formal user
study comparing two versions of Private WebMail (Pwm) that differ only in their
support for manual and automatic encryption. Our results demonstrate that
manual encryption and automatic encryption perform equally well in terms of
usability, users' understanding of secure email, and users' ability to avoid
mistakes. Furthermore, both versions achieve the highest System Usability Scale
(SUS) score ever recorded for a secure email tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08447</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08447</id><created>2015-10-28</created><authors><author><keyname>Schwarz</keyname><forenames>Martin</forenames></author></authors><title>An exponential time upper bound for Quantum Merlin-Arthur games with
  unentangled provers</title><categories>quant-ph cs.CC</categories><comments>13 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a deterministic exponential time upper bound for Quantum
Merlin-Arthur games with k unentangled provers. This is the first non-trivial
upper bound of QMA(k) better than NEXP and can be considered an exponential
improvement, unless EXP=NEXP. The key ideas of our proof are to use
perturbation theory to reduce the QMA(2)-complete Separable Sparse Hamiltonian
problem to a variant of the Separable Local Hamiltonian problem with an
exponentially small promise gap, and then to decide this instance using
epsilon-net methods. Our results imply an exponential time algorithm for the
Pure State N-Representability problem in quantum chemistry, which is in QMA(2),
but is not known to be in QMA. We also discuss the implications of our results
on the Best Separable State problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08470</identifier>
 <datestamp>2015-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08470</id><created>2015-10-28</created><authors><author><keyname>Holloway</keyname><forenames>Jason</forenames></author><author><keyname>Asif</keyname><forenames>M. Salman</forenames></author><author><keyname>Sharma</keyname><forenames>Manoj Kumar</forenames></author><author><keyname>Matsuda</keyname><forenames>Nathan</forenames></author><author><keyname>Horstmeyer</keyname><forenames>Roarke</forenames></author><author><keyname>Cossairt</keyname><forenames>Oliver</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author></authors><title>Toward Long Distance, Sub-diffraction Imaging Using Coherent Camera
  Arrays</title><categories>cs.CV physics.optics</categories><comments>13 pages, 16 figures, submitted to IEEE Transactions on Computational
  Imaging</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose using camera arrays coupled with coherent
illumination as an effective method of improving spatial resolution in long
distance images by a factor of ten and beyond. Recent advances in ptychography
have demonstrated that one can image beyond the diffraction limit of the
objective lens in a microscope. We demonstrate a similar imaging system to
image beyond the diffraction limit in long range imaging. We emulate a camera
array with a single camera attached to an X-Y translation stage. We show that
an appropriate phase retrieval based reconstruction algorithm can be used to
effectively recover the lost high resolution details from the multiple low
resolution acquired images. We analyze the effects of noise, required degree of
image overlap, and the effect of increasing synthetic aperture size on the
reconstructed image quality. We show that coherent camera arrays have the
potential to greatly improve imaging performance. Our simulations show
resolution gains of 10x and more are achievable. Furthermore, experimental
results from our proof-of-concept systems show resolution gains of 4x-7x for
real scenes. Finally, we introduce and analyze in simulation a new strategy to
capture macroscopic Fourier Ptychography images in a single snapshot, albeit
using a camera array.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08473</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08473</id><created>2015-10-28</created><updated>2015-12-15</updated><authors><author><keyname>Meiklejohn</keyname><forenames>Christopher S.</forenames></author></authors><title>A Certain Tendency Of The Database Community</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We posit that striving for distributed systems that provide &quot;single system
image&quot; semantics is fundamentally flawed and at odds with how systems operate
in the physical world. We realize the database as an optimization of this
system: a required, essential optimization in practice that facilitates central
data placement and ease of access to participants in a system. We motivate a
new model of computation that is designed to address the problems of
computation over &quot;eventually consistent&quot; information in a large-scale
distributed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08474</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08474</id><created>2015-10-28</created><authors><author><keyname>Yoo</keyname><forenames>Chanyeol</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Control with Probabilistic Signal Temporal Logic</title><categories>cs.SY cs.LO cs.RO</categories><comments>7 pages, submitted to the 2016 American Control Conference (ACC 2016)
  on September, 30, 2015 (under review)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous agents often operate in uncertain environments where their
decisions are made based on beliefs over states of targets. We are interested
in controller synthesis for complex tasks defined over belief spaces. Designing
such controllers is challenging due to computational complexity and the lack of
expressivity of existing specification languages. In this paper, we propose a
probabilistic extension to signal temporal logic (STL) that expresses tasks
over continuous belief spaces. We present an efficient synthesis algorithm to
find a control input that maximises the probability of satisfying a given task.
We validate our algorithm through simulations of an unmanned aerial vehicle
deployed for surveillance and search missions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08480</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08480</id><created>2015-10-28</created><authors><author><keyname>Pavalanathan</keyname><forenames>Umashanthi</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>Emoticons vs. Emojis on Twitter: A Causal Inference Approach</title><categories>cs.CL</categories><comments>In review at AAAI Spring Symposium 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online writing lacks the non-verbal cues present in face-to-face
communication, which provide additional contextual information about the
utterance, such as the speaker's intention or affective state. To fill this
void, a number of orthographic features, such as emoticons, expressive
lengthening, and non-standard punctuation, have become popular in social media
services including Twitter and Instagram. Recently, emojis have been introduced
to social media, and are increasingly popular. This raises the question of
whether these predefined pictographic characters will come to replace earlier
orthographic methods of paralinguistic communication. In this abstract, we
attempt to shed light on this question, using a matching approach from causal
inference to test whether the adoption of emojis causes individual users to
employ fewer emoticons in their text on Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08484</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08484</id><created>2015-10-28</created><authors><author><keyname>Snyder</keyname><forenames>David</forenames></author><author><keyname>Chen</keyname><forenames>Guoguo</forenames></author><author><keyname>Povey</keyname><forenames>Daniel</forenames></author></authors><title>MUSAN: A Music, Speech, and Noise Corpus</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report introduces a new corpus of music, speech, and noise. This dataset
is suitable for training models for voice activity detection (VAD) and
music/speech discrimination. Our corpus is released under a flexible Creative
Commons license. The dataset consists of music from several genres, speech from
twelve languages, and a wide assortment of technical and non-technical noises.
We demonstrate use of this corpus for music/speech discrimination on Broadcast
news and VAD for speaker identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08485</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08485</id><created>2015-10-28</created><authors><author><keyname>Wang</keyname><forenames>Wenhao</forenames></author><author><keyname>Sun</keyname><forenames>Zhi</forenames></author><author><keyname>Kui</keyname><forenames>Ren</forenames></author><author><keyname>Zhu</keyname><forenames>Bocheng</forenames></author><author><keyname>Piao</keyname><forenames>Sixu</forenames></author></authors><title>Wireless Physical-Layer Identification: Modeling and Validation</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wireless physical-layer identification (WPLI) techniques utilize the
unique features of the physical waveforms of wireless signals to identify and
classify authorized devices. As the inherent physical layer features are
difficult to forge, WPLI is deemed as a promising technique for wireless
security solutions. However, as of today it still remains unclear whether
existing WPLI techniques can be applied under real-world requirements and
constraints. In this paper, through both theoretical modeling and experiment
validation, the reliability and differentiability of WPLI techniques are
rigorously evaluated, especially under the constraints of state-of-art wireless
devices, real operation environments, as well as wireless protocols and
regulations. Specifically, a theoretical model is first established to
systematically describe the complete procedure of WPLI. More importantly, the
proposed model is then implemented to thoroughly characterize various WPLI
techniques that utilize the spectrum features coming from the non-linear
RF-front-end, under the influences from different transmitters, receivers, and
wireless channels. Subsequently, the limitations of existing WPLI techniques
are revealed and evaluated in details using both the developed theoretical
model and in-lab experiments. The real-world requirements and constraints are
characterized along each step in WPLI, including i) the signal processing at
the transmitter (device to be identified), ii) the various physical layer
features that originate from circuits, antenna, and environments, iii) the
signal propagation in various wireless channels, iv) the signal reception and
processing at the receiver (the identifier), and v) the fingerprint extraction
and classification at the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08487</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08487</id><created>2015-10-28</created><authors><author><keyname>Rao</keyname><forenames>Adithya</forenames></author><author><keyname>Spasojevic</keyname><forenames>Nemanja</forenames></author><author><keyname>Li</keyname><forenames>Zhisheng</forenames></author><author><keyname>DSouza</keyname><forenames>Trevor</forenames></author></authors><title>Klout Score: Measuring Influence Across Multiple Social Networks</title><categories>cs.SI cs.IR</categories><comments>8 pages, 2015 IEEE International Big Data Conference - Workshop on
  Mining Big Data in Social Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present the Klout Score, an influence scoring system that
assigns scores to 750 million users across 9 different social networks on a
daily basis. We propose a hierarchical framework for generating an influence
score for each user, by incorporating information for the user from multiple
networks and communities. Over 3600 features that capture signals of
influential interactions are aggregated across multiple dimensions for each
user. The features are scalably generated by processing over 45 billion
interactions from social networks every day, as well as by incorporating
factors that indicate real world influence. Supervised models trained from
labeled data determine the weights for features, and the final Klout Score is
obtained by hierarchically combining communities and networks. We validate the
correctness of the score by showing that users with higher scores are able to
spread information more effectively in a network. Finally, we use several
comparisons to other ranking systems to show that highly influential and
recognizable users across different domains have high Klout scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08490</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08490</id><created>2015-10-28</created><updated>2016-02-12</updated><authors><author><keyname>Civitarese</keyname><forenames>Jamil</forenames></author><author><keyname>Concatto</keyname><forenames>Fernanda</forenames></author><author><keyname>Abreu</keyname><forenames>Cl&#xe1;udio</forenames></author></authors><title>Psychological Determinants and Consequences of Complex Networks</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents two models that exemplify psychological factors as a
determinant and as a consequence of social network characteristics. There is an
endogeneity considered in network formation: while the social experiences have
impacts on people, their current psychological states and traits affect network
evolution. The first model is an agent-based model over Bianconi-Barabasi
networks, used to explain the relation between network size, extroversion, and
age of individuals. The second model deals with the emergence of urban tribes
as a consequence of a smaller propensity to communicate with different with
different traits and opinions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08496</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08496</id><created>2015-10-28</created><updated>2016-01-07</updated><authors><author><keyname>Poojary</keyname><forenames>Sudheer</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>A Markovian Approximation of TCP CUBIC</title><categories>cs.NI</categories><comments>Submission for ICC 2016 longer version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive an expression for computing average window size of a
single TCP CUBIC connection under random losses. Throughput expression for TCP
CUBIC has been computed earlier under deterministic periodic packet losses. We
validate this expression theoretically. We then use insights from the
deterministic loss based model to derive an expression for computing average
window size of a single TCP CUBIC connection under random losses. For this
computation, we first consider the sequence of TCP CUBIC window evolution
processes indexed by the drop rate, p and show that with a suitable scaling
this sequence converges to a limiting Markov chain as p tends to 0. The
stationary distribution of the limiting Markov chain is then used to derive the
average window size for small packet error rates. We validate our model and
approximations via simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08498</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08498</id><created>2015-10-28</created><authors><author><keyname>Berger</keyname><forenames>Ulrich</forenames></author><author><keyname>Spreen</keyname><forenames>Dieter</forenames></author></authors><title>A Coinductive Approach to Computing with Compact Sets</title><categories>cs.LO</categories><comments>34 pages</comments><msc-class>03B70, 03F60, 68T15</msc-class><acm-class>F.4.1; I.2.2; I.2.3; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact representations of real numbers such as the signed digit representation
or more generally linear fractional representations or the infinite Gray code
represent real numbers as infinite streams of digits. In earlier work by the
first author it was shown how to extract certified algorithms working with the
signed digit representations from constructive proofs. In this paper we lay the
foundation for doing a similar thing with nonempty compact sets. It turns out
that a representation by streams of finitely many digits is impossible and
instead trees are needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08505</identifier>
 <datestamp>2015-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08505</id><created>2015-10-28</created><updated>2015-11-11</updated><authors><author><keyname>Chi</keyname><forenames>Guanghua</forenames></author><author><keyname>Liu</keyname><forenames>Yu</forenames></author><author><keyname>Wu</keyname><forenames>Zhengwei</forenames></author><author><keyname>Wu</keyname><forenames>Haishan</forenames></author></authors><title>Ghost Cities Analysis Based on Positioning Data in China</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>added references for Case Study; corrected typos; revised argument in
  Introduction; added a sentence to explain the second-tier and third tier
  cities in Result; added two sentences to introduce the background of the
  study in Conclusion; added two people in the Acknowledgements; added a
  coauthor for his contribution in designing the algorithms of home-work
  detection and migration calculation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real estate projects are developed excessively in China in this decade. Many
new housing districts are built, but they far exceed the actual demand in some
cities. These cities with a high housing vacancy rate are called ghost cities.
The real situation of vacant housing areas in China has not been studied in
previous research. This study, using Baidu positioning data, presents the
spatial distribution of the vacant housing areas in China and classifies cities
with a large vacant housing area as cities or tourism sites. To the best of our
knowledge, it is the first time that we detected and analyzed the ghost cities
in China at such fine scale. To understand the human dynamic in ghost cities,
we select one city and one tourism sites as cases to analyze the features of
human dynamics. This study illustrates the capability of big data in sensing
our cities objectively and comprehensively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08506</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08506</id><created>2015-10-28</created><authors><author><keyname>Lin</keyname><forenames>Anthony W.</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong Khanh</forenames></author><author><keyname>R&#xfc;mmer</keyname><forenames>Philipp</forenames></author><author><keyname>Sun</keyname><forenames>Jun</forenames></author></authors><title>Regular Symmetry Patterns (Technical Report)</title><categories>cs.LO</categories><comments>Technical report of VMCAI'16 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetry reduction is a well-known approach for alleviating the state
explosion problem in model checking. Automatically identifying symmetries in
concurrent systems, however, is computationally expensive. We propose a
symbolic framework for capturing symmetry patterns in parameterised systems
(i.e. an infinite family of finite-state systems): two regular word transducers
to represent, respectively, parameterised systems and symmetry patterns. The
framework subsumes various types of symmetry relations ranging from weaker
notions (e.g. simulation preorders) to the strongest notion (i.e.
isomorphisms). Our framework enjoys two algorithmic properties: (1) symmetry
verification: given a transducer, we can automatically check whether it is a
symmetry pattern of a given system, and (2) symmetry synthesis: we can
automatically generate a symmetry pattern for a given system in the form of a
transducer. Furthermore, our symbolic language allows additional constraints
that the symmetry patterns need to satisfy to be easily incorporated in the
verification/synthesis. We show how these properties can help identify symmetry
patterns in examples like dining philosopher protocols, self-stabilising
protocols, and prioritised resource-allocator protocol. In some cases (e.g.
Gries's coffee can problem), our technique automatically synthesises a
safety-preserving finite approximant, which can then be verified for safety
solely using a finite-state model checker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08507</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08507</id><created>2015-10-28</created><authors><author><keyname>Ren</keyname><forenames>Yuwei</forenames></author><author><keyname>Song</keyname><forenames>Yang</forenames></author><author><keyname>Su</keyname><forenames>Xin</forenames></author></authors><title>Low-Complexity Channel Reconstruction Methods Based on SVD-ZF Precoding
  in Massive 3D-MIMO Systems</title><categories>cs.IT math.IT</categories><comments>9 pages, 7 figures, Practical issues such as precoding process in TDD
  mode and analysis of channel reconstruction. Accepted to appear in China
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the low-complexity channel reconstruction methods for
downlink precoding in massive multiple-Input multiple-Output (MIMO) systems.
When the user is allocated less streams than the number of its antennas, the
base station (BS) or user usually utilizes the singular value decomposition
(SVD) to get the effective channels, whose dimension is equal to the number of
streams. This process is called channel reconstruction and done in BS for time
division duplex (TDD) mode. However, with the increasing of antennas in BS, the
computation burden of SVD is getting incredible. Here, we propose a series of
novel low-complexity channel reconstruction methods for downlink precoding in
3D spatial channel model. We consider different correlations between elevation
and azimuth antennas, and divide the large dimensional matrix SVD into two
kinds of small-size matrix SVD. The simulation results show that the proposed
methods only produce less than 10% float computation than the traditional SVD
zero-forcing (SVD-ZF) precoding method, while keeping nearly the same
performance of 1Gbps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08510</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08510</id><created>2015-10-28</created><authors><author><keyname>Greifenberg</keyname><forenames>Timo</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Architectural Consistency Checking in Plugin-Based Software Systems</title><categories>cs.SE</categories><comments>7 pages, 2 figures, 4 listings, European Conference on Software
  Architecture Workshops (ECSAW), pp. 58:1-58:7, Cavtat, Croatia, ACM New York,
  2015</comments><journal-ref>European Conference on Software Architecture Workshops (ECSAW),
  pp. 58:1-58:7, Cavtat, Croatia, ACM New York, 2015</journal-ref><doi>10.1145/2797433.2797493</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manually ensuring that the implementation of a software system is consistent
with the software architecture is a laborious and error-prone task. Thus, a
variety of approaches towards automated consistency checking have been
developed to counteract architecture erosion. However, these approaches lack
means to define and check architectural restrictions concerning plugin
dependencies, which is required for plugin-based software systems. In this
paper, we propose a domain-specific language called Dependency Constraint
Language (DepCoL) to facilitate the definition of constraints concerning plugin
dependencies. Using DepCoL, it is possible to define constraints affecting
groups of plugins, reducing the required specification effort, to formulate
constraints for specific plugins only and to refine constraints. Moreover, we
provide an Eclipse plugin, which checks whether the software system under
development is consistent with the modeled constraints. This enables a seamless
integration into the development process to effortless check consistency during
development of the software system. In this way, developers are informed about
dependency violations immediately and this supports developers in counteracting
architecture erosion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08517</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08517</id><created>2015-10-28</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Fu</keyname><forenames>Hongfei</forenames></author><author><keyname>Novotny</keyname><forenames>Petr</forenames></author><author><keyname>Hasheminezhad</keyname><forenames>Rouzbeh</forenames></author></authors><title>Algorithmic Analysis of Qualitative and Quantitative Termination
  Problems for Affine Probabilistic Programs</title><categories>cs.LO cs.PL</categories><comments>24 pages, full version to the conference paper on POPL 2016</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider termination of probabilistic programs with
real-valued variables. The questions concerned are:
  1. qualitative ones that ask (i) whether the program terminates with
probability 1 (almost-sure termination) and (ii) whether the expected
termination time is finite (finite termination); 2. quantitative ones that ask
(i) to approximate the expected termination time (expectation problem) and (ii)
to compute a bound B such that the probability to terminate after B steps
decreases exponentially (concentration problem).
  To solve these questions, we utilize the notion of ranking supermartingales
which is a powerful approach for proving termination of probabilistic programs.
In detail, we focus on algorithmic synthesis of linear ranking-supermartingales
over affine probabilistic programs (APP's) with both angelic and demonic
non-determinism. An important subclass of APP's is LRAPP which is defined as
the class of all APP's over which a linear ranking-supermartingale exists.
  Our main contributions are as follows. Firstly, we show that the membership
problem of LRAPP (i) can be decided in polynomial time for APP's with at most
demonic non-determinism, and (ii) is NP-hard and in PSPACE for APP's with
angelic non-determinism; moreover, the NP-hardness result holds already for
APP's without probability and demonic non-determinism. Secondly, we show that
the concentration problem over LRAPP can be solved in the same complexity as
for the membership problem of LRAPP. Finally, we show that the expectation
problem over LRAPP can be solved in 2EXPTIME and is PSPACE-hard even for APP's
without probability and non-determinism (i.e., deterministic programs). Our
experimental results demonstrate the effectiveness of our approach to answer
the qualitative and quantitative questions over APP's with at most demonic
non-determinism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08520</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08520</id><created>2015-10-28</created><updated>2015-11-18</updated><authors><author><keyname>Yang</keyname><forenames>Yingzhen</forenames></author><author><keyname>Feng</keyname><forenames>Jiashi</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Learning with $\ell^{0}$-Graph: $\ell^{0}$-Induced Sparse Subspace
  Clustering</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse subspace clustering methods, such as Sparse Subspace Clustering (SSC)
\cite{ElhamifarV13} and $\ell^{1}$-graph \cite{YanW09,ChengYYFH10}, are
effective in partitioning the data that lie in a union of subspaces. Most of
those methods use $\ell^{1}$-norm or $\ell^{2}$-norm with thresholding to
impose the sparsity of the constructed sparse similarity graph, and certain
assumptions, e.g. independence or disjointness, on the subspaces are required
to obtain the subspace-sparse representation, which is the key to their
success. Such assumptions are not guaranteed to hold in practice and they limit
the application of sparse subspace clustering on subspaces with general
location. In this paper, we propose a new sparse subspace clustering method
named $\ell^{0}$-graph. In contrast to the required assumptions on subspaces
for most existing sparse subspace clustering methods, it is proved that
subspace-sparse representation can be obtained by $\ell^{0}$-graph for
arbitrary distinct underlying subspaces almost surely under the mild i.i.d.
assumption on the data generation. We develop a proximal method to obtain the
sub-optimal solution to the optimization problem of $\ell^{0}$-graph with
proved guarantee of convergence. Moreover, we propose a regularized
$\ell^{0}$-graph that encourages nearby data to have similar neighbors so that
the similarity graph is more aligned within each cluster and the graph
connectivity issue is alleviated. Extensive experimental results on various
data sets demonstrate the superiority of $\ell^{0}$-graph compared to other
competing clustering methods, as well as the effectiveness of regularized
$\ell^{0}$-graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08525</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08525</id><created>2015-10-28</created><authors><author><keyname>Alvin</keyname><forenames>Chris</forenames></author><author><keyname>Gulwani</keyname><forenames>Sumit</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Supratik</forenames></author></authors><title>Automatic Synthesis of Geometry Problems for an Intelligent Tutoring
  System</title><categories>cs.AI</categories><comments>A formal version of the accepted AAAI '14 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an intelligent tutoring system, GeoTutor, for Euclidean
Geometry that is automatically able to synthesize proof problems and their
respective solutions given a geometric figure together with a set of properties
true of it. GeoTutor can provide personalized practice problems that address
student deficiencies in the subject matter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08530</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08530</id><created>2015-10-28</created><authors><author><keyname>Chen</keyname><forenames>Jiachen</forenames></author><author><keyname>Arumaithurai</keyname><forenames>Mayutan</forenames></author><author><keyname>Fu</keyname><forenames>Xiaoming</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>K. K.</forenames></author></authors><title>SAID: A Control Protocol for Scalable and Adaptive Information
  Dissemination in ICN</title><categories>cs.NI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information dissemination applications (video, news, social media, etc.) with
large number of receivers need to be efficient but also have limited loss
tolerance. The new Information-Centric Networks (ICN) paradigm offers an
alternative approach for reliably delivering data by naming content and
exploiting data available at any intermediate point (e.g., caches). However,
receivers are often heterogeneous, with widely varying receive rates. When
using existing ICN congestion control mechanisms with in-sequence delivery, a
particularly thorny problem of receivers going out-of-sync results in
inefficiency and unfairness with heterogeneous receivers. We argue that
separating reliability from congestion control leads to more scalable,
efficient and fair data dissemination, and propose SAID, a Control Protocol for
Scalable and Adaptive Information Dissemination in ICN. To maximize the amount
of data transmitted at the first attempt, receivers request any next packet
(ANP) of a flow instead of next-in-sequence packet, independent of the
provider's transmit rate. This allows providers to transmit at an
application-efficient rate, without being limited by the slower receivers. SAID
ensures reliable delivery to all receivers eventually, by cooperative repair,
while preserving privacy without unduly trusting other receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08531</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08531</id><created>2015-10-28</created><updated>2015-10-31</updated><authors><author><keyname>Tu</keyname><forenames>Guan-Hua</forenames></author><author><keyname>Li</keyname><forenames>Yuanjie</forenames></author><author><keyname>Peng</keyname><forenames>Chunyi</forenames></author><author><keyname>Li</keyname><forenames>Chi-Yu</forenames></author><author><keyname>Raza</keyname><forenames>Muhammad Taqi</forenames></author><author><keyname>Tseng</keyname><forenames>Hsiao-Yun</forenames></author><author><keyname>Lu</keyname><forenames>Songwu</forenames></author></authors><title>New Threats to SMS-Assisted Mobile Internet Services from 4G LTE:
  Lessons Learnt from Distributed Mobile-Initiated Attacks towards Facebook and
  Other Services</title><categories>cs.CR</categories><comments>16 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Internet is becoming the norm. With more personalized mobile devices
in hand, many services choose to offer alternative, usually more convenient,
approaches to authenticating and delivering the content between mobile users
and service providers. One main option is to use SMS (i.e., short messaging
service). Such carrier-grade text service has been widely used to assist
versatile mobile services, including social networking, banking, to name a few.
Though the text service can be spoofed via certain Internet text service
providers which cooperated with carriers, such attacks haven well studied and
defended by industry due to the efforts of research community. However, as
cellular network technology advances to the latest IP-based 4G LTE, we find
that these mobile services are somehow exposed to new threats raised by this
change, particularly on 4G LTE Text service (via brand-new distributed
Mobile-Initiated Spoofed SMS attack which is not available in legacy 2G/3G
systems). The reason is that messaging service over LTE shifts from the
circuit-switched (CS) design to the packet-switched (PS) paradigm as 4G LTE
supports PS only. Due to this change, 4G LTE Text Service becomes open to
access. However, its shields to messaging integrity and user authentication are
not in place. As a consequence, such weaknesses can be exploited to launch
attacks (e.g., hijack Facebook accounts) against a targeted individual, a large
scale of mobile users and even service providers, from mobile devices. Current
defenses for Internet-Initiated Spoofed SMS attacks cannot defend the
unprecedented attack. Our study shows that 53 of 64 mobile services over 27
industries are vulnerable to at least one threat. We validate these
proof-of-concept attacks in one major US carrier which supports more than 100
million users. We finally propose quick fixes and discuss security insights and
lessons we have learnt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08532</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08532</id><created>2015-10-28</created><authors><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>The Singular Value Decomposition, Applications and Beyond</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The singular value decomposition (SVD) is not only a classical theory in
matrix computation and analysis, but also is a powerful tool in machine
learning and modern data analysis. In this tutorial we first study the basic
notion of SVD and then show the central role of SVD in matrices. Using
majorization theory, we consider variational principles of singular values and
eigenvalues. Built on SVD and a theory of symmetric gauge functions, we discuss
unitarily invariant norms, which are then used to formulate general results for
matrix low rank approximation. We study the subdifferentials of unitarily
invariant norms. These results would be potentially useful in many machine
learning problems such as matrix completion and matrix data classification.
Finally, we discuss matrix low rank approximation and its recent developments
such as randomized SVD, approximate matrix multiplication, CUR decomposition,
and Nystrom approximation. Randomized algorithms are important approaches to
large scale SVD as well as fast matrix computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08535</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08535</id><created>2015-10-28</created><authors><author><keyname>Wang</keyname><forenames>Qichun</forenames></author></authors><title>On the Covering Radius of the Second Order Reed-Muller Code of Length
  128</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1981, Schatz proved that the covering radius of the binary Reed-Muller
code $RM(2,6)$ is 18. For $RM(2,7)$, we only know that its covering radius is
between 40 and 44. In this paper, we prove that the covering radius of the
binary Reed-Muller code $RM(2,7)$ is at most 42. Moreover, we give a sufficient
and necessary condition for Boolean functions of 7-variable to achieve the
second-order nonlinearity 42.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08542</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08542</id><created>2015-10-28</created><updated>2016-02-26</updated><authors><author><keyname>H&#xe9;bert-Dufresne</keyname><forenames>Laurent</forenames></author><author><keyname>Grochow</keyname><forenames>Joshua A.</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author></authors><title>Multi-scale structure and topological anomaly detection via a new
  network statistic: The onion decomposition</title><categories>physics.soc-ph cond-mat.dis-nn cs.DM cs.SI math.CO</categories><comments>8 pages manuscript, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new network statistic that measures diverse structural
properties at the micro-, meso-, and macroscopic scales, while still being easy
to compute and easy to interpret at a glance. Our statistic, the onion
spectrum, is based on the onion decomposition, which refines the k-core
decomposition, a standard network fingerprinting method. The onion spectrum is
exactly as easy to compute as the k-cores: It is based on the stages at which
each vertex gets removed from a graph in the standard algorithm for computing
the k-cores. But the onion spectrum reveals much more information about a
network, and at multiple scales; for example, it can be used to quantify node
heterogeneity, degree correlations, centrality, and tree- or lattice-likeness
of the whole network as well as of each k-core. Furthermore, unlike the k-core
decomposition, the combined degree-onion spectrum immediately gives a clear
local picture of the network around each node which allows the detection of
interesting subgraphs whose topological structure differs from the global
network organization. This local description can also be leveraged to easily
generate samples from the ensemble of networks with a given joint degree-onion
distribution. We demonstrate the utility of the onion spectrum for
understanding both static and dynamic properties on several standard graph
models and on many real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08544</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08544</id><created>2015-10-28</created><updated>2015-11-26</updated><authors><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>Empirical Output Distribution of Good Delay-Limited Codes for
  Quasi-Static Fading Channels</title><categories>cs.IT math.IT</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers delay-limited communication over quasi-static fading
channels under a long-term power constraint. A sequence of length-$n$
delay-limited codes for a quasi-static fading channel is said to be
capacity-achieving if the codes achieve the delay-limited capacity, which is
defined to be the maximum rate achievable by delay-limited codes. The
delay-limited capacity is sometimes referred to as the zero-outage capacity in
wireless communications. The delay-limited capacity is the appropriate choice
of performance measure for delay-sensitive applications such as voice and video
over fading channels. It is shown that for any sequence of capacity-achieving
delay-limited codes with vanishing error probabilities, the normalized relative
entropy between the output distribution induced by the length-$n$ code and the
$n$-fold product of the capacity-achieving output distribution, denoted by
$\frac{1}{n}D(p_{Y^n}\|p_{Y^n}^*)$, converges to zero. Additionally, we extend
our convergence result to capacity-achieving delay-limited codes with
non-vanishing error probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08545</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08545</id><created>2015-10-28</created><authors><author><keyname>Habib</keyname><forenames>Salman</forenames></author><author><keyname>Roser</keyname><forenames>Robert</forenames></author><author><keyname>LeCompte</keyname><forenames>Tom</forenames></author><author><keyname>Marshall</keyname><forenames>Zach</forenames></author><author><keyname>Borgland</keyname><forenames>Anders</forenames></author><author><keyname>Viren</keyname><forenames>Brett</forenames></author><author><keyname>Nugent</keyname><forenames>Peter</forenames></author><author><keyname>Asai</keyname><forenames>Makoto</forenames></author><author><keyname>Bauerdick</keyname><forenames>Lothar</forenames></author><author><keyname>Finkel</keyname><forenames>Hal</forenames></author><author><keyname>Gottlieb</keyname><forenames>Steve</forenames></author><author><keyname>Hoeche</keyname><forenames>Stefan</forenames></author><author><keyname>Sheldon</keyname><forenames>Paul</forenames></author><author><keyname>Vay</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Kirby</keyname><forenames>Michael</forenames></author><author><keyname>Patton</keyname><forenames>Simon</forenames></author><author><keyname>Potekhin</keyname><forenames>Maxim</forenames></author><author><keyname>Yanny</keyname><forenames>Brian</forenames></author><author><keyname>Calafiura</keyname><forenames>Paolo</forenames></author><author><keyname>Dart</keyname><forenames>Eli</forenames></author><author><keyname>Gutsche</keyname><forenames>Oliver</forenames></author><author><keyname>Izubuchi</keyname><forenames>Taku</forenames></author><author><keyname>Lyon</keyname><forenames>Adam</forenames></author><author><keyname>Petravick</keyname><forenames>Don</forenames></author></authors><title>High Energy Physics Forum for Computational Excellence: Working Group
  Reports (I. Applications Software II. Software Libraries and Tools III.
  Systems)</title><categories>physics.comp-ph cs.CE cs.DC hep-ex</categories><comments>72 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing plays an essential role in all aspects of high energy physics. As
computational technology evolves rapidly in new directions, and data throughput
and volume continue to follow a steep trend-line, it is important for the HEP
community to develop an effective response to a series of expected challenges.
In order to help shape the desired response, the HEP Forum for Computational
Excellence (HEP-FCE) initiated a roadmap planning activity with two key
overlapping drivers -- 1) software effectiveness, and 2) infrastructure and
expertise advancement. The HEP-FCE formed three working groups, 1) Applications
Software, 2) Software Libraries and Tools, and 3) Systems (including systems
software), to provide an overview of the current status of HEP computing and to
present findings and opportunities for the desired HEP computational roadmap.
The final versions of the reports are combined in this document, and are
presented along with introductory material.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08546</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08546</id><created>2015-10-28</created><authors><author><keyname>Gilbert</keyname><forenames>Seth</forenames></author><author><keyname>Liu</keyname><forenames>Xiao</forenames></author><author><keyname>Yu</keyname><forenames>Haifeng</forenames></author></authors><title>On Differentially Private Online Collaborative Recommendation Systems</title><categories>cs.CR cs.DS</categories><comments>35 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In collaborative recommendation systems, privacy may be compromised, as
users' opinions are used to generate recommendations for others. In this paper,
we consider an online collaborative recommendation system, and we measure
users' privacy in terms of the standard differential privacy. We give the first
quantitative analysis of the trade-offs between recommendation quality and
users' privacy in such a system by showing a lower bound on the best achievable
privacy for any non-trivial algorithm, and proposing a near-optimal algorithm.
From our results, we find that there is actually little trade-off between
recommendation quality and privacy for any non-trivial algorithm. Our results
also identify the key parameters that determine the best achievable privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08551</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08551</id><created>2015-10-28</created><authors><author><keyname>Nip</keyname><forenames>Kameng</forenames></author><author><keyname>Wang</keyname><forenames>Zhenbo</forenames></author><author><keyname>Wang</keyname><forenames>Zizhuo</forenames></author></authors><title>Scheduling under Linear Constraints</title><categories>cs.DS</categories><comments>21 pages</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a parallel machine scheduling problem in which the processing
times of jobs are not given in advance but are determined by a system of linear
constraints. The objective is to minimize the makespan, i.e., the maximum job
completion time among all feasible choices. This novel problem is motivated by
various real-world application scenarios. We discuss the computational
complexity and algorithms for various settings of this problem. In particular,
we show that if there is only one machine with an arbitrary number of linear
constraints, or there is an arbitrary number of machines with no more than two
linear constraints, or both the number of machines and the number of linear
constraints are fixed constants, then the problem is polynomial-time solvable
via solving a series of linear programming problems. If both the number of
machines and the number of constraints are inputs of the problem instance, then
the problem is NP-Hard. We further propose several approximation algorithms for
the latter case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08554</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08554</id><created>2015-10-28</created><updated>2016-01-11</updated><authors><author><keyname>Ruoti</keyname><forenames>Scott</forenames></author><author><keyname>Andersen</keyname><forenames>Jeff</forenames></author><author><keyname>Heidbrink</keyname><forenames>Scott</forenames></author><author><keyname>O'Neil</keyname><forenames>Mark</forenames></author><author><keyname>Vaziripour</keyname><forenames>Elham</forenames></author><author><keyname>Wu</keyname><forenames>Justin</forenames></author><author><keyname>Zappala</keyname><forenames>Daniel</forenames></author><author><keyname>Seamons</keyname><forenames>Kent</forenames></author></authors><title>&quot;We're on the Same Page&quot;: A Usability Study of Secure Email Using Pairs
  of Novice Users</title><categories>cs.CR cs.HC</categories><comments>34th Annual ACM Conference on Human Factors in Computing Systems (CHI
  2016)</comments><acm-class>H.1.2; H.5.2</acm-class><doi>10.1145/2858036.2858400</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure email is increasingly being touted as usable by novice users, with a
push for adoption based on recent concerns about government surveillance. To
determine whether secure email is for grassroots adoption, we employ a
laboratory user study that recruits pairs of novice to install and use several
of the latest systems to exchange secure messages. We present quantitative and
qualitative results from 25 pairs of novice users as they use Pwm, Tutanota,
and Virtru. Participants report being more at ease with this type of study and
better able to cope with mistakes since both participants are &quot;on the same
page&quot;. We find that users prefer integrated solutions over depot-based
solutions, and that tutorials are important in helping first-time users. Hiding
the details of how a secure email system provides security can lead to a lack
of trust in the system. Participants expressed a desire to use secure email,
but few wanted to use it regularly and most were unsure of when they might use
it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08555</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08555</id><created>2015-10-28</created><updated>2016-01-13</updated><authors><author><keyname>Ruoti</keyname><forenames>Scott</forenames></author><author><keyname>Andersen</keyname><forenames>Jeff</forenames></author><author><keyname>Zappala</keyname><forenames>Daniel</forenames></author><author><keyname>Seamons</keyname><forenames>Kent</forenames></author></authors><title>Why Johnny Still, Still Can't Encrypt: Evaluating the Usability of a
  Modern PGP Client</title><categories>cs.CR cs.HC</categories><comments>This is the Mailvelope study discussed in the CHI 2016 paper
  arXiv:1510.08554 &quot;We're on the Same Page&quot;: A Usability Study of Secure Email
  Using Pairs of Novice Users&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the results of a laboratory study involving Mailvelope, a
modern PGP client that integrates tightly with existing webmail providers. In
our study, we brought in pairs of participants and had them attempt to use
Mailvelope to communicate with each other. Our results shown that more than a
decade and a half after \textit{Why Johnny Can't Encrypt}, modern PGP tools are
still unusable for the masses. We finish with a discussion of pain points
encountered using Mailvelope, and discuss what might be done to address them in
future PGP systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08564</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08564</id><created>2015-10-29</created><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames></author></authors><title>Build your own clarithmetic I</title><categories>cs.LO</categories><msc-class>03F50, 03D75, 03D15, 68Q10, 68T27, 68T30</msc-class><acm-class>F.1.1; F.1.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clarithmetics are number theories based on computability logic (see
http://www.csc.villanova.edu/~japaridz/CL/ ). Formulas of these theories
represent interactive computational problems, and their &quot;truth&quot; is understood
as existence of an algorithmic solution. Various complexity constraints on such
solutions induce various versions of clarithmetic. The present paper introduces
a parameterized/schematic version CLA11(P1,P2,P3,P4). By tuning the three
parameters P1,P2,P3 in an essentially mechanical manner, one automatically
obtains sound and complete theories with respect to a wide range of target
tricomplexity classes, i.e. combinations of time (set by P3), space (set by P2)
and so called amplitude (set by P1) complexities. Sound in the sense that every
theorem T of the system represents an interactive number-theoretic
computational problem with a solution from the given tricomplexity class and,
furthermore, such a solution can be automatically extracted from a proof of T.
And complete in the sense that every interactive number-theoretic problem with
a solution from the given tricomplexity class is represented by some theorem of
the system. Furthermore, through tuning the 4th parameter P4, at the cost of
sacrificing recursive axiomatizability but not simplicity or elegance, the
above extensional completeness can be strengthened to intensional completeness,
according to which every formula representing a problem with a solution from
the given tricomplexity class is a theorem of the system. This article is
published in two parts. The present Part I introduces the system and proves its
soundness, while the forthcoming Part II is devoted to a completeness proof and
some corollaries of the main results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08565</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08565</id><created>2015-10-29</created><updated>2015-11-05</updated><authors><author><keyname>Yao</keyname><forenames>Kaisheng</forenames></author><author><keyname>Zweig</keyname><forenames>Geoffrey</forenames></author><author><keyname>Peng</keyname><forenames>Baolin</forenames></author></authors><title>Attention with Intention for a Neural Network Conversation Model</title><categories>cs.NE cs.AI cs.HC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a conversation or a dialogue process, attention and intention play
intrinsic roles. This paper proposes a neural network based approach that
models the attention and intention processes. It essentially consists of three
recurrent networks. The encoder network is a word-level model representing
source side sentences. The intention network is a recurrent network that models
the dynamics of the intention process. The decoder network is a recurrent
network produces responses to the input from the source side. It is a language
model that is dependent on the intention and has an attention mechanism to
attend to particular source side words, when predicting a symbol in the
response. The model is trained end-to-end without labeling data. Experiments
show that this model generates natural responses to user inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08566</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08566</id><created>2015-10-29</created><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames></author></authors><title>Build your own clarithmetic II</title><categories>cs.LO</categories><msc-class>03F50, 03D75, 03D15, 68Q10, 68T27, 68T30</msc-class><acm-class>F.1.1; F.1.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clarithmetics are number theories based on computability logic (see
http://www.csc.villanova.edu/~japaridz/CL/ ). Formulas of these theories
represent interactive computational problems, and their &quot;truth&quot; is understood
as existence of an algorithmic solution. Various complexity constraints on such
solutions induce various versions of clarithmetic. The present paper introduces
a parameterized/schematic version CLA11(P1,P2,P3,P4). By tuning the three
parameters P1,P2,P3 in an essentially mechanical manner, one automatically
obtains sound and complete theories with respect to a wide range of target
tricomplexity classes, i.e. combinations of time (set by P3), space (set by P2)
and so called amplitude (set by P1) complexities. Sound in the sense that every
theorem T of the system represents an interactive number-theoretic
computational problem with a solution from the given tricomplexity class and,
furthermore, such a solution can be automatically extracted from a proof of T.
And complete in the sense that every interactive number-theoretic problem with
a solution from the given tricomplexity class is represented by some theorem of
the system. Furthermore, through tuning the 4th parameter P4, at the cost of
sacrificing recursive axiomatizability but not simplicity or elegance, the
above extensional completeness can be strengthened to intensional completeness,
according to which every formula representing a problem with a solution from
the given tricomplexity class is a theorem of the system. This article is
published in two parts. The previous Part I has introduced the system and
proved its soundness, while the present Part II is devoted to a completeness
proof and some corollaries of the main results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08567</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08567</id><created>2015-10-29</created><authors><author><keyname>Liu</keyname><forenames>Chenxi</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author></authors><title>Location-Based Beamforming for Rician Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a location-based beamforming scheme for wiretap channels, where a
source communicates with a legitimate receiver in the presence of an
eavesdropper. We assume that the source and the eavesdropper are equipped with
multiple antennas, while the legitimate receiver is equipped with a single
antenna. We also assume that all channels are in a Rician fading environment,
the channel state information from the legitimate receiver is perfectly known
at the source, and that the only information on the eavesdropper available at
the source is her location. We first describe how the beamforming vector that
minimizes the secrecy outage probability of the system is obtained,
illustrating its dependence on the eavesdropper's location. We then derive an
easy-to-compute expression for the secrecy outage probability when our proposed
location-based beamforming is adopted. Finally, we investigate the impact
location uncertainty has on the secrecy outage probability, showing how our
proposed solution can still allow for secrecy even when the source has limited
information on the eavesdropper's location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08568</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08568</id><created>2015-10-29</created><authors><author><keyname>Gao</keyname><forenames>Wanru</forenames></author><author><keyname>Nallaperuma</keyname><forenames>Samadhi</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author></authors><title>Feature-Based Diversity Optimization for Problem Instance Classification</title><categories>cs.NE cs.AI</categories><comments>15 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the behaviour of heuristic search methods is a challenge. This
even holds for simple local search methods such as 2-OPT for the Traveling
Salesperson problem. In this paper, we present a general framework that is able
to construct a diverse set of instances that are hard or easy for a given
search heuristic. Such a diverse set is obtained by using an evolutionary
algorithm for constructing hard or easy instances that are diverse with respect
to different features of the underlying problem. Examining the constructed
instance sets, we show that many combinations of two or three features give a
good classification of the TSP instances in terms of whether they are hard to
be solved by 2-OPT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08578</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08578</id><created>2015-10-29</created><updated>2016-01-23</updated><authors><author><keyname>Ganzfried</keyname><forenames>Sam</forenames></author></authors><title>My Reflections on the First Man vs. Machine No-Limit Texas Hold 'em
  Competition</title><categories>cs.GT cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first ever human vs. computer no-limit Texas hold 'em competition took
place from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this
article I present my thoughts on the competition design, agent architecture,
and lessons learned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08583</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08583</id><created>2015-10-29</created><updated>2015-11-05</updated><authors><author><keyname>Tonge</keyname><forenames>Ashwini</forenames></author><author><keyname>Caragea</keyname><forenames>Cornelia</forenames></author></authors><title>Privacy Prediction of Images Shared on Social Media Sites Using Deep
  Features</title><categories>cs.CV cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online image sharing in social media sites such as Facebook, Flickr, and
Instagram can lead to unwanted disclosure and privacy violations, when privacy
settings are used inappropriately. With the exponential increase in the number
of images that are shared online every day, the development of effective and
efficient prediction methods for image privacy settings are highly needed. The
performance of models critically depends on the choice of the feature
representation. In this paper, we present an approach to image privacy
prediction that uses deep features and deep image tags as feature
representations. Specifically, we explore deep features at various neural
network layers and use the top layer (probability) as an auto-annotation
mechanism. The results of our experiments show that models trained on the
proposed deep features and deep image tags substantially outperform baselines
such as those based on SIFT and GIST as well as those that use &quot;bag of tags&quot; as
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08592</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08592</id><created>2015-10-29</created><authors><author><keyname>Bhattaram</keyname><forenames>Roop Kumar</forenames></author><author><keyname>Vaddi</keyname><forenames>Mahesh Babu</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>A Lifting Construction for Scalar Linear Index Codes</title><categories>cs.IT math.IT</categories><comments>9 pages. arXiv admin note: text overlap with arXiv:1510.05435</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with scalar linear index codes for canonical multiple
unicast index coding problems where there is a source with K messages and there
are K receivers each wanting a unique message and having symmetric (with
respect to the receiver index) antidotes (side information). Optimal scalar
linear index codes for several such instances of this class of problems have
been reported in \cite{MRRarXiv}. These codes can be viewed as special cases of
the symmetric unicast index coding problems discussed in \cite{MCJ}. In this
paper a lifting construction is given which constructs a sequence of multiple
unicast index problems starting from a given multiple unicast index coding
problem. Also, it is shown that if an optimal scalar linear index code is known
for the problem given starting problem then optimal scalar linear index codes
can be obtained from the known code for all the problems arising from the
proposed lifting construction. For several of the known classes of multiple
unicast problems our construction is used to obtain several sequences of
multiple unicast problem with optimal scalar linear index codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08612</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08612</id><created>2015-10-29</created><authors><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Ahmadzadeh</keyname><forenames>Arman</forenames></author><author><keyname>Jardin</keyname><forenames>Christophe</forenames></author><author><keyname>Sticht</keyname><forenames>Heinrich</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Channel Estimation Techniques for Diffusion-Based Molecular
  Communications</title><categories>cs.IT math.IT</categories><comments>This paper has been submitted for presentation at IEEE International
  Conference on Communications (ICC) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In molecular communication (MC) systems, the expected number of molecules
observed at the receiver over time after the instantaneous release of molecules
by the transmitter is referred to as the channel impulse response (CIR).
Knowledge of the CIR is needed for the design of detection and equalization
schemes. In this paper, we present a training-based CIR estimation framework
for MC systems which aims at estimating the CIR based on the observed number of
molecules at the receiver due to emission of a sequence of known numbers of
molecules by the transmitter. In particular, we derive maximum likelihood (ML)
and least sum of square errors (LSSE) estimators. We also study the Cramer Rao
(CR) lower bound and training sequence design for the considered system.
Simulation results confirm the analysis and compare the performance of the
proposed estimation techniques with the CR lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08628</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08628</id><created>2015-10-29</created><updated>2016-03-02</updated><authors><author><keyname>Chen</keyname><forenames>Jianfei</forenames></author><author><keyname>Li</keyname><forenames>Kaiwei</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>Wenguang</forenames></author></authors><title>WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet
  Allocation</title><categories>stat.ML cs.DC cs.IR cs.LG</categories><acm-class>H.3.4; G.3; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing efficient and scalable algorithms for Latent Dirichlet Allocation
(LDA) is of wide interest for many applications. Previous work has developed an
O(1) Metropolis-Hastings sampling method for each token. However, the
performance is far from being optimal due to random accesses to the parameter
matrices and frequent cache misses.
  In this paper, we first carefully analyze the memory access efficiency of
existing algorithms for LDA by the scope of random access, which is the size of
the memory region in which random accesses fall, within a short period of time.
We then develop WarpLDA, an LDA sampler which achieves both the best O(1) time
complexity per token and the best O(K) scope of random access. Our empirical
results in a wide range of testing conditions demonstrate that WarpLDA is
consistently 5-15x faster than the state-of-the-art Metropolis-Hastings based
LightLDA, and is comparable or faster than the sparsity aware F+LDA. With
WarpLDA, users can learn up to one million topics from hundreds of millions of
documents in a few hours, at an unprecedentedly throughput of 11G tokens per
second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08636</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08636</id><created>2015-10-29</created><authors><author><keyname>Lu</keyname><forenames>Zhenliang</forenames></author><author><keyname>Zhu</keyname><forenames>Shixin</forenames></author></authors><title>$\mathbb{Z}_p\mathbb{Z}_p[u]$-additive codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study $\mathbb{Z}_p\mathbb{Z}_p[u]$-additive codes, where
$p$ is prime and $u^{2}=0$. In particular, we determine a Gray map from $
\mathbb{Z}_p\mathbb{Z}_p[u]$ to $\mathbb{Z}_p^{ \alpha+2 \beta}$ and study
generator and parity check matrices for these codes. We prove that a Gray map
$\Phi$ is a distance preserving map from ($\mathbb{Z}_p\mathbb{Z}_p[u]$,Gray
distance) to ($\mathbb{Z}_p^{\alpha+2\beta}$,Hamming distance), it is a weight
preserving map as well. Furthermore we study the structure of
$\mathbb{Z}_p\mathbb{Z}_p[u]$-additive cyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08642</identifier>
 <datestamp>2015-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08642</id><created>2015-10-29</created><authors><author><keyname>Kouya</keyname><forenames>Tomonori</forenames></author></authors><title>Performance evaluation of multiple precision matrix multiplications
  using parallelized Strassen and Winograd algorithms</title><categories>math.NA cs.MS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that Strassen and Winograd algorithms can reduce the
computational costs associated with dense matrix multiplication. We have
already shown that they are also very effective for software-based multiple
precision floating-point arithmetic environments such as the MPFR/GMP library.
In this paper, we show that we can obtain the same effectiveness for
double-double (DD) and quadruple-double (QD) environments supported by the QD
library, and that parallelization can increase the speed of these multiple
precision matrix multiplications. Finally, we demonstrate that our implemented
parallelized Strassen and Winograd algorithms can increase the speed of
parallelized LU decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08646</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08646</id><created>2015-10-29</created><updated>2015-11-01</updated><authors><author><keyname>Mayer</keyname><forenames>Wilfried</forenames></author><author><keyname>Zauner</keyname><forenames>Aaron</forenames></author><author><keyname>Schmiedecker</keyname><forenames>Martin</forenames></author><author><keyname>Huber</keyname><forenames>Markus</forenames></author></authors><title>No Need for Black Chambers: Testing TLS in the E-mail Ecosystem at Large</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  TLS is the most widely used cryptographic protocol on the Internet. While
many recent studies focused on its use in HTTPS, none so far analyzed TLS usage
in e-mail related protocols, which often carry highly sensitive information.
Since end-to-end encryption mechanisms like PGP are seldomly used, today
confidentiality in the e-mail ecosystem is mainly based on the encryption of
the transport layer. A well-positioned attacker may be able to intercept
plaintext passively and at global scale. In this paper we are the first to
present a scalable methodology to assess the state of security mechanisms in
the e-mail ecosystem using commodity hardware and open-source software. We draw
a comprehensive picture of the current state of every e-mail related TLS
configuration for the entire IPv4 range. We collected and scanned a massive
data-set of 20 million IP/port combinations of all related protocols (SMTP,
POP3, IMAP) and legacy ports. Over a time span of approx. three months we
conducted more than 10 billion TLS handshakes. Additionally, we show that
securing server-to-server communication using e.g. SMTP is inherently more
difficult than securing client-to-server communication. Lastly, we analyze the
volatility of TLS certificates and trust anchors in the e-mail ecosystem and
argue that while the overall trend points in the right direction, there are
still many steps needed towards secure e-mail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08656</identifier>
 <datestamp>2015-11-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08656</id><created>2015-10-29</created><authors><author><keyname>Samorodnitsky</keyname><forenames>Alex</forenames></author></authors><title>The &quot;Most informative boolean function&quot; conjecture holds for high noise</title><categories>cs.IT math.CO math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the &quot;Most informative boolean function&quot; conjecture of Courtade and
Kumar for high noise $\epsilon \ge 1/2 - \delta$, for some absolute constant
$\delta &gt; 0$.
  Namely, if $X$ is uniformly distributed in $\{0,1\}^n$ and $Y$ is obtained by
flipping each coordinate of $X$ independently with probability $\epsilon$,
then, provided $\epsilon \ge 1/2 - \delta$, for any boolean function $f$ holds
$I(f(X);Y) \le 1 - H(\epsilon)$. This conjecture was previously known to hold
only for balanced functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08660</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08660</id><created>2015-10-29</created><updated>2015-11-26</updated><authors><author><keyname>Kahou</keyname><forenames>Samira Ebrahimi</forenames></author><author><keyname>Michalski</keyname><forenames>Vincent</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author></authors><title>RATM: Recurrent Attentive Tracking Model</title><categories>cs.LG</categories><comments>Under review as a conference paper at ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents an attention based approach to tracking objects in video.
A recurrent neural network is trained to predict the position of an object in
the video at time t+1 given a series of selective glimpses at times 1 to t.
Glimpses are selected based on a differentiable (soft-)attention mechanism,
which makes it possible to train the model end-to-end using standard stochastic
gradient descent. Experiments on artificial data-sets show the importance of
various design choices and that the model is able to perform simple tracking
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08662</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08662</id><created>2015-10-29</created><updated>2015-12-01</updated><authors><author><keyname>Laan</keyname><forenames>Steven</forenames></author><author><keyname>Marx</keyname><forenames>Maarten</forenames></author><author><keyname>Mokken</keyname><forenames>Robert J.</forenames></author></authors><title>Close Communities in Social Networks: Boroughs and 2-Clubs</title><categories>cs.SI physics.soc-ph</categories><comments>Keywords: Social networks, close communication, close communities,
  boroughs, 2-clubs, diameter 2, ego-networks</comments><msc-class>91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of close communication, contacts and association in social
networks is studied in the form of maximal subgraphs of diameter 2 (2-clubs),
corresponding to three types of close communities: hamlets, social circles and
coteries. The concept of borough of a graph is defined and introduced. Each
borough is a chained union of 2-clubs of the network and any 2-club of the
network belongs to one borough. Thus the set of boroughs of a network, together
with the 2-clubs held by them, are shown to contain the structure of close
communication in a network. Applications are given with examples from real
world network data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08663</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08663</id><created>2015-10-29</created><authors><author><keyname>Elvey-Price</keyname><forenames>Andrew</forenames></author><author><keyname>Guttmann</keyname><forenames>Anthony J</forenames></author></authors><title>Permutations sortable by two stacks in series</title><categories>math.CO cs.DS</categories><comments>16 pages, 7 figures</comments><msc-class>05Axx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have studied the problem of the number of permutations that can be sorted
by two stacks in series. We do this by first counting all such permutations of
length less than 20 exactly, then using a numerical technique to obtain eleven
further coefficients approximately. Analysing these coefficients by a variety
of methods we conclude that the OGF behaves as $$S(z) \sim A (1 - \mu \cdot
z)^\gamma,$$ where $\mu =12.4 \pm 0.1,$ $\gamma= 1.5 \pm 0.3,$ and $A \approx
0.026$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08674</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08674</id><created>2015-10-29</created><updated>2015-11-05</updated><authors><author><keyname>Mokken</keyname><forenames>Robert J.</forenames></author><author><keyname>Laan</keyname><forenames>Steven</forenames></author></authors><title>Close Communication and 2-Clubs in Corporate Networks: Europe 2010</title><categories>cs.SI</categories><comments>Keywords: social networks, corporate networks, interlocking
  directorates, close communication, 2-clubs, social circle, hamlet, coterie,
  borough, pivot</comments><msc-class>91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Corporate networks, as induced by interlocking directorates between
corporations, provide structures of personal communication at the level of
their boards. This paper studies such networks from a perspective of close
communication in sub-networks, where each pair of nodes (boards of a
corporation) are either neighbours, or have a common neighbour. These
correspond to subgraphs of diameter at most 2, designated by us earlier as
2-clubs, with three types (coteries, social circles and hamlets) as degrees of
close communication in social networks, within the concept of boroughs of a
network. Boroughs are maximal areas and containers of close communication
between nodes of a network. This framework is applied in this paper to an
analysis of corporate board interlocks between the top 300 European
corporations 2010, as studied by Heemkerk (2013), with data provided by him for
that purpose. The paper gives results for several perspectives of close
communication in the European corporate network of 2010, a year close to the
global crash of 2008, as a further elaboration of those given in Heemskerk
(2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08692</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08692</id><created>2015-10-29</created><authors><author><keyname>Shang</keyname><forenames>Xiaocheng</forenames></author><author><keyname>Zhu</keyname><forenames>Zhanxing</forenames></author><author><keyname>Leimkuhler</keyname><forenames>Benedict</forenames></author><author><keyname>Storkey</keyname><forenames>Amos J.</forenames></author></authors><title>Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale
  Bayesian Sampling</title><categories>stat.ML cs.LG</categories><comments>Advances in Neural Information Processing Systems (NIPS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte Carlo sampling for Bayesian posterior inference is a common approach
used in machine learning. The Markov Chain Monte Carlo procedures that are used
are often discrete-time analogues of associated stochastic differential
equations (SDEs). These SDEs are guaranteed to leave invariant the required
posterior distribution. An area of current research addresses the computational
benefits of stochastic gradient methods in this setting. Existing techniques
rely on estimating the variance or covariance of the subsampling error, and
typically assume constant variance. In this article, we propose a
covariance-controlled adaptive Langevin thermostat that can effectively
dissipate parameter-dependent noise while maintaining a desired target
distribution. The proposed method achieves a substantial speedup over popular
alternative schemes for large-scale machine learning applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08713</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08713</id><created>2015-10-26</created><authors><author><keyname>Batra</keyname><forenames>Nipun</forenames></author><author><keyname>Baijal</keyname><forenames>Rishi</forenames></author><author><keyname>Singh</keyname><forenames>Amarjeet</forenames></author><author><keyname>Whitehouse</keyname><forenames>Kamin</forenames></author></authors><title>How good is good enough? Re-evaluating the bar for energy disaggregation</title><categories>cs.LG</categories><comments>Under submission to Percom 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the early 1980s, the research community has developed ever more
sophisticated algorithms for the problem of energy disaggregation, but despite
decades of research, there is still a dearth of applications with demonstrated
value. In this work, we explore a question that is highly pertinent to this
research community: how good does energy disaggregation need to be in order to
infer characteristics of a household? We present novel techniques that use
unsupervised energy disaggregation to predict both household occupancy and
static properties of the household such as size of the home and number of
occupants. Results show that basic disaggregation approaches performs up to 30%
better at occupancy estimation than using aggregate power data alone, and are
up to 10% better at estimating static household characteristics. These results
show that even rudimentary energy disaggregation techniques are sufficient for
improved inference of household characteristics. To conclude, we re-evaluate
the bar set by the community for energy disaggregation accuracy and try to
answer the question &quot;how good is good enough?&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08748</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08748</id><created>2015-10-29</created><updated>2016-01-19</updated><authors><author><keyname>Bille</keyname><forenames>Philip</forenames></author><author><keyname>G&#xf8;rtz</keyname><forenames>Inge Li</forenames></author><author><keyname>Skjoldjensen</keyname><forenames>Frederik Rye</forenames></author></authors><title>Subsequence Automata with Default Transitions</title><categories>cs.FL cs.DS</categories><comments>Corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a string of length $n$ with characters from an alphabet of size
$\sigma$. The \emph{subsequence automaton} of $S$ (often called the
\emph{directed acyclic subsequence graph}) is the minimal deterministic finite
automaton accepting all subsequences of $S$. A straightforward construction
shows that the size (number of states and transitions) of the subsequence
automaton is $O(n\sigma)$ and that this bound is asymptotically optimal.
  In this paper, we consider subsequence automata with \emph{default
transitions}, that is, special transitions to be taken only if none of the
regular transitions match the current character, and which do not consume the
current character. We show that with default transitions, much smaller
subsequence automata are possible, and provide a full trade-off between the
size of the automaton and the \emph{delay}, i.e., the maximum number of
consecutive default transitions followed before consuming a character.
  Specifically, given any integer parameter $k$, $1 &lt; k \leq \sigma$, we
present a subsequence automaton with default transitions of size
$O(nk\log_{k}\sigma)$ and delay $O(\log_k \sigma)$. Hence, with $k = 2$ we
obtain an automaton of size $O(n \log \sigma)$ and delay $O(\log \sigma)$. On
the other extreme, with $k = \sigma$, we obtain an automaton of size $O(n
\sigma)$ and delay $O(1)$, thus matching the bound for the standard subsequence
automaton construction. Finally, we generalize the result to multiple strings.
The key component of our result is a novel hierarchical automata construction
of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08779</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08779</id><created>2015-10-29</created><updated>2016-02-22</updated><authors><author><keyname>DasGupta</keyname><forenames>Bhaskar</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Mobasheri</keyname><forenames>Nasim</forenames></author><author><keyname>Yahyanejad</keyname><forenames>Farzaneh</forenames></author></authors><title>Effect of Gromov-hyperbolicity Parameter on Cuts and Expansions in
  Graphs and Some Algorithmic Implications</title><categories>cs.CC cs.DM math.CO</categories><comments>Includes changes: Clarified that motivation is not just to study
  hyperbolic graphs (constant \delta), but study as \delta\ varies with n, in
  which case topology can be far from tree. Noted that vulnerability problem is
  network design application where greedy fails. Clarified that SSE solution
  works for non-constant \delta\ if degree is restricted. Added to reference
  seminal result of Kleinberg</comments><msc-class>68Q25, 68W25, 68W40, 05C85</msc-class><acm-class>F.2.2; G.2.2; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\delta$-hyperbolic graphs, originally conceived by Gromov in 1987, include
non-trivial interesting classes of &quot;non-expander&quot; graphs; for fixed $\delta$,
such graphs are simply called hyperbolic graphs. Our goal in this paper is to
study the effect of the hyperbolicity measure $\delta$ on expansion and
cut-size bounds on graphs (here $\delta$ need not be a constant, i.e., the
graph is not necessarily hyperbolic), and investigate up to what values of
$\delta$ these results could provide improved approximation algorithms for
related combinatorial problems. To this effect, we provide the following
results.
  (a) We provide constructive bounds on node expansions and cut-sizes for
$\delta$-hyperbolic graphs as a function of $\delta$, and show that witnesses
for such non-expansion or cut-size can be computed efficiently in polynomial
time. To the best of our knowledge, these are the first such constructive
bounds proven. We also show how to find a large family of s-t cuts with small
number of cut-edges when s and t are sufficiently far apart.
  (b) We also provide the following algorithmic consequences of these bounds
and their related proof techniques for a few problems related to cuts and paths
for $\delta$-hyperbolic graphs (where $\delta$ need not necessarily a constant
but may be a function f of the number of nodes, the exact nature of growth of f
depends on the problem considered):
  (b-1) We provide improved approximation algorithms for minimizing the number
of bottleneck edges that arises in network design applications. En route, we
also formulate the hitting set problem of size-constrained cuts and show a
connection between approximability issues of these two problems.
  (b-2) We provide a polynomial-time solution for a type of small-set expansion
problem that arises in the investigation of unique games conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08786</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08786</id><created>2015-10-29</created><authors><author><keyname>L&#xfc;ck</keyname><forenames>Martin</forenames></author></authors><title>Quirky Quantifiers: Optimal Models and Complexity of Computation Tree
  Logic</title><categories>cs.LO cs.CC</categories><msc-class>68Q17</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The satisfiability problem of the branching time logic CTL is studied in
terms of computational complexity. Tight upper and lower bounds are provided
for each temporal operator fragment. In parallel the minimal model size is
studied with a suitable notion of minimality. Thirdly, flat CTL is
investigated, i.e., formulas with very low temporal operator nesting depth. A
sharp dichotomy is shown in terms of complexity and optimal models: Temporal
depth one has low expressive power, while temporal depth two is equivalent to
full CTL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08789</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08789</id><created>2015-10-29</created><updated>2015-10-30</updated><authors><author><keyname>Johnston</keyname><forenames>Travis</forenames></author><author><keyname>Zhang</keyname><forenames>Boyu</forenames></author><author><keyname>Liwo</keyname><forenames>Adam</forenames></author><author><keyname>Crivelli</keyname><forenames>Silvia</forenames></author><author><keyname>Taufer</keyname><forenames>Michela</forenames></author></authors><title>In-Situ Data Analysis of Protein Folding Trajectories</title><categories>cs.CE cs.DC q-bio.BM</categories><comments>40 pages, 15 figures, this paper is presently in the format request
  of the journal to which it was submitted for publication</comments><msc-class>68U20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transition from petascale to exascale computers is characterized by
substantial changes in the computer architectures and technologies. The
research community relying on computational simulations is being forced to
revisit the algorithms for data generation and analysis due to various
concerns, such as higher degrees of concurrency, deeper memory hierarchies,
substantial I/O and communication constraints. Simulations today typically save
all data to analyze later. Simulations at the exascale will require us to
analyze data as it is generated and save only what is really needed for
analysis, which must be performed predominately in-situ, i.e., executed
sufficiently fast locally, limiting memory and disk usage, and avoiding the
need to move large data across nodes.
  In this paper, we present a distributed method that enables in-situ data
analysis for large protein folding trajectory datasets. Traditional trajectory
analysis methods currently follow a centralized approach that moves the
trajectory datasets to a centralized node and processes the data only after
simulations have been completed. Our method, on the other hand, captures
conformational information in-situ using local data only while reducing the
storage space needed for the part of the trajectory under consideration. This
method processes the input trajectory data in one pass, breaks from the
centralized approach of traditional analysis, avoids the movement of trajectory
data, and still builds the global knowledge on the formation of individual
$\alpha$-helices or $\beta$-strands as trajectory frames are generated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08792</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08792</id><created>2015-10-29</created><authors><author><keyname>Hines</keyname><forenames>Paul D. H.</forenames></author><author><keyname>Blumsack</keyname><forenames>Seth</forenames></author><author><keyname>Schl&#xe4;pfer</keyname><forenames>Markus</forenames></author></authors><title>Centralized versus Decentralized Infrastructure Networks</title><categories>physics.soc-ph cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While many large infrastructure networks, such as power, water, and natural
gas systems, have similar physical properties governing flows, these systems
tend to have distinctly different sizes and topological structures. This paper
seeks to understand how these different size-scales and topological features
can emerge from relatively simple design principles. Specifically, we seek to
describe the conditions under which it is optimal to build decentralized
network infrastructures, such as a microgrid, rather than centralized ones,
such as a large high-voltage power system. While our method is simple it is
useful in explaining why sometimes, but not always, it is economical to build
large, interconnected networks and in other cases it is preferable to use
smaller, distributed systems. The results indicate that there is not a single
set of infrastructure cost conditions under which optimally-designed networks
will have highly centralized architectures. Instead, as costs increase we find
that average network sizes increase gradually according to a power-law. When we
consider the reliability costs, however, we do observe a transition point at
which optimally designed networks become more centralized with larger
geographic scope. As the losses associated with node and edge failures become
more costly, this transition becomes more sudden.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08797</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08797</id><created>2015-10-29</created><updated>2015-11-17</updated><authors><author><keyname>Lin</keyname><forenames>Bo</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoxian</forenames></author><author><keyname>Yoshida</keyname><forenames>Ruriko</forenames></author></authors><title>Convexity in Tree Spaces</title><categories>math.MG cs.CG math.CO q-bio.PE</categories><comments>23 pages, 5 figures; Figures 3 and 5 correct an earlier example</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the geometry of metrics and convexity structures on the space of
phylogenetic trees, here realized as the tropical linear space of all
ultrametrics. The CAT(0)-metric of Billera-Holmes-Vogtman arises from the
theory of orthant spaces. While its geodesics can be computed by the
Owen-Provan algorithm, geodesic triangles are complicated and can have
arbitrarily high dimension. Tropical convexity and the tropical metric are
better behaved, as they exhibit properties that are desirable for geometric
statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08803</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08803</id><created>2015-10-29</created><authors><author><keyname>Mahesh</keyname><forenames>Anjana Ambika</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Noisy Index Coding with Quadrature Amplitude Modulation (QAM)</title><categories>cs.IT math.IT</categories><comments>6 pages; 5 figures. arXiv admin note: text overlap with
  arXiv:1509.05874</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses noisy index coding problem over Gaussian broadcast
channel. We propose a technique for mapping the index coded bits to M-QAM
symbols such that the receivers whose side information satisfies certain
conditions get coding gain, which we call the QAM side information coding gain.
We compare this with the PSK side information coding gain, which was discussed
in &quot;Index Coded PSK Modulation,&quot; arXiv:1356200, [cs.IT] 19 September 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08805</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08805</id><created>2015-10-29</created><authors><author><keyname>Tejaswi</keyname><forenames>R.</forenames></author><author><keyname>Narasimhan</keyname><forenames>T. Lakshmi</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Quad-LED Complex Modulation (QCM) for Visible Light Wireless
  Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a simple and novel complex modulation scheme for
multiple-LED wireless communication, termed as {\em quad-LED complex modulation
(QCM)}. The proposed QCM scheme uses four LEDs (hence the name `quad-LED'), one
LED each to map positive real, negative real, positive imaginary, and negative
imaginary parts of complex modulation symbols like QAM/PSK symbols. The QCM
scheme does not need Hermitian symmetry operation to generate LED compatible
positive real transmit signals. Instead it exploits spatial indexing of LEDs to
convey sign information. The proposed QCM module can serve as a basic building
block to bring in the benefits of complex modulation to VLC. For example, QCM
with phase rotation (QAM-PR) where the complex modulation symbols are rotated
in phase before mapping the signals to the LEDs achieves improved bit error
performance. We also find that the proposed QCM when used along with OFDM,
termed as QCM-OFDM, achieves very good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08829</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08829</id><created>2015-10-29</created><authors><author><keyname>Hunsberger</keyname><forenames>Eric</forenames></author><author><keyname>Eliasmith</keyname><forenames>Chris</forenames></author></authors><title>Spiking Deep Networks with LIF Neurons</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We train spiking deep networks using leaky integrate-and-fire (LIF) neurons,
and achieve state-of-the-art results for spiking networks on the CIFAR-10 and
MNIST datasets. This demonstrates that biologically-plausible spiking LIF
neurons can be integrated into deep networks can perform as well as other
spiking models (e.g. integrate-and-fire). We achieved this result by softening
the LIF response function, such that its derivative remains bounded, and by
training the network with noise to provide robustness against the variability
introduced by spikes. Our method is general and could be applied to other
neuron types, including those used on modern neuromorphic hardware. Our work
brings more biological realism into modern image classification models, with
the hope that these models can inform how the brain performs this difficult
task. It also provides new methods for training deep networks to run on
neuromorphic hardware, with the aim of fast, power-efficient image
classification for robotics applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08865</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08865</id><created>2015-10-29</created><authors><author><keyname>Wei</keyname><forenames>Kai</forenames></author><author><keyname>Iyer</keyname><forenames>Rishabh</forenames></author><author><keyname>Wang</keyname><forenames>Shengjie</forenames></author><author><keyname>Bai</keyname><forenames>Wenruo</forenames></author><author><keyname>Bilmes</keyname><forenames>Jeff</forenames></author></authors><title>Mixed Robust/Average Submodular Partitioning: Fast Algorithms,
  Guarantees, and Applications</title><categories>cs.DS cs.DM cs.LG</categories><comments>To appear NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate two novel mixed robust/average-case submodular data
partitioning problems that we collectively call \emph{Submodular Partitioning}.
These problems generalize purely robust instances of the problem, namely
\emph{max-min submodular fair allocation} (SFA) and \emph{min-max submodular
load balancing} (SLB), and also average-case instances, that is the
\emph{submodular welfare problem} (SWP) and \emph{submodular multiway
partition} (SMP). While the robust versions have been studied in the theory
community, existing work has focused on tight approximation guarantees, and the
resultant algorithms are not generally scalable to large real-world
applications. This is in contrast to the average case, where most of the
algorithms are scalable. In the present paper, we bridge this gap, by proposing
several new algorithms (including greedy, majorization-minimization,
minorization-maximization, and relaxation algorithms) that not only scale to
large datasets but that also achieve theoretical approximation guarantees
comparable to the state-of-the-art. We moreover provide new scalable algorithms
that apply to additive combinations of the robust and average-case objectives.
We show that these problems have many applications in machine learning (ML),
including data partitioning and load balancing for distributed ML, data
clustering, and image segmentation. We empirically demonstrate the efficacy of
our algorithms on real-world problems involving data partitioning for
distributed optimization (of convex and deep neural network objectives), and
also purely unsupervised image segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08867</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08867</id><created>2015-10-29</created><authors><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author><author><keyname>Fairclough</keyname><forenames>Ruth</forenames></author></authors><title>The influence of time and discipline on the magnitude of correlations
  between citation counts and quality scores</title><categories>cs.DL</categories><journal-ref>Journal of Informetrics, 9(3), 529-541 (2015)</journal-ref><doi>10.1016/j.joi.2015.05.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although various citation-based indicators are commonly used to help research
evaluations, there are ongoing controversies about their value. In response,
they are often correlated with quality ratings or with other quantitative
indicators in order to partly assess their validity. When correlations are
calculated for sets of publications from multiple disciplines or years,
however, the magnitude of the correlation coefficient may be reduced, masking
the strength of the underlying correlation. In response, this article uses
simulations to systematically investigate the extent to which mixing years or
disciplines reduces correlations. The results show that mixing two sets of
articles with different correlation strengths can reduce the correlation for
the combined set to substantially below the average of the two. Moreover, even
mixing two sets of articles with the same correlation strength but different
mean citation counts can substantially reduce the correlation for the combined
set. The extent of the reduction in correlation also depends upon whether the
articles assessed have been pre-selected for being high quality and whether the
relationship between the quality ratings and citation counts is linear or
exponential. The results underline the importance of using homogeneous data
sets but also help to interpret correlation coefficients when this is
impossible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08874</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08874</id><created>2015-10-29</created><authors><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author><author><keyname>Fairclough</keyname><forenames>Ruth</forenames></author></authors><title>Geometric journal impact factors correcting for individual highly cited
  articles</title><categories>cs.DL</categories><journal-ref>Journal of Informetrics, 9(2), 263-272 (2015)</journal-ref><doi>10.1016/j.joi.2015.02.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Journal impact factors (JIFs) are widely used and promoted but have important
limitations. In particular, JIFs can be unduly influenced by individual highly
cited articles and hence are inherently unstable. A logical way to reduce the
impact of individual high citation counts is to use the geometric mean rather
than the standard mean in JIF calculations. Based upon journal rankings
2004-2014 in 50 sub-categories within 5 broad categories, this study shows that
journal rankings based on JIF variants tend to be more stable over time if the
geometric mean is used rather than the standard mean. The same is true for JIF
variants using Mendeley reader counts instead of citation counts. Thus,
although the difference is not large, the geometric mean is recommended instead
of the arithmetic mean for future JIF calculations. In addition, Mendeley
readership-based JIF variants are as stable as those using Scopus citations,
confirming the value of Mendeley readership as an academic impact indicator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08877</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08877</id><created>2015-10-29</created><authors><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author><author><keyname>Wilson</keyname><forenames>Paul</forenames></author></authors><title>Regression for citation data: An evaluation of different methods</title><categories>cs.DL</categories><journal-ref>Journal of Informetrics, 8(4), 963-971 (2014)</journal-ref><doi>10.1016/j.joi.2014.09.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citations are increasingly used for research evaluations. It is therefore
important to identify factors affecting citation scores that are unrelated to
scholarly quality or usefulness so that these can be taken into account.
Regression is the most powerful statistical technique to identify these factors
and hence it is important to identify the best regression strategy for citation
data. Citation counts tend to follow a discrete lognormal distribution and, in
the absence of alternatives, have been investigated with negative binomial
regression. Using simulated discrete lognormal data (continuous lognormal data
rounded to the nearest integer) this article shows that a better strategy is to
add one to the citations, take their log and then use the general linear
(ordinary least squares) model for regression (e.g., multiple linear
regression, ANOVA), or to use the generalised linear model without the log.
Reasonable results can also be obtained if all the zero citations are
discarded, the log is taken of the remaining citation counts and then the
general linear model is used, or if the generalised linear model is used with
the continuous lognormal distribution. Similar approaches are recommended for
altmetric data, if it proves to be lognormally distributed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08881</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08881</id><created>2015-10-29</created><authors><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author><author><keyname>Wilson</keyname><forenames>Paul</forenames></author></authors><title>Distributions for cited articles from individual subjects and years</title><categories>cs.DL</categories><journal-ref>Journal of Informetrics, 8(4), 824-839 (2014)</journal-ref><doi>10.1016/j.joi.2014.08.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The citations to a set of academic articles are typically unevenly shared,
with many articles attracting few citations and few attracting many. It is
important to know more precisely how citations are distributed in order to help
statistical analyses of citations, especially for sets of articles from a
single discipline and a small range of years, as normally used for research
evaluation. This article fits discrete versions of the power law, the lognormal
distribution and the hooked power law to 20 different Scopus categories, using
citations to articles published in 2004 and ignoring uncited articles. The
results show that, despite its popularity, the power law is not a suitable
model for collections of articles from a single subject and year, even for the
purpose of estimating the slope of the tail of the citation data. Both the
hooked power law and the lognormal distributions fit best for some subjects but
neither is a universal optimal choice and parameter estimates for both seem to
be unreliable. Hence only the hooked power law and discrete lognormal
distributions should be considered for subject-and-year-based citation analysis
in future and parameter estimates should always be interpreted cautiously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08883</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08883</id><created>2015-10-29</created><updated>2016-02-09</updated><authors><author><keyname>Zhang</keyname><forenames>Hui</forenames></author><author><keyname>Skachek</keyname><forenames>Vitaly</forenames></author></authors><title>Bounds for Batch Codes with Restricted Query Size</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new upper bounds on the parameters of batch codes with restricted
query size. These bounds are an improvement on the Singleton bound. The
techniques for derivations of these bounds are based on the ideas in the
literature for codes with locality. By employing additional ideas, we obtain
further improvements, which are specific for batch codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08884</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08884</id><created>2015-10-29</created><authors><author><keyname>Fairclough</keyname><forenames>R.</forenames></author><author><keyname>Thelwall</keyname><forenames>M.</forenames></author></authors><title>National research impact indicators from Mendeley readers</title><categories>cs.DL</categories><journal-ref>Journal of Informetrics, 9(4), 845-859 (2015)</journal-ref><doi>10.1016/j.joi.2015.08.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  National research impact indicators derived from citation counts are used by
governments to help assess their national research performance and to identify
the effect of funding or policy changes. Citation counts lag research by
several years, however, and so their information is somewhat out of date. Some
of this lag can be avoided by using readership counts from the social reference
sharing site Mendeley because these accumulate more quickly than citations.
This article introduces a method to calculate national research impact
indicators from Mendeley, using citation counts from older time periods to
partially compensate for international biases in Mendeley readership. A
refinement to accommodate recent national changes in Mendeley uptake makes
little difference, despite being theoretically more accurate. The Mendeley
patterns using the methods broadly reflect the results from similar
calculations with citations and seem to reflect impact trends about a year
earlier. Nevertheless, the reasons for the differences between the indicators
from the two data sources are unclear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08887</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08887</id><created>2015-10-29</created><authors><author><keyname>Kimmel</keyname><forenames>Shelby</forenames></author><author><keyname>Liu</keyname><forenames>Yi-Kai</forenames></author></authors><title>Quantum Compressed Sensing Using 2-Designs</title><categories>quant-ph cs.IT math.IT math.ST stat.TH</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a method for quantum process tomography that combines the
efficiency of compressed sensing with the robustness of randomized
benchmarking. Our method is robust to state preparation and measurement errors,
and it achieves a quadratic speedup over conventional tomography when the
unknown process is a generic unitary evolution.
  Our method is based on PhaseLift, a convex programming technique for phase
retrieval. We show that this method achieves approximate recovery of almost all
signals, using measurements sampled from spherical or unitary 2-designs. This
is the first positive result on PhaseLift using 2-designs. We also show that
exact recovery of all signals is possible using unitary 4-designs. Previous
positive results for PhaseLift required spherical 4-designs, while PhaseLift
was known to fail in certain cases when using spherical 2-designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08892</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08892</id><created>2015-10-29</created><authors><author><keyname>Zehavi</keyname><forenames>Meirav</forenames></author></authors><title>A Randomized Algorithm for Long Directed Cycle</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a directed graph $G$ and a parameter $k$, the {\sc Long Directed Cycle
(LDC)} problem asks whether $G$ contains a simple cycle on at least $k$
vertices, while the {\sc $k$-Path} problems asks whether $G$ contains a simple
path on exactly $k$ vertices. Given a deterministic (randomized) algorithm for
{\sc $k$-Path} as a black box, which runs in time $t(G,k)$, we prove that {\sc
LDC} can be solved in deterministic time $O^*(\max\{t(G,2k),4^{k+o(k)}\})$
(randomized time $O^*(\max\{t(G,2k),4^k\})$). In particular, we get that {\sc
LDC} can be solved in randomized time $O^*(4^k)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08893</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08893</id><created>2015-10-29</created><authors><author><keyname>Baraldi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Grana</keyname><forenames>Costantino</forenames></author><author><keyname>Cucchiara</keyname><forenames>Rita</forenames></author></authors><title>A Deep Siamese Network for Scene Detection in Broadcast Videos</title><categories>cs.CV cs.MM</categories><comments>ACM Multimedia 2015</comments><doi>10.1145/2733373.2806316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model that automatically divides broadcast videos into coherent
scenes by learning a distance measure between shots. Experiments are performed
to demonstrate the effectiveness of our approach by comparing our algorithm
against recent proposals for automatic scene segmentation. We also propose an
improved performance measure that aims to reduce the gap between numerical
evaluation and expected results, and propose and release a new benchmark
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08896</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08896</id><created>2015-10-29</created><authors><author><keyname>Jin</keyname><forenames>Chi</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Musco</keyname><forenames>Cameron</forenames></author><author><keyname>Netrapalli</keyname><forenames>Praneeth</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author></authors><title>Robust Shift-and-Invert Preconditioning: Faster and More Sample
  Efficient Algorithms for Eigenvector Computation</title><categories>cs.DS cs.LG math.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide faster algorithms and improved sample complexities for
approximating the top eigenvector of a matrix.
  Offline Setting: Given an $n \times d$ matrix $A$, we show how to compute an
$\epsilon$ approximate top eigenvector in time $\tilde O ( [nnz(A) + \frac{d
\cdot sr(A)}{gap^2}]\cdot \log 1/\epsilon )$ and $\tilde O([\frac{nnz(A)^{3/4}
(d \cdot sr(A))^{1/4}}{\sqrt{gap}}]\cdot \log1/\epsilon )$. Here $sr(A)$ is the
stable rank and $gap$ is the multiplicative eigenvalue gap. By separating the
$gap$ dependence from $nnz(A)$ we improve on the classic power and Lanczos
methods. We also improve prior work using fast subspace embeddings and
stochastic optimization, giving significantly improved dependencies on $sr(A)$
and $\epsilon$. Our second running time improves this further when $nnz(A) \le
\frac{d\cdot sr(A)}{gap^2}$.
  Online Setting: Given a distribution $D$ with covariance matrix $\Sigma$ and
a vector $x_0$ which is an $O(gap)$ approximate top eigenvector for $\Sigma$,
we show how to refine to an $\epsilon$ approximation using $\tilde
O(\frac{v(D)}{gap^2} + \frac{v(D)}{gap \cdot \epsilon})$ samples from $D$. Here
$v(D)$ is a natural variance measure. Combining our algorithm with previous
work to initialize $x_0$, we obtain a number of improved sample complexity and
runtime results. For general distributions, we achieve asymptotically optimal
accuracy as a function of sample size as the number of samples grows large.
  Our results center around a robust analysis of the classic method of
shift-and-invert preconditioning to reduce eigenvector computation to
approximately solving a sequence of linear systems. We then apply fast SVRG
based approximate system solvers to achieve our claims. We believe our results
suggest the general effectiveness of shift-and-invert based approaches and
imply that further computational gains may be reaped in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08897</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08897</id><created>2015-10-29</created><authors><author><keyname>Dimitriadou</keyname><forenames>Kyriaki</forenames></author><author><keyname>Papaemmanouil</keyname><forenames>Olga</forenames></author><author><keyname>Diao</keyname><forenames>Yanlei</forenames></author></authors><title>AIDE: An Automated Sample-based Approach for Interactive Data
  Exploration</title><categories>cs.DB cs.IR</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we argue that database systems be augmented with an automated
data exploration service that methodically steers users through the data in a
meaningful way. Such an automated system is crucial for deriving insights from
complex datasets found in many big data applications such as scientific and
healthcare applications as well as for reducing the human effort of data
exploration. Towards this end, we present AIDE, an Automatic Interactive Data
Exploration framework that assists users in discovering new interesting data
patterns and eliminate expensive ad-hoc exploratory queries.
  AIDE relies on a seamless integration of classification algorithms and data
management optimization techniques that collectively strive to accurately learn
the user interests based on his relevance feedback on strategically collected
samples. We present a number of exploration techniques as well as optimizations
that minimize the number of samples presented to the user while offering
interactive performance. AIDE can deliver highly accurate query predictions for
very common conjunctive queries with small user effort while, given a
reasonable number of samples, it can predict with high accuracy complex
disjunctive queries. It provides interactive performance as it limits the user
wait time per iteration of exploration to less than a few seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08901</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08901</id><created>2015-10-29</created><authors><author><keyname>Ouyang</keyname><forenames>Feng</forenames></author></authors><title>Upper Bounds of Interference Alignment Degree of Freedom</title><categories>cs.IT math.AG math.IT</categories><msc-class>14N25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment allows multiple users to share the same frequency and
time resource in a wireless communications system. At present, two performance
bounds, in terms of degree of freedom, have been proposed. One is for
infinite-dimension extension and the other is for MIMO systems. This paper
provides an understanding of the MIMO bound by examining its proofs and shows
that it does not apply to a more practical case: MIMO-OFDM. Several approaches
are proposed in searching for DoF bounds for systems such as finite-dimension
time extension and MIMO-OFDM systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08906</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08906</id><created>2015-10-29</created><updated>2016-01-19</updated><authors><author><keyname>Dann</keyname><forenames>Christoph</forenames></author><author><keyname>Brunskill</keyname><forenames>Emma</forenames></author></authors><title>Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning</title><categories>stat.ML cs.AI cs.LG</categories><comments>28 pages, appeared in Neural Information Processing Systems (NIPS)
  2015, updated version with fixed typos and modified Lemma 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been significant progress in understanding reinforcement
learning in discounted infinite-horizon Markov decision processes (MDPs) by
deriving tight sample complexity bounds. However, in many real-world
applications, an interactive learning agent operates for a fixed or bounded
period of time, for example tutoring students for exams or handling customer
service requests. Such scenarios can often be better treated as episodic
fixed-horizon MDPs, for which only looser bounds on the sample complexity
exist. A natural notion of sample complexity in this setting is the number of
episodes required to guarantee a certain performance with high probability (PAC
guarantee). In this paper, we derive an upper PAC bound $\tilde
O(\frac{|\mathcal S|^2 |\mathcal A| H^2}{\epsilon^2} \ln\frac 1 \delta)$ and a
lower PAC bound $\tilde \Omega(\frac{|\mathcal S| |\mathcal A| H^2}{\epsilon^2}
\ln \frac 1 {\delta + c})$ that match up to log-terms and an additional linear
dependency on the number of states $|\mathcal S|$. The lower bound is the first
of its kind for this setting. Our upper bound leverages Bernstein's inequality
to improve on previous bounds for episodic finite-horizon MDPs which have a
time-horizon dependency of at least $H^3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08915</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08915</id><created>2015-10-29</created><authors><author><keyname>Sabau</keyname><forenames>Serban</forenames></author><author><keyname>Oara</keyname><forenames>Cristian</forenames></author><author><keyname>Warnick</keyname><forenames>Sean</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Optimal distributed control for platooning via sparse coprime
  factorizations</title><categories>cs.SY</categories><comments>48 pages, 8 figures, Provisionally accepted for publication in IEEE
  Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel distributed control architecture for heterogeneous
platoons of linear time--invariant autonomous vehicles. Our approach is based
on a generalization of the concept of {\em leader--follower} controllers for
which we provide a Youla--like parameterization while the sparsity constraints
are imposed on the controller's left coprime factors, outlying a new concept of
structural constraints in distributed control. The proposed scheme is amenable
to optimal controller design via norm based costs, it guarantees string
stability and eliminates the accordion effect from the behavior of the platoon.
We also introduce a synchronization mechanism for the exact compensation of the
time delays induced by the wireless broadcasting of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08917</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08917</id><created>2015-10-29</created><authors><author><keyname>Lin</keyname><forenames>Chia-Hsiang</forenames></author><author><keyname>Chi</keyname><forenames>Chong-Yung</forenames></author><author><keyname>Wang</keyname><forenames>Yu-Hsiang</forenames></author><author><keyname>Chan</keyname><forenames>Tsung-Han</forenames></author></authors><title>A Fast Hyperplane-Based Minimum-Volume Enclosing Simplex Algorithm for
  Blind Hyperspectral Unmixing</title><categories>cs.CG</categories><comments>36 pages, 8 figures, IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral unmixing (HU) is a crucial signal processing procedure to
identify the underlying materials (or endmembers) and their corresponding
proportions (or abundances) from an observed hyperspectral scene. A well-known
blind HU criterion, advocated by Craig in early 1990's, considers the vertices
of the minimum-volume enclosing simplex of the data cloud as good endmember
estimates, and it has been empirically and theoretically found effective even
in the scenario of no pure pixels. However, such kind of algorithms may suffer
from heavy simplex volume computations in numerical optimization, etc. In this
work, without involving any simplex volume computations, by exploiting a convex
geometry fact that a simplest simplex of N vertices can be defined by N
associated hyperplanes, we propose a fast blind HU algorithm, for which each of
the N hyperplanes associated with the Craig's simplex of N vertices is
constructed from N-1 affinely independent data pixels, together with an
endmember identifiability analysis for its performance support. Without
resorting to numerical optimization, the devised algorithm searches for the
N(N-1) active data pixels via simple linear algebraic computations, accounting
for its computational efficiency. Monte Carlo simulations and real data
experiments are provided to demonstrate its superior efficacy over some
benchmark Craig-criterion-based algorithms in both computational efficiency and
estimation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08929</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08929</id><created>2015-10-29</created><authors><author><keyname>Tan</keyname><forenames>Xin</forenames></author><author><keyname>Sun</keyname><forenames>Zhi</forenames></author><author><keyname>Jornet</keyname><forenames>Josep M.</forenames></author><author><keyname>Pados</keyname><forenames>Dimitris</forenames></author></authors><title>Increasing Indoor Spectrum Sharing Capacity using Smart Reflect-Array</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The radio frequency (RF) spectrum becomes overly crowded in some indoor
environments due to the high density of users and bandwidth demands. To
accommodate the tremendous wireless data demands, efficient spectrum-sharing
approaches are highly desired. To this end, this paper introduces a new
spectrum sharing solution for indoor environments based on the usage of a
reconfigurable reflect-array in the middle of the wireless channel. By
optimally controlling the phase shift of each element on the reflect-array, the
useful signals for each transmission pair can be enhanced while the
interferences can be canceled. As a result, multiple wireless users in the same
room can access the same spectrum band at the same time without interfering
each other. Hence, the network capacity can be dramatically increased. To prove
the feasibility of the proposed solution, an experimental testbed is first
developed and evaluated. Then, the effects of the reflect-array on transport
capacity of the indoor wireless networks are investigated. Through experiments,
theoretical deduction, and simulations, this paper demonstrates that
significantly higher spectrum-spatial efficiency can be achieved by using the
smart reflect-array without any modification of the hardware and software in
the users' devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08940</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08940</id><created>2015-10-29</created><authors><author><keyname>Carlini</keyname><forenames>Emanuele</forenames></author></authors><title>Combining Peer-to-Peer and Cloud Computing for Large Scale On-line Games</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This thesis investigates the combination of Peer-to-Peer (P2P) and Cloud
Computing to support Massively Multiplayer On- line Games (MMOGs). MMOGs are
large-scale distributed applications where a large number of users concurrently
share a real-time virtual environment. Commercial MMOG infrastructures are
sized to support peak loads, incurring in high economical cost. Cloud Computing
represents an attractive solution, as it lifts MMOG operators from the burden
of buying and maintaining hardware, while offering the illusion of infinite
machines. However, it requires balancing the tradeoff between resource
provisioning and operational costs. P2P- based solutions present several
advantages, including the inherent scalability, self-repairing, and natural
load distribution capabilities. They require additional mechanisms to suit the
requirements of a MMOG, such as backup solutions to cope with peer
unreliability and heterogeneity. We propose mechanisms that integrate P2P and
Cloud Computing combining their advantages. Our techniques allow operators to
select the ideal tradeoff between performance and economical costs. Using
realistic workloads, we show that hybrid infrastructures can reduce the
economical effort of the operator, while offering a level of service comparable
with centralized architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08943</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08943</id><created>2015-10-29</created><authors><author><keyname>Ruoti</keyname><forenames>Scott</forenames></author><author><keyname>Zappala</keyname><forenames>Daniel</forenames></author><author><keyname>Seamons</keyname><forenames>Kent</forenames></author></authors><title>MessageGuard: Retrofitting the Web with User-to-user Encryption</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Users today share a great deal of private information on the Web. While HTTPS
protects this data during transmission, it does not protect data at rest, nor
does it protect user data from the websites which store or transmit that data.
These issues can be addressed with user-to-user encryption, an approach where
data is encrypted and decrypted at the user's computer and is opaque to
websites. In this paper we present MessageGuard, the first system that
retrofits the Web with user-to-user encryption and is designed to work with all
websites, in all browsers, on all platforms. We demonstrate that MessageGuard
operates out-of-the-box on 47 of the Alexa top 50 sites, has minimal
performance overhead, and is rated as highly usable by study participants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08946</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08946</id><created>2015-10-29</created><authors><author><keyname>Ai</keyname><forenames>Bing</forenames></author><author><keyname>Wong</keyname><forenames>David Shan-Hill</forenames></author><author><keyname>Jang</keyname><forenames>Shi-Shang</forenames></author></authors><title>Stability analysis of semiconductor manufacturing process with EWMA
  run-to-run controllers</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the semiconductor manufacturing batch processes, each step is a
complicated physiochemical batch process; generally it is difficult to perform
measurements online or carry out the measurement for each run, and hence there
will be delays in the feedback of the system. The effect of the delay on the
stability of the system is an important issue which needs to be understood.
Based on the exponentially weighted moving average (EWMA) algorithm, we propose
two kinds of controllers, EWMA-I and II controllers for single product process
and mixed product process in semiconductor manufacturing in this paper. For the
single product process, the stabilities of systems with both controllers which
undergo different kinds of metrology delays are investigated. Necessary and
sufficient conditions for the stochastic stability are established.
Routh-Hurwitz criterion and Lyapunov's direct method are used to obtain the
stability regions for the system with fixed metrology delay. By using
Lyapunov's direct method, the stability region is established for the system
with fixed sampling metrology and with stochastic metrology delay. We also
extended the theorems of single product process to mixed product process. Based
on the proposed theorems, some numerical examples are provided to illustrate
the stability of the delay system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08949</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08949</id><created>2015-10-29</created><authors><author><keyname>Bachman</keyname><forenames>Philip</forenames></author><author><keyname>Krueger</keyname><forenames>David</forenames></author><author><keyname>Precup</keyname><forenames>Doina</forenames></author></authors><title>Testing Visual Attention in Dynamic Environments</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate attention as the active pursuit of useful information. This
contrasts with attention as a mechanism for the attenuation of irrelevant
information. We also consider the role of short-term memory, whose use is
critical to any model incapable of simultaneously perceiving all information on
which its output depends. We present several simple synthetic tasks, which
become considerably more interesting when we impose strong constraints on how a
model can interact with its input, and on how long it can take to produce its
output. We develop a model with a different structure from those seen in
previous work, and we train it using stochastic variational inference with a
learned proposal distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08950</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08950</id><created>2015-10-29</created><authors><author><keyname>Chen</keyname><forenames>Hanchi</forenames></author><author><keyname>Samarasinghe</keyname><forenames>Prasanga N.</forenames></author><author><keyname>Abhayapala</keyname><forenames>Thushara D.</forenames></author><author><keyname>Zhang</keyname><forenames>Wen</forenames></author></authors><title>Estimation of the direct-to-reverberant Energy Ratio using a spherical
  microphone array</title><categories>cs.SD</categories><comments>In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</comments><report-no>ACEChallenge/2015/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a practical approach to estimate the
direct-to-reverberant energy ratio (DRR) using a spherical microphone array
without having knowledge of the source signal. We base our estimation on a
theoretical relationship between the DRR and the coherence estimation function
between coincident pressure and particle velocity. We discuss the proposed
method's ability to estimate the DRR in a wide variety of room sizes,
reverberation times and source receiver distances with appropriate examples.
Test results show that the method can estimate the room DRR for frequencies
between 199 - 2511 Hz, with $\pm$ 3 dB accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08952</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08952</id><created>2015-10-29</created><authors><author><keyname>D&#xf6;rpinghaus</keyname><forenames>Meik</forenames></author><author><keyname>Rold&#xe1;n</keyname><forenames>&#xc9;dgar</forenames></author><author><keyname>Neri</keyname><forenames>Izaak</forenames></author><author><keyname>Meyr</keyname><forenames>Heinrich</forenames></author><author><keyname>J&#xfc;licher</keyname><forenames>Frank</forenames></author></authors><title>Information Density and Optimality of Sequential Decision-Making</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an information theoretic analysis of Wald's sequential probability
ratio test. The optimality of the Wald test in the sense that it yields the
minimum average decision time for a binary decision problem is reflected by the
evolution of the information densities over time. Information densities are
considered as they take into account the fact that the termination time of the
Wald test depends on the actual realization of the observation sequence. Based
on information densities we show that in case the test terminates at time
instant $k$ the probability to decide for hypothesis $\mathcal{H}_1$ (or the
counter-hypothesis $\mathcal{H}_0$) is independent of time. We use this
characteristic to evaluate the evolution of the mutual information between the
binary random variable and the decision variable of the Wald test. Our results
establish a connection between minimum mean decision times and the
corresponding information processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08953</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08953</id><created>2015-10-29</created><authors><author><keyname>Ding</keyname><forenames>Ni</forenames></author><author><keyname>Chan</keyname><forenames>Chung</forenames></author><author><keyname>Liu</keyname><forenames>Tie</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author></authors><title>A Game-theoretic Perspective on Communication for Omniscience</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, submitted to AUSCTW 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a coalition game model for the problem of communication for
omniscience (CO). In this game model, the core contains all achievable rate
vectors for CO with sum-rate being equal to a given value. Any rate vector in
the core distributes the sum-rate among users in a way that makes all users
willing to cooperate in CO. We give the necessary and sufficient condition for
the core to be nonempty. Based on this condition, we derive the expression of
the minimum sum-rate for CO and show that this expression is consistent with
the results in multivariate mutual information (MMI) and coded cooperative data
exchange (CCDE). We prove that the coalition game model is convex if the
sum-rate is no less than the minimal value. In this case, the core is non-empty
and a rate vector in the core that allocates the sum-rate among the users in a
fair manner can be found by calculating the Shapley value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08954</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08954</id><created>2015-10-29</created><authors><author><keyname>Vijayaraghavan</keyname><forenames>V. S.</forenames></author><author><keyname>James</keyname><forenames>R. G.</forenames></author><author><keyname>Crutchfield</keyname><forenames>J. P.</forenames></author></authors><title>Anatomy of a Spin: The Information-Theoretic Structure of Classical Spin
  Systems</title><categories>cond-mat.stat-mech cs.IT math.IT nlin.AO</categories><comments>9 pages, 6 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/ising_bmu.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collective organization in matter plays a significant role in its expressed
physical properties. Typically, it is detected via an order parameter,
appropriately defined for a given system's observed emergent patterns. Recent
developments in information theory suggest how to quantify collective
organization in a system- and phenomenon-agnostic way: decompose the system's
thermodynamic entropy density into a localized entropy, that solely contained
in the dynamics at a single location, and a bound entropy, that stored in space
as domains, clusters, excitations, or other emergent structures. We compute
this decomposition and related quantities explicitly for the nearest-neighbor
Ising model on the 1D chain, the Bethe lattice with coordination number k = 3,
and the 2D square lattice, illustrating its generality and the functional
insights it gives near and away from phase transitions. In particular, we
consider the roles that different spin motifs play (cluster bulk, cluster
edges, and the like) and how these affect the dependencies between spins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08956</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08956</id><created>2015-10-29</created><authors><author><keyname>Mueller</keyname><forenames>Jonas</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author></authors><title>Principal Differences Analysis: Interpretable Characterization of
  Differences between Distributions</title><categories>stat.ML cs.LG stat.ME</categories><comments>Advances in Neural Information Processing Systems 28 (NIPS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce principal differences analysis (PDA) for analyzing differences
between high-dimensional distributions. The method operates by finding the
projection that maximizes the Wasserstein divergence between the resulting
univariate populations. Relying on the Cramer-Wold device, it requires no
assumptions about the form of the underlying distributions, nor the nature of
their inter-class differences. A sparse variant of the method is introduced to
identify features responsible for the differences. We provide algorithms for
both the original minimax formulation as well as its semidefinite relaxation.
In addition to deriving some convergence results, we illustrate how the
approach may be applied to identify differences between cell populations in the
somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our
broader framework extends beyond the specific choice of Wasserstein divergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08963</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08963</id><created>2015-10-29</created><authors><author><keyname>Hioka</keyname><forenames>Yusuke</forenames></author><author><keyname>Niwa</keyname><forenames>Kenta</forenames></author></authors><title>PSD estimation in Beamspace for Estimating Direct-to-Reverberant Ratio
  from A Reverberant Speech Signal</title><categories>cs.SD</categories><comments>In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA2015 (arXiv:1510.00383)</comments><report-no>ACEChallenge/2015/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method for estimation of direct-to-reverberant ratio (DRR) using a
microphone array is proposed. The proposed method estimates the power spectral
density (PSD) of the direct sound and the reverberation using the algorithm
\textit{PSD estimation in beamspace} with a microphone array and calculates the
DRR of the observed signal. The speech corpus of the ACE (Acoustic
Characterisation of Environments) Challenge was utilised for evaluating the
practical feasibility of the proposed method. The experimental results revealed
that the proposed method was able to effectively estimate the DRR from a
recording of a reverberant speech signal which included various environmental
noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08970</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08970</id><created>2015-10-30</created><authors><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author><author><keyname>Neider</keyname><forenames>Daniel</forenames></author></authors><title>Robust Linear Temporal Logic</title><categories>cs.LO cs.SY math.OC</categories><msc-class>03B44</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although it is widely accepted that every system should be robust, in the
sense that &quot;small&quot; violations of environment assumptions should lead to &quot;small&quot;
violations of system guarantees, it is less clear how to make this intuitive
notion of robustness mathematically precise. In this paper, we address this
problem by developing a robust version of Linear Temporal Logic (LTL), which we
call robust LTL and denote by rLTL. Formulas in rLTL are syntactically
identical to LTL formulas but are endowed with a many-valued semantics that
encodes robustness. In particular, the semantics of the rLTL formula $\varphi
\Rightarrow \psi$ is such that a &quot;small&quot; violation of the environment
assumption $\varphi$ is guaranteed to only produce a &quot;small&quot; violation of the
system guarantee $\psi$. In addition to introducing rLTL, we study the
verification and synthesis problems for this logic: similarly to LTL, we show
that both problems are decidable, that the verification problem can be solved
in time exponential in the number of subformulas of the rLTL formula at hand,
and that the synthesis problem can be solved in doubly exponential time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08971</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08971</id><created>2015-10-30</created><authors><author><keyname>Kang</keyname><forenames>Zhao</forenames></author><author><keyname>Peng</keyname><forenames>Chong</forenames></author><author><keyname>Cheng</keyname><forenames>Qiang</forenames></author></authors><title>Robust Subspace Clustering via Tighter Rank Approximation</title><categories>cs.CV cs.AI cs.LG stat.ML</categories><comments>ACM CIKM 2015</comments><doi>10.1145/2806416.2806506</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix rank minimization problem is in general NP-hard. The nuclear norm is
used to substitute the rank function in many recent studies. Nevertheless, the
nuclear norm approximation adds all singular values together and the
approximation error may depend heavily on the magnitudes of singular values.
This might restrict its capability in dealing with many practical problems. In
this paper, an arctangent function is used as a tighter approximation to the
rank function. We use it on the challenging subspace clustering problem. For
this nonconvex minimization problem, we develop an effective optimization
procedure based on a type of augmented Lagrange multipliers (ALM) method.
Extensive experiments on face clustering and motion segmentation show that the
proposed method is effective for rank approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08973</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08973</id><created>2015-10-30</created><authors><author><keyname>Sadeghi</keyname><forenames>Fereshteh</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author><author><keyname>Farhadi</keyname><forenames>Ali</forenames></author></authors><title>VISALOGY: Answering Visual Analogy Questions</title><categories>cs.CV</categories><comments>To appear in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of answering visual analogy questions.
These questions take the form of image A is to image B as image C is to what.
Answering these questions entails discovering the mapping from image A to image
B and then extending the mapping to image C and searching for the image D such
that the relation from A to B holds for C to D. We pose this problem as
learning an embedding that encourages pairs of analogous images with similar
transformations to be close together using convolutional neural networks with a
quadruple Siamese architecture. We introduce a dataset of visual analogy
questions in natural images, and show first results of its kind on solving
analogy questions on natural images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08974</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08974</id><created>2015-10-30</created><authors><author><keyname>Barsky</keyname><forenames>Daniel</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author></authors><title>CONQUER: Confusion Queried Online Bandit Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new recommendation setting for picking out two items from a
given set to be highlighted to a user, based on contextual input. These two
items are presented to a user who chooses one of them, possibly stochastically,
with a bias that favours the item with the higher value. We propose a
second-order algorithm framework that members of it use uses relative
upper-confidence bounds to trade off exploration and exploitation, and some
explore via sampling. We analyze one algorithm in this framework in an
adversarial setting with only mild assumption on the data, and prove a regret
bound of $O(Q_T + \sqrt{TQ_T\log T} + \sqrt{T}\log T)$, where $T$ is the number
of rounds and $Q_T$ is the cumulative approximation error of item values using
a linear model. Experiments with product reviews from 33 domains show the
advantage of our methods over algorithms designed for related settings, and
that UCB based algorithms are inferior to greed or sampling based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08979</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08979</id><created>2015-10-30</created><authors><author><keyname>Hermerschmidt</keyname><forenames>Lars</forenames></author><author><keyname>Kugelmann</keyname><forenames>Stephan</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Towards More Security in Data Exchange: Defining Unparsers with
  Context-Sensitive Encoders for Context-Free Grammars</title><categories>cs.SE</categories><comments>8 pages, 4 listings</comments><journal-ref>Proceedings of 2015 IEEE Security and Privacy Workshops, pp.
  134-141, San Jose, CA, USA, IEEE Computer Society, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To exchange complex data structures in distributed systems, documents written
in context-free languages are exchanged among communicating parties. Unparsing
these documents correctly is as important as parsing them correctly because
errors during unparsing result in injection vulnerabilities such as cross-site
scripting (XSS) and SQL injection. Injection attacks are not limited to the web
world. Every program that uses input to produce documents in a context-free
language may be vulnerable to this class of attack. Even for widely used
languages such as HTML and JavaScript, there are few approaches that prevent
injection attacks by context-sensitive encoding, and those approaches are tied
to the language. Therefore, the aim of this paper is to derive
context-sensitive encoder from context-free grammars to provide correct
unparsing of maliciously crafted input data for all context-free languages. The
presented solution integrates encoder definition into context-free grammars and
provides a generator for context-sensitive encoders and decoders that are used
during (un)parsing. This unparsing process results in documents where the input
data does neither influence the structure of the document nor change their
intended semantics. By defining encoding during language definition, developers
who use the language are provided with a clean interface for writing and
reading documents written in that language, without the need to care about
security-relevant encoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08980</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08980</id><created>2015-10-30</created><authors><author><keyname>Mavronicolas</keyname><forenames>Marios</forenames></author><author><keyname>Monien</keyname><forenames>Burkhard</forenames></author></authors><title>The Complexity of Equilibria for Risk-Modeling Valuations</title><categories>cs.GT cs.CC</categories><comments>49 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of deciding the existence of mixed equilibria for
minimization games where players use valuations other than expectation to
evaluate their costs. We consider risk-averse players seeking to minimize the
sum ${\mathsf{V}} = {\mathsf{E}} + {\mathsf{R}}$ of expectation ${\mathsf{E}}$
and a risk valuation ${\mathsf{R}}$ of their costs; ${\mathsf{R}}$ is
non-negative and vanishes exactly when the cost incurred to a player is
constant over all choices of strategies by the other players. In a
${\mathsf{V}}$-equilibrium, no player could unilaterally reduce her cost.
  Say that ${\mathsf{V}}$ has the Weak-Equilibrium-for-Expectation property if
all strategies supported in a player's best-response mixed strategy incur the
same conditional expectation of her cost. We introduce ${\mathsf{E}}$-strict
concavity and observe that every ${\mathsf{E}}$-strictly concave valuation has
the Weak-Equilibrium-for-Expectation property. We focus on a broad class of
valuations shown to have the Weak-Equilibrium-for-Expectation property, which
we exploit to prove two main complexity results, the first of their kind, for
the two simplest cases of the problem: games with two strategies, or games with
two players. For each case, we show that deciding the existence of a
${\mathsf{V}}$-equilibrium is strongly ${\mathcal{NP}}$-hard for certain
choices of significant valuations (including variance and standard deviation).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08981</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08981</id><created>2015-10-30</created><authors><author><keyname>Hermerschmidt</keyname><forenames>Lars</forenames></author><author><keyname>H&#xf6;lldobler</keyname><forenames>Katrin</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>Generating Domain-Specific Transformation Languages for Component &amp;
  Connector Architecture Descriptions</title><categories>cs.SE</categories><comments>6 pages, 4 figures in 2nd International Workshop on Model-Driven
  Engineering for Component-Based Software Systems (ModComp), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Component-based software engineering (CBSE) decomposes complex systems into
reusable components. Model-driven engineering (MDE) aims to abstract from
complexities by lifting abstract models to primary development artifacts.
Component and connector architecture description languages (ADLs) combine CBSE
and MDE to describe software systems as hierarchies of component models. Using
models as development artifacts is accompanied with the need to evolve,
maintain and refactor those models, which can be achieved by model
transformations. Domain-specific transformation languages (DSTLs) are tailored
to a specific modeling language as the modeling language's concrete syntax is
used to describe transformations. To automate the development of DSTLs for
ADLs, we present a framework to systematically derive such languages from
domain-specific C&amp;C language grammars. These DSTLs enable to describe such
model transformations concisely in vocabulary of the underlying ADL. These
domain-specific transformations are better comprehensible to ADL experts than
generic transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08982</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08982</id><created>2015-10-30</created><updated>2015-11-01</updated><authors><author><keyname>Lee</keyname><forenames>Kooktae</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>Asynchronous Parallel Computing Algorithm implemented in 1D Heat
  Equation with CUDA</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1503.03952</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we present the stability as well as performance analysis of
asynchronous parallel computing algorithm implemented in 1D heat equation with
CUDA. The primary objective of this note lies in dissemination of asynchronous
parallel computing algorithm by providing CUDA code for fast and easy
implementation. We show that the simulations carried out on nVIDIA GPU device
with asynchronous scheme outperforms synchronous parallel computing algorithm.
In addition, we also discuss some drawbacks of asynchronous parallel computing
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08983</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08983</id><created>2015-10-30</created><updated>2016-01-11</updated><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Chen</keyname><forenames>Guoguo</forenames></author><author><keyname>Yu</keyname><forenames>Dong</forenames></author><author><keyname>Yao</keyname><forenames>Kaisheng</forenames></author><author><keyname>Khudanpur</keyname><forenames>Sanjeev</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author></authors><title>Highway Long Short-Term Memory RNNs for Distant Speech Recognition</title><categories>cs.NE cs.AI cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend the deep long short-term memory (DLSTM) recurrent
neural networks by introducing gated direct connections between memory cells in
adjacent layers. These direct links, called highway connections, enable
unimpeded information flow across different layers and thus alleviate the
gradient vanishing problem when building deeper LSTMs. We further introduce the
latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole
history while keeping the latency under control. Efficient algorithms are
proposed to train these novel networks using both frame and sequence
discriminative criteria. Experiments on the AMI distant speech recognition
(DSR) task indicate that we can train deeper LSTMs and achieve better
improvement from sequence training with highway LSTMs (HLSTMs). Our novel model
obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all
previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and
$5.3\%$ relative improvement respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08985</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08985</id><created>2015-10-30</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Chuangsuwanich</keyname><forenames>Ekapol</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author><author><keyname>Yu</keyname><forenames>Dong</forenames></author></authors><title>Prediction-Adaptation-Correction Recurrent Neural Networks for
  Low-Resource Language Speech Recognition</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the use of prediction-adaptation-correction
recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A
PAC-RNN is comprised of a pair of neural networks in which a {\it correction}
network uses auxiliary information given by a {\it prediction} network to help
estimate the state probability. The information from the correction network is
also used by the prediction network in a recurrent loop. Our model outperforms
other state-of-the-art neural networks (DNNs, LSTMs) on IARPA-Babel tasks.
Moreover, transfer learning from a language that is similar to the target
language can help improve performance further.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08989</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08989</id><created>2015-10-30</created><authors><author><keyname>Lipi&#x144;ski</keyname><forenames>Zbigniew</forenames></author></authors><title>Maximum lifetime problem in sensor networks with limited channel
  capacity</title><categories>cs.NI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper we analyze the maximum lifetime problem in sensor networks with
limited channel capacity for multipoint-to-multipoint and broadcast data
transmission services. We show, that in order to achieve an optimal data
transmission regarding the maximum lifetime problem we cannot allow for any
interference of signals. We propose a new Signal to Interference plus Noise
Ratio function and used is to modify the Shannon-Hartley channel capacity
formula. For the modified channel capacity formula we solve the maximum
lifetime problem in one dimensional regular sensor network $L_N$ for discussed
data transmission services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08999</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08999</id><created>2015-10-30</created><authors><author><keyname>Xu</keyname><forenames>Liang</forenames></author><author><keyname>Mo</keyname><forenames>Yilin</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Mean Square Stabilization of Vector LTI Systems over Power Constrained
  Lossy Channels</title><categories>math.OC cs.SY</categories><comments>Submitted to 2016 American Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the mean square stabilization problem of vector LTI
systems over power constrained lossy channels. The communication channel is
with packet dropouts, additive noises and input power constraints. To overcome
the difficulty of optimally allocating channel resources among different
sub-dynamics, schedulers are designed with time division multiplexing of
channels. An adaptive TDMA (Time Division Multiple Access) scheduler is
proposed first, which is shown to be able to achieve a larger stabilizability
region than the conventional TDMA scheduler, and is optimal under some special
cases. In particular, for two-dimensional systems, an optimal scheduler is
designed, which provides the necessary and sufficient condition for mean square
stabilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09005</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09005</id><created>2015-10-30</created><authors><author><keyname>Guigour&#xe8;s</keyname><forenames>Romain</forenames><affiliation>SAMM</affiliation></author><author><keyname>Boull&#xe9;</keyname><forenames>Marc</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>A Study of the Spatio-Temporal Correlations in Mobile Calls Networks</title><categories>stat.ML cs.SI</categories><comments>Advances in Knowledge Discovery and Management, 615, Springer
  International Publishing, pp.3-17, 2015, Studies in Computational
  Intelligence</comments><proxy>ccsd</proxy><doi>10.1007/978-3-319-23751-0_1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the last few years, the amount of data has significantly increased in the
companies. It is the reason why data analysis methods have to evolve to meet
new demands. In this article, we introduce a practical analysis of a large
database from a telecommunication operator. The problem is to segment a
territory and characterize the retrieved areas owing to their inhabitant
behavior in terms of mobile telephony. We have call detail records collected
during five months in France. We propose a two stages analysis. The first one
aims at grouping source antennas which originating calls are similarly
distributed on target antennas and conversely for target antenna w.r.t. source
antenna. A geographic projection of the data is used to display the results on
a map of France. The second stage discretizes the time into periods between
which we note changes in distributions of calls emerging from the clusters of
source antennas. This enables an analysis of temporal changes of inhabitants
behavior in every area of the country.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09021</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09021</id><created>2015-10-30</created><authors><author><keyname>Chen</keyname><forenames>Tehuan</forenames></author><author><keyname>Xu</keyname><forenames>Chao</forenames></author></authors><title>Computational Optimal Control of the Saint-Venant PDE Model Using the
  Time-scaling Technique</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new time-scaling approach for computational optimal
control of a distributed parameter system governed by the Saint-Venant PDEs. We
propose the time-scaling approach, which can change a uniform time partition to
a nonuniform one. We also derive the gradient formulas by using the variational
method. Then the method of lines (MOL) is applied to compute the Saint-Venant
PDEs after implementing the time-scaling transformation and the associate
costate PDEs. Finally, we compare the optimization results using the proposed
time-scaling approach with the one not using it. The simulation result
demonstrates the effectiveness of the proposed time-scaling method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09025</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09025</id><created>2015-10-30</created><authors><author><keyname>Celis</keyname><forenames>L. Elisa</forenames></author><author><keyname>Mousavifar</keyname><forenames>Aida Sadat</forenames></author></authors><title>A Simple Network Model for Social Media</title><categories>cs.SI cs.CY cs.DM cs.GT cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a simple game theoretical network model for social media
(e.g., Twitter) as opposed to social networks (e.g., Facebook). In our directed
model, connecting to other agents has a cost, and reaching other agents via
(short) directed paths has a benefit; in effect, a user wants to have
information diffused to them quickly from a specific set of agents, but without
the cost of directly following each and every one.
  We formally introduce the model and use it to define a k-dimensional
clustering coefficient. We hypothesize that this generalized clustering
coefficient in social media networks are fundamentally different than in social
networks. We evaluate two real-world network subsets from Twitter and Google
Plus which lends support to this hypothesis.
  We then work towards understanding our model from a theoretical standpoint.
We study the stability, efficiency and fairness of static networks, and we
prove that a stochastic asynchronous best-response dynamics always converges to
a stable network; moreover the set of stable networks that can be sinks of the
dynamics support complex structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09033</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09033</id><created>2015-10-30</created><authors><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Turing's Red Flag</title><categories>cs.AI cs.CY</categories><comments>To appear in the Communications of the ACM</comments><acm-class>I.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sometime in the future we will have to deal with the impact of AI's being
mistaken for humans. For this reason, I propose that any autonomous system
should be designed so that it is unlikely to be mistaken for anything besides
an autonomous sysem, and should identify itself at the start of any interaction
with another agent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09037</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09037</id><created>2015-10-30</created><updated>2015-11-15</updated><authors><author><keyname>Tak&#xe1;cs</keyname><forenames>Krist&#xf3;f</forenames></author></authors><title>Multiple sequence alignment for short sequences</title><categories>q-bio.QM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple sequence alignment (MSA) has been one of the most important problems
in bioinformatics for more decades and it is still heavily examined by many
mathematicians and biologists. However, mostly because of the practical
motivation of this problem, the research on this topic is focused on aligning
long sequences. It is understandable, since the sequences that need to be
aligned (usually DNA or protein sequences) are generally quite long (e. g., at
least 30-40 characters). Nevertheless, it is a challenging question that
exactly where MSA starts to become a real hard problem (since it is known that
MSA is NP-complete [2]), and the key to answer this question is to examine
short sequences. If the optimal alignment for short sequences could be
determined in polynomial time, then these results may help to develop faster or
more accurate heuristic algorithms for aligning long sequences. In this work,
it is shown that for length-1 sequences using arbitrary metric, as well as for
length-2 sequences using unit metric, the optimum of the MSA problem can be
achieved by the trivial alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09040</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09040</id><created>2015-10-30</created><updated>2015-12-21</updated><authors><author><keyname>Durand</keyname><forenames>Arnaud</forenames></author><author><keyname>Hannula</keyname><forenames>Miika</forenames></author><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author><author><keyname>Meier</keyname><forenames>Arne</forenames></author><author><keyname>Virtema</keyname><forenames>Jonni</forenames></author></authors><title>Approximation and Dependence via Multiteam Semantics</title><categories>cs.LO math.LO</categories><comments>Accepted paper for FoIKS conference, long version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a variant of team semantics called multiteam semantics based on
multisets and study the properties of various logics in this framework. In
particular, we define natural probabilistic versions of inclusion and
independence atoms and certain approximation operators motivated by approximate
dependence atoms of V\&quot;a\&quot;an\&quot;anen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09041</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09041</id><created>2015-10-30</created><authors><author><keyname>Dar</keyname><forenames>Yehuda</forenames></author><author><keyname>Bruckstein</keyname><forenames>Alfred M.</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author><author><keyname>Giryes</keyname><forenames>Raja</forenames></author></authors><title>Postprocessing of Compressed Images via Sequential Denoising</title><categories>cs.CV</categories><comments>Submitted to IEEE Transactions on Image Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a novel postprocessing technique for
compression-artifact reduction. Our approach is based on posing this task as an
inverse problem, with a regularization that leverages on existing
state-of-the-art image denoising algorithms. We rely on the recently proposed
Plug-and-Play Prior framework, suggesting the solution of general inverse
problems via Alternating Direction Method of Multipliers (ADMM), leading to a
sequence of Gaussian denoising steps. A key feature in our scheme is a
linearization of the compression-decompression process, so as to get a
formulation that can be optimized. In addition, we supply a thorough analysis
of this linear approximation for several basic compression procedures. The
proposed method is suitable for diverse compression techniques that rely on
transform coding. Specifically, we demonstrate impressive gains in image
quality for several leading compression methods - JPEG, JPEG2000, and HEVC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09046</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09046</id><created>2015-10-30</created><authors><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Maier</keyname><forenames>Henning</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author><author><keyname>Mathar</keyname><forenames>Rudolf</forenames></author></authors><title>Three-Way Channels with Multiple Unicast Sessions: Capacity
  Approximation via Network Transformation</title><categories>cs.IT math.IT</categories><comments>44 pages, 10 figures, extension of results presented in Allerton 2014
  and ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increase in the number of devices capable of communicating with
other devices in their vicinity, new communication topologies arise.
Consequently, it becomes important to investigate such topologies in terms of
performance. In this paper, an elemental network of 3 nodes mutually
communicating to each other is studied. This full-duplex multi-way network is a
suitable model for 3-user device-to-device communications. The main goal of
this paper is to characterize the capacity region of the underlying 3-way
channel (3WC) in the Gaussian setting within a constant gap. To this end, the
3WC is first transformed into an equivalent star-channel. Then, this
star-channel is decomposed into a set of `successive' sub-channels. This
decomposition leads to sub-channel allocation problem, and constitutes a
general framework for studying the capacity of different types of Gaussian
networks. Using backward decoding, interference neutralization, and known
results on the capacity of the star-channel, an achievable rate region for the
3WC is obtained. Furthermore, an outer bound is derived using cut-set bounds
and genie-aided bounds. It is shown that the outer bound and achievable rate
region are within a constant gap. This paper highlights the importance of
physical-layer network-coding schemes in this network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09069</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09069</id><created>2015-10-28</created><authors><author><keyname>Ozgen</keyname><forenames>Oktar</forenames></author><author><keyname>Kallmann</keyname><forenames>Marcelo</forenames></author><author><keyname>Brown</keyname><forenames>Eric</forenames></author></authors><title>Simulating the Dynamic Behavior of Shear Thickening Fluids</title><categories>cs.GR physics.flu-dyn</categories><comments>Work being submitted for peer review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While significant research has been dedicated to the simulation of fluids,
not much attention has been given to exploring new interesting behavior that
can be generated with the different types of non-Newtonian fluids with
non-constant viscosity. Going in this direction, this paper introduces a
computational model for simulating the interesting phenomena observed in
non-Newtonian shear thickening fluids, which are fluids where the viscosity
increases with increased stress. These fluids have unique and unconventional
behavior, and they often appear in real world scenarios such as when sinking in
quicksand or when experimenting with popular cornstarch and water mixtures.
While interesting behavior of shear thickening fluids can be easily observed in
the real world, the most interesting phenomena of these fluids have not been
simulated before in computer graphics. The fluid exhibits unique phase changes
between solid and liquid states, great impact resistance in its solid state and
strong hysteresis effects. Our proposed approach builds on existing
non-Newtonian fluid models in computer graphics and introduces an efficient
history-based stiffness term that is essential to produce the most interesting
shear thickening phenomena. The history-based stiffness is formulated through
the use of fractional derivatives, leveraging the fractional calculus ability
to depict both the viscoelastic behavior and the history effects of
history-dependent systems. Simulations produced by our method are compared
against real experiments and the results demonstrate that the proposed model
successfully captures key phenomena observed in shear thickening fluids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09077</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09077</id><created>2015-10-29</created><authors><author><keyname>Peake</keyname><forenames>Ian D.</forenames></author><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author><author><keyname>Thomas</keyname><forenames>Ian</forenames></author><author><keyname>May</keyname><forenames>Nicholas</forenames></author><author><keyname>Schmidt</keyname><forenames>Heinz W.</forenames></author><author><keyname>Fernando</keyname><forenames>Lasith</forenames></author><author><keyname>Sreenivasamurthy</keyname><forenames>Ravi</forenames></author></authors><title>The Virtual Experiences Lab - a platform for global collaborative
  engineering and beyond</title><categories>cs.OH</categories><comments>This is a pre-print of an extended talk abstract accepted for
  eResearch Australasia, Brisbane, October 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are developing the Virtual Experiences (Vx)Lab, a research and research
training infrastructure and capability platform for global collaboration. VxLab
comprises labs with visualisation capabilities, including underpinning
networking to global points of presence, videoconferencing and high-performance
computation, simulation and rendering, and sensors and actuators such as
robotic instruments locally and in connected remote labs. VxLab has been used
for industry projects in industrial automation, experimental research in cloud
deployment, workshops and remote capability demonstrations, teaching
advanced-level courses in games development, and student software engineering
projects. Our goal is for resources to become a &quot;catalyst&quot; for IT-driven
research results both within the university and with external industry
partners. Use cases include: multi-disciplinary collaboration, prototyping and
troubleshooting requiring multiple viewpoints and architectures, dashboards and
decision support for global remote planning and operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09079</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09079</id><created>2015-10-30</created><authors><author><keyname>Gatti</keyname><forenames>Lorenzo</forenames></author><author><keyname>Guerini</keyname><forenames>Marco</forenames></author><author><keyname>Turchi</keyname><forenames>Marco</forenames></author></authors><title>SentiWords: Deriving a High Precision and High Coverage Lexicon for
  Sentiment Analysis</title><categories>cs.CL</categories><comments>in Affective Computing, IEEE Transactions on (2015)</comments><doi>10.1109/TAFFC.2015.2476456</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deriving prior polarity lexica for sentiment analysis - where positive or
negative scores are associated with words out of context - is a challenging
task. Usually, a trade-off between precision and coverage is hard to find, and
it depends on the methodology used to build the lexicon. Manually annotated
lexica provide a high precision but lack in coverage, whereas automatic
derivation from pre-existing knowledge guarantees high coverage at the cost of
a lower precision. Since the automatic derivation of prior polarities is less
time consuming than manual annotation, there has been a great bloom of these
approaches, in particular based on the SentiWordNet resource. In this paper, we
compare the most frequently used techniques based on SentiWordNet with newer
ones and blend them in a learning framework (a so called 'ensemble method'). By
taking advantage of manually built prior polarity lexica, our ensemble method
is better able to predict the prior value of unseen words and to outperform all
the other SentiWordNet approaches. Using this technique we have built
SentiWords, a prior polarity lexicon of approximately 155,000 words, that has
both a high precision and a high coverage. We finally show that in sentiment
analysis tasks, using our lexicon allows us to outperform both the single
metrics derived from SentiWordNet and popular manually annotated sentiment
lexica.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09083</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09083</id><created>2015-10-30</created><updated>2015-11-12</updated><authors><author><keyname>Lai</keyname><forenames>Hanjiang</forenames></author><author><keyname>Xiao</keyname><forenames>Shengtao</forenames></author><author><keyname>Cui</keyname><forenames>Zhen</forenames></author><author><keyname>Pan</keyname><forenames>Yan</forenames></author><author><keyname>Xu</keyname><forenames>Chunyan</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Deep Cascaded Regression for Face Alignment</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel cascaded regression framework for face alignment based on
a deep convolutional neural network (CNN). In most existing cascaded regression
methods, the shape-indexed features are either obtained by hand-crafted visual
descriptors or by leaning from the shallow models. This setting may be
suboptimal for the face alignment task. To solve this problem, we propose an
end-to-end CNN architecture to learn highly discriminative shape-indexed
features. First, our deep architecture encodes the image into high-level
feature maps in the same size of the image via three main operations:
convolution, pooling and deconvolution. Then, we propose &quot;Shape-Indexed
Pooling&quot; to extract the deep features from these high level descriptors. We
refine the shape via sequential regressions by using the deep shape-indexed
features, which demonstrates outstanding performance. We also propose to learn
the probability mask for each landmark that can be used to choose the
initialization from the shape space. Extensive evaluations conducted on several
benchmark datasets demonstrate that the proposed deep framework shows
significant improvement over the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09085</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09085</id><created>2015-10-30</created><authors><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Accelerating coordination in temporal networks by engineering the link
  order</title><categories>physics.soc-ph cs.SI</categories><comments>5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social dynamics on a network may be accelerated or decelerated depending on
which pairs of individuals in the network communicate early and which pairs do
later. The order with which the links in a given network are sequentially used,
which we call the link order, may be a strong determinant of dynamical
behaviour on networks, potentially adding a new dimension to effects of
temporal networks relative to static networks. Here we study the effect of the
link order on linear synchronisation dynamics. We show that the synchronisation
speed considerably depends on specific orders of links. In addition, applying
each single link for a long time to ensure strong pairwise synchronisation
before moving to a next pair of individuals does not often enhance
synchronisation of the entire network. We also implement a simple greedy
algorithm to optimise the link order in favour of fast synchronisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09091</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09091</id><created>2015-10-30</created><updated>2016-02-01</updated><authors><author><keyname>Bjelakovic</keyname><forenames>Igor</forenames></author><author><keyname>Mohammadi</keyname><forenames>Jafar</forenames></author><author><keyname>Sta&#x144;czak</keyname><forenames>S&#x142;awomir</forenames></author></authors><title>Strong Secrecy and Stealth for Broadcast Channels with Confidential
  Messages</title><categories>cs.IT math.IT</categories><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a discrete memoryless broadcast channel consists of two users and
a sender. The sender has two independent confidential messages for each user.
We extend the work of Liu et al.\ on broadcast channels with two confidential
messages with weak secrecy criterion to strong secrecy. Our results are based
on an extension of the techniques developed by Hou and Kramer on bounding
Kullback-Leibler divergence in context of \textit{resolvability} and
\textit{effective secrecy}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09092</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09092</id><created>2015-10-30</created><authors><author><keyname>Ramos</keyname><forenames>Marcus V. M.</forenames></author><author><keyname>de Queiroz</keyname><forenames>Ruy J. G. B.</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author><author><keyname>Almeida</keyname><forenames>Jos&#xe9; Carlos Bacelar</forenames></author></authors><title>Formalization of context-free language theory</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-free language theory is a subject of high importance in computer
language processing technology as well as in formal language theory. This paper
presents a formalization, using the Coq proof assistant, of fundamental results
related to context-free grammars and languages. These include closure
properties (union, concatenation and Kleene star), grammar simplification
(elimination of useless symbols inaccessible symbols, empty rules and unit
rules) and the existence of a Chomsky Normal Form for context-free grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09093</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09093</id><created>2015-10-30</created><authors><author><keyname>Berntsen</keyname><forenames>Alexander</forenames></author><author><keyname>Ellingsen</keyname><forenames>Stian</forenames></author><author><keyname>Flakk</keyname><forenames>Emil Henry</forenames></author></authors><title>Enabling Learning by Teaching: Intuitive Composing of E-Learning Modules</title><categories>cs.CY</categories><comments>20 pages + 26 pages appendix -- 46 pages in total, 7 figures</comments><acm-class>K.3.1</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In an effort to foster learning by teaching, we propose the development of a
canvas system that makes composing e-learning modules intuitive. We try to
empower and liberate non-technical module users by lowering the bar for turning
them into module authors, a bar previously set far too high. In turn, this
stimulates learning through teaching. By making a damn fine piece of software,
we furthermore make module authoring more pleasant for experienced authors as
well. We propose a system that initially enables users to easily compose H5P
modules. These modules are successively easy to share and modify. Through
gamification we encourage authors to share their work, and to improve the works
of others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09095</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09095</id><created>2015-10-30</created><authors><author><keyname>Dahlqvist</keyname><forenames>Fredrik</forenames></author></authors><title>Completeness-via-canonicity for coalgebraic logics</title><categories>cs.LO math.LO</categories><comments>PhD thesis. 288 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis aims to provide a suite of techniques to generate completeness
results for coalgebraic logics with axioms of arbitrary rank. We have chosen to
investigate the possibility to generalize what is arguably one of the most
successful methods to prove completeness results in `classical' modal logic,
namely completeness-via-canonicity. This technique is particularly well-suited
to a coalgebraic generalization because of its clean and abstract algebraic
formalism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09099</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09099</id><created>2015-10-30</created><authors><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author><author><keyname>Henneken</keyname><forenames>Edwin A.</forenames></author></authors><title>Measuring Metrics - A forty year longitudinal cross-validation of
  citations, downloads, and peer review in Astrophysics</title><categories>physics.soc-ph astro-ph.IM cs.DL</categories><comments>Author's version of manuscript accepted for publication in the
  Journal of the Association for Information Science and Technology (JASIST);
  35 pages 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation measures, and newer altmetric measures such as downloads are now
commonly used to inform personnel decisions. How well do or can these measures
measure or predict the past, current of future scholarly performance of an
individual? Using data from the Smithsonian/NASA Astrophysics Data System we
analyze the publication, citation, download, and distinction histories of a
cohort of 922 individuals who received a U.S. PhD in astronomy in the period
1972-1976. By examining the same and different measures at the same and
different times for the same individuals we are able to show the capabilities
and limitations of each measure. Because the distributions are lognormal
measurement uncertainties are multiplicative; we show that in order to state
with 95% confidence that one person's citations and/or downloads are
significantly higher than another person's, the log difference in the ratio of
counts must be at least 0.3 dex, which corresponds to a multiplicative factor
of two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09102</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09102</id><created>2015-10-30</created><updated>2016-01-08</updated><authors><author><keyname>Fijalkow</keyname><forenames>Nathanael</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Shirmohammadi</keyname><forenames>Mahsa</forenames></author></authors><title>Trace Refinement in Labelled Markov Decision Processes</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two labelled Markov decision processes (MDPs), the trace-refinement
problem asks whether for all strategies of the first MDP there exists a
strategy of the second MDP such that the induced labelled Markov chains are
trace-equivalent. We show that this problem is decidable in polynomial time if
the second MDP is a Markov chain. The algorithm is based on new results on a
particular notion of bisimulation between distributions over the states.
However, we show that the general trace-refinement problem is undecidable, even
if the first MDP is a Markov chain. Decidability of those problems was stated
as open in 2008. We further study the decidability and complexity of the
trace-refinement problem provided that the strategies are restricted to be
memoryless.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09106</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09106</id><created>2015-10-30</created><authors><author><keyname>Hota</keyname><forenames>Ashish R.</forenames></author><author><keyname>Sundaram</keyname><forenames>Shreyas</forenames></author></authors><title>Interdependent Security Games on Networks under Behavioral Probability
  Weighting</title><categories>cs.GT cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of interdependent security games on networks where each
node chooses a personal level of security investment. The attack probability
experienced by a node is a function of her own investment and the investment by
her neighbors in the network. Most of the existing work in these settings
consider players who are risk neutral or expected value maximizers. In
contrast, studies in behavioral decision theory have shown that individuals
often deviate from risk neutral behavior while making decisions under
uncertainty. In particular, the true probabilities associated with uncertain
outcomes are often transformed into perceived probabilities in a highly
nonlinear fashion by the users, which then influence their decisions. In this
paper, we investigate the effects of such behavioral probability weightings by
the nodes on their optimal investment strategies and the resulting security
risk profiles that arise in the Nash equilibria of interdependent network
security games. We characterize graph topologies that achieve the largest and
smallest worst case average attack probabilities at Nash equilibria in Total
Effort games, and equilibrium investments in Weakest Link and Best Shot games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09115</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09115</id><created>2015-10-30</created><authors><author><keyname>Bellaiche</keyname><forenames>Levi-Itzhak</forenames></author><author><keyname>Bruckstein</keyname><forenames>Alfred</forenames></author></authors><title>Continuous Time Gathering of Agents with Limited Visibility and
  Bearing-Only Sensing</title><categories>cs.MA cs.SY</categories><comments>30 pages, 24 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A group of mobile agents, identical, anonymous, and oblivious (memoryless),
having the capability to sense only the relative direction (bearing) to
neighborhing agents within a finite visibility range, are shown to gather to a
meeting point in finite time by applying a very simple rule of motion. The
agents' rule of motion is : set your velocity vector to be the sum of the two
unit vectors in R^2 pointing to your &quot;extremal&quot; neighbours determining the
smallest visibility disc sector in which all your visible neighbors reside,
provided it spans an angle smaller than pi, otherwise, since you are
&quot;surrounded&quot; by visible neighbors, simply stay put (set your velocity to 0). Of
course, the initial constellation of agents must have a visibility graph that
is connected, and provided this we prove that the agents gather to a common
meeting point in finite time, while the distances between agents that initially
see each other monotically decreases. We will also prove a geometrical result,
a tight lower bound on the sum of cosines of the interior angles of a convex
polygon, that we will use to prove the gathering of our dynamical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09117</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09117</id><created>2015-10-30</created><authors><author><keyname>Skipsey</keyname><forenames>Samuel Cadellin</forenames></author><author><keyname>Todev</keyname><forenames>Paulin</forenames></author><author><keyname>Britton</keyname><forenames>David</forenames></author><author><keyname>Crooks</keyname><forenames>David</forenames></author><author><keyname>Roy</keyname><forenames>Gareth</forenames></author></authors><title>Extending DIRAC File Management with Erasure-Coding for efficient
  storage</title><categories>cs.DC hep-ex</categories><comments>21st International Conference on Computing for High Energy and
  Nuclear Physics (CHEP2015)</comments><doi>10.1088/1742-6596/664/4/042051</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The state of the art in Grid style data management is to achieve increased
resilience of data via multiple complete replicas of data files across multiple
storage endpoints. While this is effective, it is not the most space-efficient
approach to resilience, especially when the reliability of individual storage
endpoints is sufficiently high that only a few will be inactive at any point in
time. We report on work performed as part of GridPP\cite{GridPP}, extending the
Dirac File Catalogue and file management interface to allow the placement of
erasure-coded files: each file distributed as N identically-sized chunks of
data striped across a vector of storage endpoints, encoded such that any M
chunks can be lost and the original file can be reconstructed. The tools
developed are transparent to the user, and, as well as allowing up and
downloading of data to Grid storage, also provide the possibility of
parallelising access across all of the distributed chunks at once, improving
data transfer and IO performance. We expect this approach to be of most
interest to smaller VOs, who have tighter bounds on the storage available to
them, but larger (WLCG) VOs may be interested as their total data increases
during Run 2. We provide an analysis of the costs and benefits of the approach,
along with future development and implementation plans in this area. In
general, overheads for multiple file transfers provide the largest issue for
competitiveness of this approach at present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09119</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09119</id><created>2015-10-30</created><authors><author><keyname>Imbs</keyname><forenames>Damien</forenames></author><author><keyname>Rajsbaum</keyname><forenames>Sergio</forenames></author><author><keyname>Raynal</keyname><forenames>Michel</forenames></author><author><keyname>Stainer</keyname><forenames>Julien</forenames></author></authors><title>Byzantine BG Simulations for Message-Passing Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The BG-simulation is a powerful reduction algorithm designed for asynchronous
read/write crash-prone systems, namely, it allows a set of $(t+1)$ asynchronous
sequential processes to wait-free simulate (i.e., despite the crash of up to
$t$ of them) an arbitrary number $n$ of processes under the assumption that at
most $t$ of them crash. The BG simulation shows that, in read/write systems,
the crucial parameter is not the number $n$ of processes, but the upper bound
$t$ on the number of process crashes.
  The paper first focuses on BG-like simulations in the context of asynchronous
message-passing systems. It incrementally presents two reductions. The first
considers that processes may fail by crashing. Assuming $t&lt;\min(n',n/2)$, it
simulates a system of $n'$ processes where up to $t$ may crash, on top of a
basic system of $n$ processes where up to $t$ may crash. The second simulation
concerns the case where processes may commit Byzantine failures. Assuming
$t&lt;\min(n',n/3)$, it simulates a system of $n'$ processes where up to $t$ may
be Byzantine, on top of a basic system of $n$ processes where up to $t$ may be
Byzantine. It is important to notice that this algorithm is the first BG
simulation algorithm that considers Byzantine process failures. These
simulations, suited to asynchronous message-passing systems, are genuine in the
sense that they do not rely on an intermediate stage simulating a read/write
memory system. Moreover, they are built modularly.
  Each of the two new BG-like simulations relies on novel specific safe
agreement objects. The one suited to message-passing Byzantine systems has
noteworthy computability implications. It allows crash-tolerant algorithms,
designed for asynchronous read/write systems, to be executed on top of
asynchronous message-passing systems prone to Byzantine failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09123</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09123</id><created>2015-10-30</created><authors><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author><author><keyname>Zheng</keyname><forenames>Yan</forenames></author></authors><title>Subsampling in Smoothed Range Spaces</title><categories>cs.CG cs.LG</categories><comments>This is the full version of the paper which appeared in ALT 2015. 16
  pages, 3 figures. In Algorithmic Learning Theory, pp. 224-238. Springer
  International Publishing, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider smoothed versions of geometric range spaces, so an element of the
ground set (e.g. a point) can be contained in a range with a non-binary value
in $[0,1]$. Similar notions have been considered for kernels; we extend them to
more general types of ranges. We then consider approximations of these range
spaces through $\varepsilon $-nets and $\varepsilon $-samples (aka
$\varepsilon$-approximations). We characterize when size bounds for
$\varepsilon $-samples on kernels can be extended to these more general
smoothed range spaces. We also describe new generalizations for $\varepsilon
$-nets to these range spaces and show when results from binary range spaces can
carry over to these smoothed ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09124</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09124</id><created>2015-10-30</created><updated>2016-01-22</updated><authors><author><keyname>Zhu</keyname><forenames>Guangxu</forenames></author><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author></authors><title>Analog Spatial Cancellation for Tackling the Near-Far Problem in
  Wirelessly Powered Communications</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE JSAC Series on Green Communications and Networking
  (Issue 3)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implementation of wireless power transfer in wireless communication
systems opens up a new research area, known as wirelessly powered
communications (WPC). In next-generation heterogeneous networks where
ultra-dense small-cell base stations are deployed,
simultaneous-wireless-information-and-power-transfer (SWIPT) is feasible over
short ranges. One challenge for designing a WPC system is the severe near-far
problem where a user attempts to decode an information-transfer (IT) signal in
the presence of extremely strong SWIPT signals. Jointly quantizing the mixed
signals causes the IT signal to be completely corrupted by quantization noise
and thus the SWIPT signals have to be suppressed in the analog domain. This
motivates the design of a framework in this paper for analog spatial
cancellation in a multi-antenna WPC system. In the framework, an analog circuit
consisting of simple phase shifters and adders, is adapted to cancel the SWIPT
signals by multiplying it with a cancellation matrix having unit-modulus
elements and full rank, where the full rank retains the spatial-multiplexing
gain of the IT channel. The unit-modulus constraints render the conventional
zero-forcing method unsuitable. Therefore, the paper presents a novel
systematic approach for constructing cancellation matrices. For the
single-SWIPT-interferer case, the matrices are obtained as truncated
Fourier/Hadamard matrices after compensating for propagation phase shifts over
the SWIPT channel. For the more challenging multiple-SWIPT-interferer case, it
is proposed that each row of the cancellation matrix is constructed as a
Kronecker-product of component vectors, with each component vectors designed to
null the signal from a corresponding SWIPT interferer similarly as in the
preceding case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09129</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09129</id><created>2015-10-30</created><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>A weakly universal cellular automaton on the pentagrid with three states</title><categories>cs.DM nlin.CG</categories><comments>40 pages, 30 figures</comments><msc-class>68R05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove that there is a weakly universal cellular automaton
on the pentagrid with three states which is rotation invariant and which uses
\`a la Moore neighbourhood. Moreover, at each step of the computation, the set
of non quiescent states has always infinitely many cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09130</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09130</id><created>2015-10-30</created><authors><author><keyname>Zhong</keyname><forenames>Mingjun</forenames></author><author><keyname>Goddard</keyname><forenames>Nigel</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>Latent Bayesian melding for integrating individual and population models</title><categories>stat.ML cs.AI stat.AP stat.ME</categories><comments>11 pages, Advances in Neural Information Processing Systems (NIPS),
  2015. (Spotlight Presentation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many statistical problems, a more coarse-grained model may be suitable for
population-level behaviour, whereas a more detailed model is appropriate for
accurate modelling of individual behaviour. This raises the question of how to
integrate both types of models. Methods such as posterior regularization follow
the idea of generalized moment matching, in that they allow matching
expectations between two models, but sometimes both models are most
conveniently expressed as latent variable models. We propose latent Bayesian
melding, which is motivated by averaging the distributions over populations
statistics of both the individual-level and the population-level models under a
logarithmic opinion pool framework. In a case study on electricity
disaggregation, which is a type of single-channel blind source separation
problem, we show that latent Bayesian melding leads to significantly more
accurate predictions than an approach based solely on generalized moment
matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09142</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09142</id><created>2015-10-30</created><authors><author><keyname>Heess</keyname><forenames>Nicolas</forenames></author><author><keyname>Wayne</keyname><forenames>Greg</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author><author><keyname>Lillicrap</keyname><forenames>Timothy</forenames></author><author><keyname>Tassa</keyname><forenames>Yuval</forenames></author><author><keyname>Erez</keyname><forenames>Tom</forenames></author></authors><title>Learning Continuous Control Policies by Stochastic Value Gradients</title><categories>cs.LG cs.NE</categories><comments>13 pages, NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a unified framework for learning continuous control policies using
backpropagation. It supports stochastic control by treating stochasticity in
the Bellman equation as a deterministic function of exogenous noise. The
product is a spectrum of general policy gradient algorithms that range from
model-free methods with value functions to model-based methods without value
functions. We use learned models but only require observations from the
environment in- stead of observations from model-predicted trajectories,
minimizing the impact of compounded model errors. We apply these algorithms
first to a toy stochastic control problem and then to several physics-based
control problems in simulation. One of these variants, SVG(1), shows the
effectiveness of learning models, value functions, and policies simultaneously
in continuous domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09155</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09155</id><created>2015-10-30</created><authors><author><keyname>Hayamizu</keyname><forenames>Momoko</forenames></author><author><keyname>Endo</keyname><forenames>Hiroshi</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author></authors><title>A characterization of minimum spanning tree-like metric spaces</title><categories>q-bio.QM cs.DM q-bio.PE</categories><comments>9 pages, 2 figures</comments><msc-class>Primary 05C12, Secondary 05C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed a surge of biological interest in the minimum
spanning tree (MST) problem for its relevance to automatic model construction
using the distances between data points. Despite the increasing use of MST
algorithms for this purpose, the goodness-of-fit of an MST to the data is often
elusive because no quantitative criteria have been developed to measure it.
Motivated by this, we provide a necessary and sufficient condition to ensure
that a metric space on n points can be represented by a fully labeled tree on n
vertices, and thereby determine when an MST preserves all pairwise distances
between points in a finite metric space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09156</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09156</id><created>2015-10-30</created><authors><author><keyname>Ma</keyname><forenames>Fuda</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author></authors><title>A Multiple Search Operator Heuristic for the Max-k-cut Problem</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The max-k-cut problem is to partition the vertices of a weighted graph $G =
(V,E)$ into $k\geq2$ disjoint subsets such that the weight sum of the edges
crossing the different subsets is maximized. The problem is referred as the
max-cut problem when $k=2$. In this work, we present a multiple operator
heuristic (MOH) for the general max-k-cut problem. MOH employs five distinct
search operators organized into three search phases to effectively explore the
search space. Experiments on two sets of 91 well-known benchmark instances show
that the proposed algorithm is highly effective on the max-k-cut problem and
improves the current best known results (new lower bounds) of most of the
tested instances. For the popular special case $k=2$ (i.e., the max-cut
problem), MOH also performs remarkably well by discovering 6 improved best
known results. We provide additional studies to shed light on the alternative
combinations of the employed search operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09161</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09161</id><created>2015-10-30</created><authors><author><keyname>Campbell</keyname><forenames>Trevor</forenames></author><author><keyname>Straub</keyname><forenames>Julian</forenames></author><author><keyname>Fisher</keyname><forenames>John W.</forenames><suffix>III</suffix></author><author><keyname>How</keyname><forenames>Jonathan P.</forenames></author></authors><title>Streaming, Distributed Variational Inference for Bayesian Nonparametrics</title><categories>cs.LG stat.ML</categories><comments>This paper was presented at NIPS 2015. Please use the following
  BibTeX citation: @inproceedings{Campbell15_NIPS, Author = {Trevor Campbell
  and Julian Straub and John W. {Fisher III} and Jonathan P. How}, Title =
  {Streaming, Distributed Variational Inference for Bayesian Nonparametrics},
  Booktitle = {Advances in Neural Information Processing Systems (NIPS)}, Year
  = {2015}}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a methodology for creating streaming, distributed
inference algorithms for Bayesian nonparametric (BNP) models. In the proposed
framework, processing nodes receive a sequence of data minibatches, compute a
variational posterior for each, and make asynchronous streaming updates to a
central model. In contrast to previous algorithms, the proposed framework is
truly streaming, distributed, asynchronous, learning-rate-free, and
truncation-free. The key challenge in developing the framework, arising from
the fact that BNP models do not impose an inherent ordering on their
components, is finding the correspondence between minibatch and central BNP
posterior components before performing each update. To address this, the paper
develops a combinatorial optimization problem over component correspondences,
and provides an efficient solution technique. The paper concludes with an
application of the methodology to the DP mixture model, with experimental
results demonstrating its practical scalability and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09171</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09171</id><created>2015-10-30</created><authors><author><keyname>Chu</keyname><forenames>Hang</forenames></author><author><keyname>Mei</keyname><forenames>Hongyuan</forenames></author><author><keyname>Bansal</keyname><forenames>Mohit</forenames></author><author><keyname>Walter</keyname><forenames>Matthew R.</forenames></author></authors><title>Accurate Vision-based Vehicle Localization using Satellite Imagery</title><categories>cs.RO cs.CV</categories><comments>9 pages, 8 figures. Full version is submitted to ICRA 2016. Short
  version is to appear at NIPS 2015 Workshop on Transfer and Multi-Task
  Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for accurately localizing ground vehicles with the aid of
satellite imagery. Our approach takes a ground image as input, and outputs the
location from which it was taken on a georeferenced satellite image. We perform
visual localization by estimating the co-occurrence probabilities between the
ground and satellite images based on a ground-satellite feature dictionary. The
method is able to estimate likelihoods over arbitrary locations without the
need for a dense ground image database. We present a ranking-loss based
algorithm that learns location-discriminative feature projection matrices that
result in further improvements in accuracy. We evaluate our method on the
Malaga and KITTI public datasets and demonstrate significant improvements over
a baseline that performs exhaustive search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09184</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09184</id><created>2015-10-30</created><authors><author><keyname>Glenn</keyname><forenames>Taylor</forenames></author><author><keyname>Zare</keyname><forenames>Alina</forenames></author></authors><title>Estimating Target Signatures with Diverse Density</title><categories>cs.CV</categories><comments>Appeared in Proceedings of the 7th IEEE Workshop on Hyperspectral
  Image and Signal Processing: Evolution in Remote Sensing, Tokyo, Japan, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Hyperspectral target detection algorithms rely on knowing the desired target
signature in advance. However, obtaining an effective target signature can be
difficult; signatures obtained from laboratory measurements or
hand-spectrometers in the field may not transfer to airborne imagery
effectively. One approach to dealing with this difficulty is to learn an
effective target signature from training data. An approach for learning target
signatures from training data is presented. The proposed approach addresses
uncertainty and imprecision in groundtruth in the training data using a
multiple instance learning, diverse density (DD) based objective function.
After learning the target signature given data with uncertain and imprecise
groundtruth, target detection can be applied on test data. Results are shown on
simulated and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09185</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09185</id><created>2015-10-30</created><updated>2015-11-12</updated><authors><author><keyname>Wu</keyname><forenames>Huaming</forenames></author><author><keyname>Wolter</keyname><forenames>Katinka</forenames></author></authors><title>Analysis of the Energy-Performance Tradeoff for Delayed Mobile
  Offloading</title><categories>cs.PF cs.DC</categories><comments>This paper has been withdrawn by the author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the author
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09192</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09192</id><created>2015-10-30</created><authors><author><keyname>Huang</keyname><forenames>Shenwei</forenames></author><author><keyname>da Silva</keyname><forenames>Murilo V. G.</forenames></author></authors><title>A note on coloring (even-hole,cap)-free graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A {\em hole} is a chordless cycle of length at least four. A hole is {\em
even} (resp. {\em odd}) if it contains an even (resp. odd) number of vertices.
A \emph{cap} is a graph induced by a hole with an additional vertex that is
adjacent to exactly two adjacent vertices on the hole. In this note, we use a
decomposition theorem by Conforti et al. (1999) to show that if a graph $G$
does not contain any even hole or cap as an induced subgraph, then $\chi(G)\le
\lfloor\frac{3}{2}\omega(G)\rfloor$, where $\chi(G)$ and $\omega(G)$ are the
chromatic number and the clique number of $G$, respectively. This bound is
attained by odd holes and the Hajos graph. The proof leads to a polynomial-time
$3/2$-approximation algorithm for coloring (even-hole,cap)-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09193</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09193</id><created>2015-10-30</created><updated>2016-02-17</updated><authors><author><keyname>Bezakova</keyname><forenames>Ivona</forenames></author><author><keyname>Galanis</keyname><forenames>Andreas</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Guo</keyname><forenames>Heng</forenames></author><author><keyname>Stefankovic</keyname><forenames>Daniel</forenames></author></authors><title>Approximation via Correlation Decay when Strong Spatial Mixing Fails</title><categories>cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate counting via correlation decay is the core algorithmic technique
used in the sharp delineation of the computational phase transition that arises
in the approximation of the partition function of antiferromagnetic two-spin
models.
  Previous analyses of correlation-decay algorithms implicitly depended on the
occurrence of strong spatial mixing. This, roughly, means that one uses
worst-case analysis of the recursive procedure that creates the sub-instances.
We develop a new analysis method that is more refined than the worst-case
analysis. We take the shape of instances in the computation tree into
consideration and we amortise against certain &quot;bad&quot; instances that are created
as the recursion proceeds. This enables us to show correlation decay and to
obtain an FPTAS even when strong spatial mixing fails.
  We apply our technique to the problem of approximately counting independent
sets in hypergraphs with degree upper-bound Delta and with a lower bound k on
the arity of hyperedges. Liu and Lin gave an FPTAS for k&gt;=2 and Delta&lt;=5 (lack
of strong spatial mixing was the obstacle preventing this algorithm from being
generalised to Delta=6). Our technique gives a tight result for Delta=6,
showing that there is an FPTAS for k&gt;=3 and Delta&lt;=6. The best previously-known
approximation scheme for Delta=6 is the Markov-chain simulation based FPRAS of
Bordewich, Dyer and Karpinski, which only works for k&gt;=8.
  Our technique also applies for larger values of k, giving an FPTAS for
k&gt;=1.66*Delta. This bound is not as strong as existing randomised results, for
technical reasons that are discussed in the paper. Nevertheless, it gives the
first deterministic approximation schemes in this regime. We also demonstrate
that in the hypergraph independent set model, approximating the partition
function is NP-hard even within the uniqueness regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09202</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09202</id><created>2015-10-30</created><authors><author><keyname>Guo</keyname><forenames>Hongyu</forenames></author></authors><title>Generating Text with Deep Reinforcement Learning</title><categories>cs.CL cs.LG cs.NE</categories><comments>Accepted to the NIPS2015 Deep Reinforcement Learning Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel schema for sequence to sequence learning with a Deep
Q-Network (DQN), which decodes the output sequence iteratively. The aim here is
to enable the decoder to first tackle easier portions of the sequences, and
then turn to cope with difficult parts. Specifically, in each iteration, an
encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the
input sequence, automatically create features to represent the internal states
of and formulate a list of potential actions for the DQN. Take rephrasing a
natural sentence as an example. This list can contain ranked potential words.
Next, the DQN learns to make decision on which action (e.g., word) will be
selected from the list to modify the current decoded sequence. The newly
modified output sequence is subsequently used as the input to the DQN for the
next decoding iteration. In each iteration, we also bias the reinforcement
learning's attention to explore sequence portions which are previously
difficult to be decoded. For evaluation, the proposed strategy was trained to
decode ten thousands natural sentences. Our experiments indicate that, when
compared to a left-to-right greedy beam search LSTM decoder, the proposed
method performed competitively well when decoding sentences from the training
set, but significantly outperformed the baseline when decoding unseen
sentences, in terms of BLEU score obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09203</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09203</id><created>2015-10-30</created><authors><author><keyname>Peng</keyname><forenames>Chi-Han</forenames></author><author><keyname>Mitra</keyname><forenames>Niloy J.</forenames></author><author><keyname>Bao</keyname><forenames>Fan</forenames></author><author><keyname>Yan</keyname><forenames>Dong-Ming</forenames></author><author><keyname>Wonka</keyname><forenames>Peter</forenames></author></authors><title>Computational Network Design from Functional Specifications</title><categories>cs.CG cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connectivity and layout of underlying networks largely determine the behavior
of many environments. For example, transportation networks determine the flow
of traffic in cities, or maps determine the difficulty and flow in games.
Designing such networks from scratch is challenging as even local network
changes can have large global effects. We investigate how to computationally
create networks starting from {\em only} high-level functional specifications.
Such specifications can be in the form of network density, travel time versus
network length, traffic type, destination locations, etc. We propose an integer
programming-based approach that guarantees that the resultant networks are
valid by fulfilling all specified hard constraints, and score favorably in
terms of the objective function. We evaluate our algorithm in three different
design settings (i.e., street layout, floorplanning, and game level design) and
demonstrate, for the first time, that diverse networks can emerge purely from
high-level functional specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.09219</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.09219</id><created>2015-10-30</created><authors><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Submatrix localization via message passing</title><categories>stat.ML cs.IT cs.SI math.IT math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The principal submatrix localization problem deals with recovering a $K\times
K$ principal submatrix of elevated mean $\mu$ in a large $n\times n$ symmetric
matrix subject to additive standard Gaussian noise. This problem serves as a
prototypical example for community detection, in which the community
corresponds to the support of the submatrix. The main result of this paper is
that in the regime $\Omega(\sqrt{n}) \leq K \leq o(n)$, the support of the
submatrix can be weakly recovered (with $o(K)$ misclassification errors on
average) by an optimized message passing algorithm if $\lambda = \mu^2K^2/n$,
the signal-to-noise ratio, exceeds $1/e$. This extends a result by Deshpande
and Montanari previously obtained for $K=\Theta(\sqrt{n}).$ In addition, the
algorithm can be extended to provide exact recovery whenever
information-theoretically possible and achieve the information limit of exact
recovery as long as $K \geq \frac{n}{\log n} (\frac{1}{8e} + o(1))$. The total
running time of the algorithm is $O(n^2\log n)$.
  Another version of the submatrix localization problem, known as noisy
biclustering, aims to recover a $K_1\times K_2$ submatrix of elevated mean
$\mu$ in a large $n_1\times n_2$ Gaussian matrix. The optimized message passing
algorithm and its analysis are adapted to the bicluster problem assuming
$\Omega(\sqrt{n_i}) \leq K_i \leq o(n_i)$ and $K_1\asymp K_2.$ A sharp
information-theoretic condition for the weak recovery of both clusters is also
identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00021</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00021</id><created>2015-10-30</created><updated>2016-01-07</updated><authors><author><keyname>Glover</keyname><forenames>Fred</forenames></author><author><keyname>Shylo</keyname><forenames>Vladimir</forenames></author><author><keyname>Shylo</keyname><forenames>Oleg</forenames></author></authors><title>Narrow Gauge and Analytical Branching Strategies for Mixed Integer
  Programming</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the-art branch and bound algorithms for mixed integer programming
make use of special methods for making branching decisions. Strategies that
have gained prominence include modern variants of so-called strong branching
(Applegate, et al.,1995) and reliability branching (Achterberg, Koch and
Martin, 2005; Hendel, 2015), which select variables for branching by solving
associated linear programs and exploit pseudo-costs (Benichou et al., 1971). We
suggest new branching criteria and propose alternative branching approaches
called narrow gauge and analytical branching. The perspective underlying our
approaches is to focus on prioritization of child nodes to examine fewer
candidate variables at the current node of the B&amp;B tree, balanced with
procedures to extrapolate the implications of choosing these candidates by
generating a small-depth look-ahead tree. Our procedures can also be used in
rules to select among open tree nodes (those whose child nodes have not yet
been generated). We incorporate pre- and post-winnowing procedures to
progressively isolate preferred branching candidates, and employ derivative
(created) variables whose branches are able to explore the solution space more
deeply.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00040</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00040</id><created>2015-10-30</created><updated>2015-11-03</updated><authors><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author></authors><title>Quantifying the Cognitive Extent of Science</title><categories>cs.DL astro-ph.IM cs.CL physics.soc-ph</categories><comments>Accepted for publication in Journal of Informetrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the modern science is characterized by an exponential growth in
scientific literature, the increase in publication volume clearly does not
reflect the expansion of the cognitive boundaries of science. Nevertheless,
most of the metrics for assessing the vitality of science or for making funding
and policy decisions are based on productivity. Similarly, the increasing level
of knowledge production by large science teams, whose results often enjoy
greater visibility, does not necessarily mean that &quot;big science&quot; leads to
cognitive expansion. Here we present a novel, big-data method to quantify the
extents of cognitive domains of different bodies of scientific literature
independently from publication volume, and apply it to 20 million articles
published over 60-130 years in physics, astronomy, and biomedicine. The method
is based on the lexical diversity of titles of fixed quotas of research
articles. Owing to large size of quotas, the method overcomes the inherent
stochasticity of article titles to achieve &lt;1% precision. We show that the
periods of cognitive growth do not necessarily coincide with the trends in
publication volume. Furthermore, we show that the articles produced by larger
teams cover significantly smaller cognitive territory than (the same quota of)
articles from smaller teams. Our findings provide a new perspective on the role
of small teams and individual researchers in expanding the cognitive boundaries
of science. The proposed method of quantifying the extent of the cognitive
territory can also be applied to study many other aspects of &quot;science of
science.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00041</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00041</id><created>2015-10-30</created><authors><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Kocaoglu</keyname><forenames>Murat</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Learning Causal Graphs with Small Interventions</title><categories>cs.AI cs.IT cs.LG math.IT stat.ML</categories><comments>Accepted to NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning causal networks with interventions, when
each intervention is limited in size under Pearl's Structural Equation Model
with independent errors (SEM-IE). The objective is to minimize the number of
experiments to discover the causal directions of all the edges in a causal
graph. Previous work has focused on the use of separating systems for complete
graphs for this task. We prove that any deterministic adaptive algorithm needs
to be a separating system in order to learn complete graphs in the worst case.
In addition, we present a novel separating system construction, whose size is
close to optimal and is arguably simpler than previous work in combinatorics.
We also develop a novel information theoretic lower bound on the number of
interventions that applies in full generality, including for randomized
adaptive learning algorithms.
  For general chordal graphs, we derive worst case lower bounds on the number
of interventions. Building on observations about induced trees, we give a new
deterministic adaptive algorithm to learn directions on any chordal skeleton
completely. In the worst case, our achievable scheme is an
$\alpha$-approximation algorithm where $\alpha$ is the independence number of
the graph. We also show that there exist graph classes for which the sufficient
number of experiments is close to the lower bound. In the other extreme, there
are graph classes for which the required number of experiments is
multiplicatively $\alpha$ away from our lower bound.
  In simulations, our algorithm almost always performs very close to the lower
bound, while the approach based on separating systems for complete graphs is
significantly worse for random chordal graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00043</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00043</id><created>2015-10-30</created><updated>2015-11-20</updated><authors><author><keyname>Sinha</keyname><forenames>Arunesh</forenames></author><author><keyname>Kar</keyname><forenames>Debarun</forenames></author><author><keyname>Tambe</keyname><forenames>Milind</forenames></author></authors><title>Learning Adversary Behavior in Security Games: A PAC Model Perspective</title><categories>cs.AI cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent applications of Stackelberg Security Games (SSG), from wildlife crime
to urban crime, have employed machine learning tools to learn and predict
adversary behavior using available data about defender-adversary interactions.
Given these recent developments, this paper commits to an approach of directly
learning the response function of the adversary. Using the PAC model, this
paper lays a firm theoretical foundation for learning in SSGs (e.g.,
theoretically answer questions about the numbers of samples required to learn
adversary behavior) and provides utility guarantees when the learned adversary
model is used to plan the defender's strategy. The paper also aims to answer
practical questions such as how much more data is needed to improve an
adversary model's accuracy. Additionally, we explain a recently observed
phenomenon that prediction accuracy of learned adversary behavior is not enough
to discover the utility maximizing defender strategy. We provide four main
contributions: (1) a PAC model of learning adversary response functions in
SSGs; (2) PAC-model analysis of the learning of key, existing bounded
rationality models in SSGs; (3) an entirely new approach to adversary modeling
based on a non-parametric class of response functions with PAC-model analysis
and (4) identification of conditions under which computing the best defender
strategy against the learned adversary behavior is indeed the optimal strategy.
Finally, we conduct experiments with real-world data from a national park in
Uganda, showing the benefit of our new adversary modeling approach and
verification of our PAC model predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00048</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00048</id><created>2015-10-30</created><authors><author><keyname>Lattimore</keyname><forenames>Tor</forenames></author></authors><title>The Pareto Regret Frontier for Bandits</title><categories>cs.LG</categories><comments>14 pages. To appear at NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a multi-armed bandit problem it may be desirable to achieve a
smaller-than-usual worst-case regret for some special actions. I show that the
price for such unbalanced worst-case regret guarantees is rather high.
Specifically, if an algorithm enjoys a worst-case regret of B with respect to
some action, then there must exist another action for which the worst-case
regret is at least {\Omega}(nK/B), where n is the horizon and K the number of
actions. I also give upper bounds in both the stochastic and adversarial
settings showing that this result cannot be improved. For the stochastic case
the pareto regret frontier is characterised exactly up to constant factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00050</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00050</id><created>2015-10-30</created><authors><author><keyname>Solovyev</keyname><forenames>Victor</forenames></author><author><keyname>Umarov</keyname><forenames>Ramzan</forenames></author></authors><title>FendOff encryption software to secure personal information on computers
  and mobile devices</title><categories>cs.CR</categories><comments>7 pages, 9 figures, 1 table</comments><msc-class>94 A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes several original cryptographic cipher modules (VSEM) that
are based on using one time pseudorandom pad and pseudorandom transpositions.
The VSEM includes 4 modules of encryption that can be applied in combinations.
We studied ability of these modules to secure the private data against attacks
and their speed of encryption. The VSEM encryption was implemented in Fendoff
applications for mobile devices on iOS and Android platforms as well as in
computer application running Window or Mac OS. We describe these applications
designed to encrypt/decrypt various personal data such as passwords, credit
card or bank information as well as to secure content of any text or image
files.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00051</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00051</id><created>2015-10-30</created><updated>2016-02-01</updated><authors><author><keyname>Sengupta</keyname><forenames>Abhronil</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Short-Term Plasticity and Long-Term Potentiation in Magnetic Tunnel
  Junctions: Towards Volatile Synapses</title><categories>cs.ET</categories><comments>The article will appear in a future issue of Physical Review Applied</comments><journal-ref>Phys. Rev. Applied 5, 024012 (2016)</journal-ref><doi>10.1103/PhysRevApplied.5.024012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synaptic memory is considered to be the main element responsible for learning
and cognition in humans. Although traditionally non-volatile long-term
plasticity changes have been implemented in nanoelectronic synapses for
neuromorphic applications, recent studies in neuroscience have revealed that
biological synapses undergo meta-stable volatile strengthening followed by a
long-term strengthening provided that the frequency of the input stimulus is
sufficiently high. Such &quot;memory strengthening&quot; and &quot;memory decay&quot;
functionalities can potentially lead to adaptive neuromorphic architectures. In
this paper, we demonstrate the close resemblance of the magnetization dynamics
of a Magnetic Tunnel Junction (MTJ) to short-term plasticity and long-term
potentiation observed in biological synapses. We illustrate that, in addition
to the magnitude and duration of the input stimulus, frequency of the stimulus
plays a critical role in determining long-term potentiation of the MTJ. Such
MTJ synaptic memory arrays can be utilized to create compact, ultra-fast and
low power intelligent neural systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00053</identifier>
 <datestamp>2016-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00053</id><created>2015-10-30</created><authors><author><keyname>Biedermann</keyname><forenames>Daniel H.</forenames></author><author><keyname>Kielar</keyname><forenames>Peter M.</forenames></author><author><keyname>Aumann</keyname><forenames>Quirin</forenames></author><author><keyname>Osorio</keyname><forenames>Carlos M.</forenames></author><author><keyname>Lai</keyname><forenames>Celeste T. W.</forenames></author></authors><title>CarPed -- A Hybrid and Macroscopic Traffic and Pedestrian Simulator</title><categories>cs.MA</categories><comments>8 pages. In: Proceedings of the 27th Forum Bauinformatik, Aachen,
  Germany, 2015</comments><msc-class>90B20</msc-class><doi>10.13140/RG.2.1.3665.4562</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dense human flow has been a concern for the safety of public events for a
long time. Macroscopic pedestrian models, which are mainly based on fluid
dynamics, are often used to simulate huge crowds due to their low computational
costs (Columbo &amp; Rosini 2005). Similar approaches are used in the field of
traffic simulations (Lighthill &amp; Whitham 1955). A combined macroscopic
simulation of vehicles and pedestrians is extremely helpful for
all-encompassing traffic control. Therefore, we developed a hybrid model that
contains networks for vehicular traffic and human flow. This comprehensive
model supports concurrent multi-modal simulations of traffic and pedestrians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00054</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00054</id><created>2015-10-30</created><authors><author><keyname>Moore</keyname><forenames>David A.</forenames></author><author><keyname>Russell</keyname><forenames>Stuart J.</forenames></author></authors><title>Gaussian Process Random Fields</title><categories>cs.LG stat.ML</categories><comments>Advances in Neural Information Processing Systems (NIPS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes have been successful in both supervised and unsupervised
machine learning tasks, but their computational complexity has constrained
practical applications. We introduce a new approximation for large-scale
Gaussian processes, the Gaussian Process Random Field (GPRF), in which local
GPs are coupled via pairwise potentials. The GPRF likelihood is a simple,
tractable, and parallelizeable approximation to the full GP marginal
likelihood, enabling latent variable modeling and hyperparameter selection on
large datasets. We demonstrate its effectiveness on synthetic spatial data as
well as a real-world application to seismic event location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00060</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00060</id><created>2015-10-30</created><updated>2016-01-06</updated><authors><author><keyname>Zhang</keyname><forenames>Xingxing</forenames></author><author><keyname>Lu</keyname><forenames>Liang</forenames></author><author><keyname>Lapata</keyname><forenames>Mirella</forenames></author></authors><title>Top-down Tree Long Short-Term Memory Networks</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long Short-Term Memory (LSTM) networks, a type of recurrent neural network
with a more complex computational unit, have been successfully applied to a
variety of sequence modeling tasks. In this paper we develop Tree Long
Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is
designed to predict a tree rather than a linear sequence. TreeLSTM defines the
probability of a sentence by estimating the generation probability of its
dependency tree. At each time step, a node is generated based on the
representation of the generated sub-tree. We further enhance the modeling power
of TreeLSTM by explicitly representing the correlations between left and right
dependents. Application of our model to the MSR sentence completion challenge
achieves results beyond the current state of the art. We also report results on
dependency parsing reranking achieving competitive performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00067</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00067</id><created>2015-10-30</created><authors><author><keyname>He</keyname><forenames>Wangpeng</forenames></author><author><keyname>Ding</keyname><forenames>Yin</forenames></author><author><keyname>Zi</keyname><forenames>Yanyang</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan W.</forenames></author></authors><title>Sparsity-based Algorithm for Detecting Faults in Rotating Machines</title><categories>cs.SD</categories><doi>10.1016/j.ymssp.2015.11.027</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses the detection of periodic transients in vibration
signals for detecting faults in rotating machines. For this purpose, we present
a method to estimate periodic-group-sparse signals in noise. The method is
based on the formulation of a convex optimization problem. A fast iterative
algorithm is given for its solution. A simulated signal is formulated to verify
the performance of the proposed approach for periodic feature extraction. The
detection performance of comparative methods is compared with that of the
proposed approach via RMSE values and receiver operating characteristic (ROC)
curves. Finally, the proposed approach is applied to compound faults diagnosis
of motor bearings. The non-stationary vibration data were acquired from a
SpectraQuest's machinery fault simulator. The processed results show the
proposed approach can effectively detect and extract the useful features of
bearing outer race and inner race defect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00075</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00075</id><created>2015-10-31</created><updated>2015-11-15</updated><authors><author><keyname>Chen</keyname><forenames>Yijia</forenames></author><author><keyname>Lin</keyname><forenames>Bingkai</forenames></author></authors><title>The Constant Inapproximability of the Parameterized Dominating Set
  Problem</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that there is no fpt-algorithm that can approximate the dominating
set problem with any constant ratio, unless FPT= W[1]. Our hardness reduction
is built on the second author's recent W[1]-hardness proof of the biclique
problem. This yields, among other things, a proof without the PCP machinery
that the classical dominating set problem has no polynomial time constant
approximation under the exponential time hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00083</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00083</id><created>2015-10-31</created><updated>2015-12-01</updated><authors><author><keyname>Hawkins</keyname><forenames>Jeff</forenames></author><author><keyname>Ahmad</keyname><forenames>Subutai</forenames></author></authors><title>Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in
  Neocortex</title><categories>q-bio.NC cs.AI</categories><comments>Submitted for publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neocortical neurons have thousands of excitatory synapses. It is a mystery
how neurons integrate the input from so many synapses and what kind of
large-scale network behavior this enables. It has been previously proposed that
non-linear properties of dendrites enable neurons to recognize multiple
patterns. In this paper we extend this idea by showing that a neuron with
several thousand synapses arranged along active dendrites can learn to
accurately and robustly recognize hundreds of unique patterns of cellular
activity, even in the presence of large amounts of noise and pattern variation.
We then propose a neuron model where some of the patterns recognized by a
neuron lead to action potentials and define the classic receptive field of the
neuron, whereas the majority of the patterns recognized by a neuron act as
predictions by slightly depolarizing the neuron without immediately generating
an action potential. We then present a network model based on neurons with
these properties and show that the network learns a robust model of time-based
sequences. Given the similarity of excitatory neurons throughout the neocortex
and the importance of sequence memory in inference and behavior, we propose
that this form of sequence memory is a universal property of neocortical
tissue. We further propose that cellular layers in the neocortex implement
variations of the same sequence memory algorithm to achieve different aspects
of inference and behavior. The neuron and network models we introduce are
robust over a wide range of parameters as long as the network uses a sparse
distributed code of cellular activations. The sequence capacity of the network
scales linearly with the number of synapses on each neuron. Thus neurons need
thousands of synapses to learn the many temporal patterns in sensory stimuli
and motor sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00096</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00096</id><created>2015-10-31</created><authors><author><keyname>Orchard</keyname><forenames>Garrick</forenames></author><author><keyname>Etienne-Cummings</keyname><forenames>Ralph</forenames></author></authors><title>Bioinspired Visual Motion Estimation</title><categories>cs.CV</categories><comments>16 pages, 11 figures, 1 table</comments><journal-ref>Proceedings of the IEEE, vol.102, no.10, pp.1520-1536, Oct. 2014</journal-ref><doi>10.1109/JPROC.2014.2346763</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual motion estimation is a computationally intensive, but important task
for sighted animals. Replicating the robustness and efficiency of biological
visual motion estimation in artificial systems would significantly enhance the
capabilities of future robotic agents. 25 years ago, in this very journal,
Carver Mead outlined his argument for replicating biological processing in
silicon circuits. His vision served as the foundation for the field of
neuromorphic engineering, which has experienced a rapid growth in interest over
recent years as the ideas and technologies mature. Replicating biological
visual sensing was one of the first tasks attempted in the neuromorphic field.
In this paper we focus specifically on the task of visual motion estimation. We
describe the task itself, present the progression of works from the early first
attempts through to the modern day state-of-the-art, and provide an outlook for
future directions in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00098</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00098</id><created>2015-10-31</created><authors><author><keyname>Castaldo</keyname><forenames>Francesco</forenames></author><author><keyname>Zamir</keyname><forenames>Amir</forenames></author><author><keyname>Angst</keyname><forenames>Roland</forenames></author><author><keyname>Palmieri</keyname><forenames>Francesco</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author></authors><title>Semantic Cross-View Matching</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matching cross-view images is challenging because the appearance and
viewpoints are significantly different. While low-level features based on
gradient orientations or filter responses can drastically vary with such
changes in viewpoint, semantic information of images however shows an invariant
characteristic in this respect. Consequently, semantically labeled regions can
be used for performing cross-view matching. In this paper, we therefore explore
this idea and propose an automatic method for detecting and representing the
semantic information of an RGB image with the goal of performing cross-view
matching with a (non-RGB) geographic information system (GIS). A segmented
image forms the input to our system with segments assigned to semantic concepts
such as traffic signs, lakes, roads, foliage, etc. We design a descriptor to
robustly capture both, the presence of semantic concepts and the spatial layout
of those segments. Pairwise distances between the descriptors extracted from
the GIS map and the query image are then used to generate a shortlist of the
most promising locations with similar semantic concepts in a consistent spatial
layout. An experimental evaluation with challenging query images and a large
urban area shows promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00099</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00099</id><created>2015-10-31</created><authors><author><keyname>Parui</keyname><forenames>Sarthak</forenames></author><author><keyname>Mittal</keyname><forenames>Anurag</forenames></author></authors><title>Sketch-based Image Retrieval from Millions of Images under Rotation,
  Translation and Scale Variations</title><categories>cs.CV cs.IR</categories><comments>submitted to IJCV, April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proliferation of touch-based devices has made sketch-based image retrieval
practical. While many methods exist for sketch-based object detection/image
retrieval on small datasets, relatively less work has been done on large
(web)-scale image retrieval. In this paper, we present an efficient approach
for image retrieval from millions of images based on user-drawn sketches.
Unlike existing methods for this problem which are sensitive to even
translation or scale variations, our method handles rotation, translation,
scale (i.e. a similarity transformation) and small deformations. The object
boundaries are represented as chains of connected segments and the database
images are pre-processed to obtain such chains that have a high chance of
containing the object. This is accomplished using two approaches in this work:
a) extracting long chains in contour segment networks and b) extracting
boundaries of segmented object proposals. These chains are then represented by
similarity-invariant variable length descriptors. Descriptor similarities are
computed by a fast Dynamic Programming-based partial matching algorithm. This
matching mechanism is used to generate a hierarchical k-medoids based indexing
structure for the extracted chains of all database images in an offline process
which is used to efficiently retrieve a small set of possible matched images
for query chains. Finally, a geometric verification step is employed to test
geometric consistency of multiple chain matches to improve results. Qualitative
and quantitative results clearly demonstrate superiority of the approach over
existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00100</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00100</id><created>2015-10-31</created><authors><author><keyname>Orchard</keyname><forenames>Garrick</forenames></author><author><keyname>Martin</keyname><forenames>Jacob G.</forenames></author><author><keyname>Vogelstein</keyname><forenames>R. Jacob</forenames></author><author><keyname>Etienne-Cummings</keyname><forenames>Ralph</forenames></author></authors><title>Fast Neuromimetic Object Recognition using FPGA Outperforms GPU
  Implementations</title><categories>cs.CV</categories><comments>14 pages, 8 figures, 5 tables</comments><journal-ref>Neural Networks and Learning Systems, IEEE Transactions on,
  vol.24, no.8, pp.1239-1252, 2013</journal-ref><doi>10.1109/TNNLS.2013.2253563</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition of objects in still images has traditionally been regarded as a
difficult computational problem. Although modern automated methods for visual
object recognition have achieved steadily increasing recognition accuracy, even
the most advanced computational vision approaches are unable to obtain
performance equal to that of humans. This has led to the creation of many
biologically-inspired models of visual object recognition, among them the HMAX
model. HMAX is traditionally known to achieve high accuracy in visual object
recognition tasks at the expense of significant computational complexity.
Increasing complexity, in turn, increases computation time, reducing the number
of images that can be processed per unit time. In this paper we describe how
the computationally intensive, biologically inspired HMAX model for visual
object recognition can be modified for implementation on a commercial Field
Programmable Gate Array, specifically the Xilinx Virtex 6 ML605 evaluation
board with XC6VLX240T FPGA. We show that with minor modifications to the
traditional HMAX model we can perform recognition on images of size 128x128
pixels at a rate of 190 images per second with a less than 1% loss in
recognition accuracy in both binary and multi-class visual object recognition
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00104</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00104</id><created>2015-10-31</created><authors><author><keyname>Wu</keyname><forenames>Daoyuan</forenames></author><author><keyname>Chang</keyname><forenames>Rocky K. C.</forenames></author></authors><title>Indirect File Leaks in Mobile Applications</title><categories>cs.CR</categories><comments>The paper was accepted by IEEE Mobile Security Technologies (MoST)
  2015 as a regular paper (see http://ieee-security.org/TC/SPW2015/MoST/). This
  is a Technical Report version for reference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, much of our sensitive information is stored inside mobile applications
(apps), such as the browsing histories and chatting logs. To safeguard these
privacy files, modern mobile systems, notably Android and iOS, use sandboxes to
isolate apps' file zones from one another. However, we show in this paper that
these private files can still be leaked by indirectly exploiting components
that are trusted by the victim apps. In particular, we devise new indirect file
leak (IFL) attacks that exploit browser interfaces, command interpreters, and
embedded app servers to leak data from very popular apps, such as Evernote and
QQ. Unlike the previous attacks, we demonstrate that these IFLs can affect both
Android and iOS. Moreover, our IFL methods allow an adversary to launch the
attacks remotely, without implanting malicious apps in victim's smartphones. We
finally compare the impacts of four different types of IFL attacks on Android
and iOS, and propose several mitigation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00111</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00111</id><created>2015-10-31</created><authors><author><keyname>Abdelsamea</keyname><forenames>M.</forenames></author></authors><title>Regional Active Contours based on Variational level sets and Machine
  Learning for Image Segmentation</title><categories>cs.CV</categories><comments>IMT PhD thesis, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image segmentation is the problem of partitioning an image into different
subsets, where each subset may have a different characterization in terms of
color, intensity, texture, and/or other features. Segmentation is a fundamental
component of image processing, and plays a significant role in computer vision,
object recognition, and object tracking. Active Contour Models (ACMs)
constitute a powerful energy-based minimization framework for image
segmentation, which relies on the concept of contour evolution. Starting from
an initial guess, the contour is evolved with the aim of approximating better
and better the actual object boundary. Handling complex images in an efficient,
effective, and robust way is a real challenge, especially in the presence of
intensity inhomogeneity, overlap between the foreground/background intensity
distributions, objects characterized by many different intensities, and/or
additive noise. In this thesis, to deal with these challenges, we propose a
number of image segmentation models relying on variational level set methods
and specific kinds of neural networks, to handle complex images in both
supervised and unsupervised ways. Experimental results demonstrate the high
accuracy of the segmentation results, obtained by the proposed models on
various benchmark synthetic and real images compared with state-of-the-art
active contour models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00112</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00112</id><created>2015-10-31</created><authors><author><keyname>Biernacki</keyname><forenames>Arkadiusz</forenames></author></authors><title>A Novel Play-out Algorithm for HTTP Adaptive Streaming</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, we proposed a novel algorithm dedicated to adaptive video
streaming based on HTTP. The algorithm employs a hybrid play-out strategy which
combines two popular approaches: an estimation of network bandwidth and a
control of a player buffer. The proposed algorithm was implemented in two
versions which differ in the method of handling fluctuations of network
throughput.
  The proposed hybrid algorithm was evaluated against solutions which base
their play-out strategy purely on bandwidth or buffer level assessment. The
comparison was performed in an environment which emulated two systems: a Wi-Fi
network with a single immobile node and HSPA (High Speed Packet Access) network
with a mobile node. The evaluation shows that the hybrid approach in most cases
achieves better results compared to its competitors, being able to stream the
video more smoothly without unnecessary bit-rate switches. However, in certain
network conditions, this score is traded for a worse throughput utilisation
compared to other play-out strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00117</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00117</id><created>2015-10-31</created><authors><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author></authors><title>Topological chaos and chaotic iterations. Application to Hash functions</title><categories>cs.CR</categories><comments>IJCNN 2010, Int. Joint Conf. on Neural Networks, joint to WCCI'10,
  IEEE World Congress on Computational Intelligence</comments><doi>10.1109/IJCNN.2010.5596512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new notion of chaotic algorithms. These algorithms
are iterative and are based on so-called chaotic iterations. Contrary to all
existing studies on chaotic iterations, we are not interested in stable states
of such iterations but in their possible unpredictable behaviors. By
establishing a link between chaotic iterations and the notion of Devaney's
topological chaos, we give conditions ensuring that these kind of algorithms
produce topological chaos. This leads to algorithms that are highly
unpredictable. After presenting the theoretical foundations of our approach, we
are interested in its practical aspects. We show how the theoretical algorithms
give rise to computer programs that produce true topological chaos, then we
propose applications in the area of information security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00118</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00118</id><created>2015-10-31</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author></authors><title>A new chaos-based watermarking algorithm</title><categories>cs.MM</categories><comments>SECRYPT 2010, International Conference on Security and Cryptograph.
  Submitted as a regular paper, accepted as a short one. arXiv admin note: text
  overlap with arXiv:0810.4713, arXiv:1012.4620</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new watermarking algorithm based on discrete chaotic
iterations. After defining some coefficients deduced from the description of
the carrier medium, chaotic discrete iterations are used to mix the watermark
and to embed it in the carrier medium. It can be proved that this procedure
generates topological chaos, which ensures that desired properties of a
watermarking algorithm are satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00120</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00120</id><created>2015-10-31</created><authors><author><keyname>Rayguru</keyname><forenames>M M</forenames></author><author><keyname>kar</keyname><forenames>I N</forenames></author></authors><title>A Contraction Theory Approach for Analysis of Performance Recovery in
  Dynamic Surface Control</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic surface control (DSC) method uses high gain filters to avoid the
&quot;explosion of complexity&quot; issue inherent in backstepping based controller
designs. As a result, the closed loop system and filter dynamics possess time
scale separation between them. This paper attempts to design a novel
disturbance observer based dynamic surface controller using contraction
framework. In doing so the steady state error bounds are obtained in terms of
design parameters which are exploited to tune the closed loop system
performance. The results not only show that DSC technique recover the
performance of a backstepping controller for a small range of filter parameter
but also derive the maximum bound for it. Furthermore the stability bounds are
also derived in the presence of disturbances and convergence of trajectories to
a small penultimate bound is proved. The convergence results are shown to hold
for less conservative choice of filter parameter and observer gain. The
effectiveness of the proposed controller is verified through simulation
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00125</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00125</id><created>2015-10-31</created><authors><author><keyname>Usatyuk</keyname><forenames>Vasily</forenames></author></authors><title>Computing the minimum distance of nonbinary LDPC codes using block
  Korkin-Zolotarev method</title><categories>cs.IT math.IT</categories><comments>14 pages, 2 tables, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In article present measure code distance algorithm of binary and ternary
linear block code using block Korkin-Zolotarev (BKZ). Proved the upper bound on
scaling constant for measure code distance of non-systematic linear block code
using BKZ-method for different value of the block size. Introduced method show
linear decrease of runtime from number of threads and work especially good
under not dense lattices of LDPC-code. These properties allow use this
algorithm to measure the minimal distance of code with length several thousand.
The algorithm can further improve by transform into probabilistic algorithm
using lattice enumerating pruning techniques
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00133</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00133</id><created>2015-10-31</created><updated>2015-11-24</updated><authors><author><keyname>Usatyuk</keyname><forenames>Vasiliy</forenames></author></authors><title>Some problems of Graph Based Codes for Belief Propagation decoding</title><categories>cs.IT math.IT</categories><comments>19 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Short survey about code on the graph by example of hardware friendly
quasi-cycle LDPC code. We consider two main properties of code: weight
enumerator (well known from classic code theory) and Trapping sets
pseudocodewords weight spectrum (a subgraph in code graph's which become
reasone of decoding failure under Belief propagation). In paper we show fast
methods to measure first components of TS enumerator using EMD=ACE constrains
for high girth codes on the graph using graph spectral classification method.
This approach simplify solving trouble of agreed between minimal TS pseudocode
weight and code distance (which can be measure using knowledge of Authomorphism
for algebraic code design methods or measure using lattice reduction(LLL,BKZ)
for random graph design methods). In the end of article author raise several
challenge problems to improve Sparse and Dense parity-check codes decoding
under Belief propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00146</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00146</id><created>2015-10-31</created><authors><author><keyname>Khan</keyname><forenames>Mohammad Emtiyaz</forenames></author><author><keyname>Babanezhad</keyname><forenames>Reza</forenames></author><author><keyname>Lin</keyname><forenames>Wu</forenames></author><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Convergence of Proximal-Gradient Stochastic Variational Inference under
  Non-Decreasing Step-Size Sequence</title><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic approximation methods have recently gained popularity for
variational inference, but many existing approaches treat them as &quot;black-box&quot;
tools. Thus, they often do not take advantage of the geometry of the posterior
and usually require a decreasing sequence of step-sizes (which converges slowly
in practice). We introduce a new stochastic-approximation method that uses a
proximal-gradient framework. Our method exploits the geometry and structure of
the variational lower bound, and contains many existing methods, such as
stochastic variational inference, as a special case. We establish the
convergence of our method under a &quot;non-decreasing&quot; step-size schedule, which
has both theoretical and practical advantages. We consider setting the
step-size based on the continuity of the objective and the geometry of the
posterior, and show that our method gives a faster rate of convergence for
variational-Gaussian inference than existing stochastic methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00148</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00148</id><created>2015-10-31</created><authors><author><keyname>Zliobaite</keyname><forenames>Indre</forenames></author></authors><title>A survey on measuring indirect discrimination in machine learning</title><categories>cs.CY stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, many decisions are made using predictive models built on historical
data.Predictive models may systematically discriminate groups of people even if
the computing process is fair and well-intentioned. Discrimination-aware data
mining studies how to make predictive models free from discrimination, when
historical data, on which they are built, may be biased, incomplete, or even
contain past discriminatory decisions. Discrimination refers to disadvantageous
treatment of a person based on belonging to a category rather than on
individual merit. In this survey we review and organize various discrimination
measures that have been used for measuring discrimination in data, as well as
in evaluating performance of discrimination-aware predictive models. We also
discuss related measures from other disciplines, which have not been used for
measuring discrimination, but potentially could be suitable for this purpose.
We computationally analyze properties of selected measures. We also review and
discuss measuring procedures, and present recommendations for practitioners.
The primary target audience is data mining, machine learning, pattern
recognition, statistical modeling researchers developing new methods for
non-discriminatory predictive modeling. In addition, practitioners and policy
makers would use the survey for diagnosing potential discrimination by
predictive models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00150</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00150</id><created>2015-10-31</created><updated>2016-03-02</updated><authors><author><keyname>Wang</keyname><forenames>Wenhao</forenames></author><author><keyname>Sun</keyname><forenames>Zhi</forenames></author><author><keyname>Ren</keyname><forenames>Kui</forenames></author><author><keyname>Zhu</keyname><forenames>Bocheng</forenames></author></authors><title>User Capacity of Wireless Physical-layer Identification: An
  Information-theoretic Perspective</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Physical Layer Identification (WPLI) system aims at identifying or
classifying authorized devices based on the unique Radio Frequency Fingerprints
(RFFs) extracted from their radio frequency signals at the physical layer.
Current works of WPLI focus on demonstrating system feasibility based on
experimental error performance of WPLI with a fixed number of users. While an
important question remains to be answered: what's the user number that WPLI can
accommodate using different RFFs and receiving equipment. The user capacity of
the WPLI can be a major concern for practical system designers and can also be
a key metric to evaluate the classification performance of WPLI. In this work,
we establish a theoretical understanding on user capacity of WPLI in an
information-theoretic perspective. We apply information-theoretic modeling on
RFF features of WPLI. An information-theoretic approach is consequently
proposed based on mutual information between RFF and user identity to
characterize the user capacity of WPLI. Based on this theoretical tool, the
achievable user capacity of WPLI is characterized under practical constrains of
off-the-shelf receiving devices. Field experiments on classification error
performance are conducted for the validation of the information-theoretic user
capacity characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00151</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00151</id><created>2015-10-31</created><authors><author><keyname>Luks</keyname><forenames>Eugene M.</forenames></author></authors><title>Group Isomorphism with Fixed Subnormal Chains</title><categories>cs.CC math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent work, Rosenbaum and Wagner showed that isomorphism of explicitly
listed $p$-groups of order $n$ could be tested in $n^{\frac{1}{2}\log_p n +
O(p)}$ time, roughly a square root of the classical bound. The $O(p)$ term is
entirely due to an $n^{O(p)}$ cost of testing for isomorphisms that match fixed
composition series in the two groups. We focus here on the
fixed-composition-series subproblem and exhibit a polynomial-time algorithm
that is valid for general groups. A subsequent paper will construct canonical
forms within the same time bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00152</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00152</id><created>2015-10-31</created><updated>2015-11-30</updated><authors><author><keyname>Pourkamali-Anaraki</keyname><forenames>Farhad</forenames></author><author><keyname>Becker</keyname><forenames>Stephen</forenames></author></authors><title>Preconditioned Data Sparsification for Big Data with Applications to PCA
  and K-means</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a compression scheme for large data sets that randomly keeps a
small percentage of the components of each data sample. The benefit is that the
output is a sparse matrix and therefore subsequent processing, such as PCA or
K-means, is significantly faster, especially in a distributed-data setting.
Furthermore, the sampling is single-pass and applicable to streaming data. The
sampling mechanism is a variant of previous methods proposed in the literature
combined with a randomized preconditioning to smooth the data. We provide
guarantees for PCA in terms of the covariance matrix, and guarantees for
K-means in terms of the error in the center estimators at a given step. We
present numerical evidence to show both that our bounds are nearly tight and
that our algorithms provide a real benefit when applied to standard test data
sets, as well as providing certain benefits over related sampling approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00157</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00157</id><created>2015-10-31</created><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Davies</keyname><forenames>Sylvie</forenames></author><author><keyname>Liu</keyname><forenames>Bo Yang Victor</forenames></author></authors><title>Most Complex Regular Ideals</title><categories>cs.FL</categories><comments>23 pages, 11 figures. arXiv admin note: text overlap with
  arXiv:1311.4448</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A right ideal (left ideal, two-sided ideal) is a non-empty language $L$ over
an alphabet $\Sigma$ that satisfies $L=L\Sigma^*$ ($L=\Sigma^*L$,
$L=\Sigma^*L\Sigma^*$). Let $k=3$ for right ideals, 4 for left ideals and 5 for
two-sided ideals. We show that there exist sequences ($L_n \mid n \ge k $) of
right, left, and two-sided regular ideals, where $L_n$ has quotient complexity
(state complexity) $n$, such that $L_n$ is most complex in its class under the
following measures of complexity: the size of the syntactic semigroup, the
quotient complexities of the left quotients of $L_n$, the number of atoms
(intersections of complemented and uncomplemented left quotients), the quotient
complexities of the atoms, and the quotient complexities of reversal, star,
product (concatenation), and all binary boolean operations. In that sense,
these ideals are &quot;most complex&quot; languages in their classes, or &quot;universal
witnesses&quot; to the complexity of the various operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00158</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00158</id><created>2015-10-31</created><authors><author><keyname>Navarrete</keyname><forenames>Raymundo</forenames></author><author><keyname>Viswanath</keyname><forenames>Divakar</forenames></author></authors><title>Support Vector Regression, Smooth Splines, and Time Series Prediction</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay coordinates and support vector regression are among the techniques
commonly used for time series prediction. We show that the combination of these
two techniques leads to systematic error that obstructs convergence. A
preliminary step of spline smoothing restores convergence and leads to
predictions that are consistently more accurate, typically by about a factor of
$2$ or so. Since the algorithm without spline smoothing is not convergent, the
improvement in accuracy can even be as high as a factor of $100$. Assuming
local isotropy, the systematic error in the absence of spline smoothing is
estimated to be $d\sigma^{2}L/2$, where $d$ is the embedding dimension,
$\sigma^{2}$ is the variance of Gaussian noise in the signal, and $L$ is a
global bound on the Hessian of the exact predictor. The smooth spline, although
very effective, is shown not to have even first order accuracy, unless the
noise is unusually mild. The lack of order of accuracy implies that attempts to
take advantage of invariance in time to enhance fidelity of learning are
unlikely to be successful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00175</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00175</id><created>2015-10-31</created><updated>2016-01-08</updated><authors><author><keyname>Iandola</keyname><forenames>Forrest N.</forenames></author><author><keyname>Ashraf</keyname><forenames>Khalid</forenames></author><author><keyname>Moskewicz</keyname><forenames>Matthew W.</forenames></author><author><keyname>Keutzer</keyname><forenames>Kurt</forenames></author></authors><title>FireCaffe: near-linear acceleration of deep neural network training on
  compute clusters</title><categories>cs.CV</categories><comments>Version 2: Added results on 128 GPUs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Long training times for high-accuracy deep neural networks (DNNs) impede
research into new DNN architectures and slow the development of high-accuracy
DNNs. In this paper we present FireCaffe, which successfully scales deep neural
network training across a cluster of GPUs. We also present a number of best
practices to aid in comparing advancements in methods for scaling and
accelerating the training of deep neural networks. The speed and scalability of
distributed algorithms is almost always limited by the overhead of
communicating between servers; DNN training is not an exception to this rule.
Therefore, the key consideration here is to reduce communication overhead
wherever possible, while not degrading the accuracy of the DNN models that we
train. Our approach has three key pillars. First, we select network hardware
that achieves high bandwidth between GPU servers -- Infiniband or Cray
interconnects are ideal for this. Second, we consider a number of communication
algorithms, and we find that reduction trees are more efficient and scalable
than the traditional parameter server approach. Third, we optionally increase
the batch size to reduce the total quantity of communication during DNN
training, and we identify hyperparameters that allow us to reproduce the
small-batch accuracy while training with large batch sizes. When training
GoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup,
respectively, when training on a cluster of 128 GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00180</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00180</id><created>2015-10-31</created><updated>2015-11-03</updated><authors><author><keyname>Barkatou</keyname><forenames>Moulay A.</forenames></author><author><keyname>Jaroschek</keyname><forenames>Maximilian</forenames></author><author><keyname>Maddah</keyname><forenames>Suzy S.</forenames></author></authors><title>Formal Solutions of Completely Integrable Pfaffian Systems With Normal
  Crossings</title><categories>cs.SC</categories><comments>Corrected typos and removed unused references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an algorithm for computing a fundamental matrix of
formal solutions of completely integrable Pfaffian systems with normal
crossings in several variables. This algorithm is a generalization of a method
developed for the bivariate case based on a combination of several reduction
techniques and is implemented in the computer algebra system Maple.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00184</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00184</id><created>2015-10-31</created><updated>2015-11-11</updated><authors><author><keyname>Haziza</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Hol&#xed;k</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Meyer</keyname><forenames>Roland</forenames></author><author><keyname>Wolff</keyname><forenames>Sebastian</forenames></author></authors><title>Pointer Race Freedom</title><categories>cs.PL</categories><report-no>FIT-TR-2015-05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel notion of pointer race for concurrent programs
manipulating a shared heap. A pointer race is an access to a memory address
which was freed, and it is out of the accessor's control whether or not the
cell has been re-allocated. We establish two results. (1) Under the assumption
of pointer race freedom, it is sound to verify a program running under explicit
memory management as if it was running with garbage collection. (2) Even the
requirement of pointer race freedom itself can be verified under the
garbage-collected semantics. We then prove analogues of the theorems for a
stronger notion of pointer race needed to cope with performance-critical code
purposely using racy comparisons and even racy dereferences of pointers. As a
practical contribution, we apply our results to optimize a thread-modular
analysis under explicit memory management. Our experiments confirm a speed-up
of up to two orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00188</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00188</id><created>2015-10-31</created><authors><author><keyname>Gupta</keyname><forenames>Anshul</forenames></author><author><keyname>Deepak</keyname><forenames>M. S. Krishna</forenames></author><author><keyname>Padarthi</keyname><forenames>Bharath Kumar</forenames></author><author><keyname>Schewe</keyname><forenames>Sven</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author></authors><title>Incentive Stackelberg Mean-payoff Games</title><categories>cs.GT</categories><comments>15 pages, references, appendix, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study incentive equilibria for multi-player meanpayoff
games. Incentive equilibria generalise well-studied solution concepts such as
Nash equilibria and leader equilibria (also known as Stackelberg equilibria).
Recall that a strategy profile is a Nash equilibrium if no player can improve
his payoff by changing his strategy unilaterally. In the setting of incentive
and leader equilibria, there is a distinguished player called the leader who
can assign strategies to all other players, referred to as her followers. A
strategy profile is a leader strategy profile if no player, except for the
leader, can improve his payoff by changing his strategy unilaterally, and a
leader equilibrium is a leader strategy profile with a maximal return for the
leader. In the proposed case of incentive equilibria, the leader can
additionally influence the behaviour of her followers by transferring parts of
her payoff to her followers. The ability to incentivise her followers provides
the leader with more freedom in selecting strategy profiles, and we show that
this can indeed improve the payoff for the leader in such games. The key
fundamental result of the paper is the existence of incentive equilibria in
mean-payoff games. We further show that the decision problem related to
constructing incentive equilibria is NP-complete. On a positive note, we show
that, when the number of players is fixed, the complexity of the problem falls
in the same class as two-player mean-payoff games. We also present an
implementation of the proposed algorithms, and discuss experimental results
that demonstrate the feasibility of the analysis of medium sized games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00195</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00195</id><created>2015-10-31</created><authors><author><keyname>Lavin</keyname><forenames>Alexander</forenames></author></authors><title>Optimized Mission Planning for Planetary Exploration Rovers</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exploration of planetary surfaces is predominately unmanned, calling for
a landing vehicle and an autonomous and/or teleoperated rover. Artificial
intelligence and machine learning techniques can be leveraged for better
mission planning. This paper describes the coordinated use of both global
navigation and metaheuristic optimization algorithms to plan the safe,
efficient missions. The aim is to determine the least-cost combination of a
safe landing zone (LZ) and global path plan, where avoiding terrain hazards for
the lander and rover minimizes cost. Computer vision methods were used to
identify surface craters, mounds, and rocks as obstacles. Multiple search
methods were investigated for the rover global path plan. Several combinatorial
optimization algorithms were implemented to select the shortest distance path
as the preferred mission plan. Simulations were run for a sample Google Lunar X
Prize mission. The result of this study is an optimization scheme that path
plans with the A* search method, and uses simulated annealing to select ideal
LZ-path- goal combination for the mission. Simulation results show the methods
are effective in minimizing the risk of hazards and increasing efficiency. This
paper is specific to a lunar mission, but the resulting architecture may be
applied to a large variety of planetary missions and rovers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00205</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00205</id><created>2015-10-31</created><updated>2015-11-03</updated><authors><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>Eigenvalue Clustering, Control Energy, and Logarithmic Capacity</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove two bounds showing that if the eigenvalues of a matrix are clustered
in a region of the complex plane then the corresponding discrete-time linear
system requires significant energy to control. An interesting feature of one of
our bounds is that the dependence on the region is via its logarithmic
capacity, which is a measure of how well a unit of mass may be spread out over
the region to minimize a logarithmic potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00207</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00207</id><created>2015-10-31</created><updated>2016-02-28</updated><authors><author><keyname>AlAmmouri</keyname><forenames>Ahmad</forenames></author><author><keyname>ElSawy</keyname><forenames>Hesham</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Harvesting Full-Duplex Rate Gains in Cellular Networks with Half-Duplex
  User Terminals</title><categories>cs.IT math.IT</categories><comments>IEEE ICC2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-Duplex (FD) transceivers may be expensive in terms of complexity, power
consumption, and price to be implemented in all user terminals. Therefore,
techniques to exploit in-band full-duplex communication with FD base stations
(BSs) and half-duplex (HD) users' equipment (UEs) are required. In this
context, 3-node topology (3NT) has been recently proposed for FD BSs to reuse
the uplink (UL) and downlink (DL) channels with HD terminals within the same
cell. In this paper, we present a tractable mathematical framework, based on
stochastic geometry, for 3NT in cellular networks. To this end, we propose a
design paradigm via pulse-shaping and partial overlap between UL and DL
channels to maximize the harvested rate gains in 3NT. The results show that 3NT
achieves a close performance to networks with FD BSs and FD UEs, denoted by
2-node topology (2NT) networks. A maximum of 5$\%$ rate loss is reported when
3NT is compared to 2NT with efficient self-interference cancellation (SIC). If
the SIC in 2NT is not efficient, 3NT highly outperforms 2NT. Consequently, we
conclude that, irrespective to the UE duplexing scheme, it is sufficient to
have FD BSs to harvest FD rate gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00212</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00212</id><created>2015-11-01</created><authors><author><keyname>Coti</keyname><forenames>Camille</forenames></author></authors><title>Exploiting Redundant Computation in Communication-Avoiding Algorithms
  for Algorithm-Based Fault Tolerance</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication-avoiding algorithms allow redundant computations to minimize
the number of inter-process communications. In this paper, we propose to
exploit this redundancy for fault-tolerance purpose. We illustrate this idea
with QR factorization of tall and skinny matrices, and we evaluate the number
of failures our algorithm can tolerate under different semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00213</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00213</id><created>2015-11-01</created><updated>2015-11-13</updated><authors><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author><author><keyname>Petej</keyname><forenames>Ivan</forenames></author><author><keyname>Fedorova</keyname><forenames>Valentina</forenames></author></authors><title>Large-scale probabilistic predictors with and without guarantees of
  validity</title><categories>cs.LG</categories><comments>38 pages, 14 figures, to appear in Advances in Neural Information
  Processing Systems 28 (NIPS 2015). As compared with the previous version
  (v1), the MATLAB code (the 5 files with extension .m) and results of new
  empirical studies have been added</comments><report-no>13</report-no><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies theoretically and empirically a method of turning
machine-learning algorithms into probabilistic predictors that automatically
enjoys a property of validity (perfect calibration) and is computationally
efficient. The price to pay for perfect calibration is that these probabilistic
predictors produce imprecise (in practice, almost precise for large data sets)
probabilities. When these imprecise probabilities are merged into precise
probabilities, the resulting predictors, while losing the theoretical property
of perfect calibration, are consistently more accurate than the existing
methods in empirical studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00215</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00215</id><created>2015-11-01</created><authors><author><keyname>Wang</keyname><forenames>Peilu</forenames></author><author><keyname>Qian</keyname><forenames>Yao</forenames></author><author><keyname>Soong</keyname><forenames>Frank K.</forenames></author><author><keyname>He</keyname><forenames>Lei</forenames></author><author><keyname>Zhao</keyname><forenames>Hai</forenames></author></authors><title>A Unified Tagging Solution: Bidirectional LSTM Recurrent Neural Network
  with Word Embedding</title><categories>cs.CL</categories><comments>Rejected by EMNLP 2015, score: 4,3,3 (full is 5)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has
been shown to be very effective for modeling and predicting sequential data,
e.g. speech utterances or handwritten documents. In this study, we propose to
use BLSTM-RNN for a unified tagging solution that can be applied to various
tagging tasks including part-of-speech tagging, chunking and named entity
recognition. Instead of exploiting specific features carefully optimized for
each task, our solution only uses one set of task-independent features and
internal representations learnt from unlabeled text for all tasks.Requiring no
task specific knowledge or sophisticated feature engineering, our approach gets
nearly state-of-the-art performance in all these three tagging tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00217</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00217</id><created>2015-11-01</created><authors><author><keyname>Biedermann</keyname><forenames>Daniel H.</forenames></author><author><keyname>Dietrich</keyname><forenames>Felix</forenames></author><author><keyname>Handel</keyname><forenames>Oliver</forenames></author><author><keyname>Kielar</keyname><forenames>Peter M.</forenames></author><author><keyname>Seitz</keyname><forenames>Michael</forenames></author></authors><title>Using Raspberry Pi for scientific video observation of pedestrians
  during a music festival</title><categories>cs.OH</categories><comments>17 pages</comments><msc-class>90B20</msc-class><doi>10.13140/RG.2.1.4035.4407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The document serves as a reference for researchers trying to capture a large
portion of a mass event on video for several hours, while using a very limited
budget.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00221</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00221</id><created>2015-11-01</created><authors><author><keyname>Loshchilov</keyname><forenames>Ilya</forenames></author></authors><title>LM-CMA: an Alternative to L-BFGS for Large Scale Black-box Optimization</title><categories>cs.NE math.OC</categories><comments>to appear in Evolutionary Computation Journal, MIT Press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The limited memory BFGS method (L-BFGS) of Liu and Nocedal (1989) is often
considered to be the method of choice for continuous optimization when first-
and/or second- order information is available. However, the use of L-BFGS can
be complicated in a black-box scenario where gradient information is not
available and therefore should be numerically estimated. The accuracy of this
estimation, obtained by finite difference methods, is often problem-dependent
that may lead to premature convergence of the algorithm.
  In this paper, we demonstrate an alternative to L-BFGS, the limited memory
Covariance Matrix Adaptation Evolution Strategy (LM-CMA) proposed by Loshchilov
(2014). The LM-CMA is a stochastic derivative-free algorithm for numerical
optimization of non-linear, non-convex optimization problems. Inspired by the
L-BFGS, the LM-CMA samples candidate solutions according to a covariance matrix
reproduced from $m$ direction vectors selected during the optimization process.
The decomposition of the covariance matrix into Cholesky factors allows to
reduce the memory complexity to $O(mn)$, where $n$ is the number of decision
variables. The time complexity of sampling one candidate solution is also
$O(mn)$, but scales as only about 25 scalar-vector multiplications in practice.
The algorithm has an important property of invariance w.r.t. strictly
increasing transformations of the objective function, such transformations do
not compromise its ability to approach the optimum. The LM-CMA outperforms the
original CMA-ES and its large scale versions on non-separable ill-conditioned
problems with a factor increasing with problem dimension. Invariance properties
of the algorithm do not prevent it from demonstrating a comparable performance
to L-BFGS on non-trivial large scale smooth and nonsmooth optimization
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00240</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00240</id><created>2015-11-01</created><authors><author><keyname>Thunberg</keyname><forenames>Johan</forenames></author><author><keyname>Hu</keyname><forenames>Xiaoming</forenames></author><author><keyname>Goncalves</keyname><forenames>Jorge</forenames></author></authors><title>Consensus and Formation Control on SE(3) for Switching Topologies</title><categories>math.OC cs.MA cs.SY</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the consensus problem and the formation problem on SE(3)
in multi-agent systems with directed and switching interconnection topologies.
Several control laws are introduced for the consensus problem. By a simple
transformation, it is shown that the proposed control laws can be used for the
formation problem. The design is first conducted on the kinematic level, where
the velocities are the control laws. Then, for rigid bodies in space, the
design is conducted on the dynamic level, where the torques and the forces are
the control laws. On the kinematic level, first two control laws are introduced
that explicitly use Euclidean transformations, then separate control laws are
defined for the rotations and the translations. In the special case of purely
rotational motion, the consensus problem is referred to as consensus on SO(3)
or attitude synchronization. In this problem, for a broad class of local
representations or parameterizations of SO(3), including the Axis-Angle
Representation, the Rodrigues Parameters and the Modified Rodrigues Parameters,
two types of control laws are presented that look structurally the same for any
choice of local representation. For these two control laws we provide
conditions on the initial rotations and the connectivity of the graph such that
the system reaches consensus on SO(3). Among the contributions of this paper,
there are conditions for when exponential rate of convergence occur. A theorem
is provided showing that for any choice of local representation for the
rotations, there is a change of coordinates such that the transformed system
has a well known structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00243</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00243</id><created>2015-11-01</created><authors><author><keyname>Yamada</keyname><forenames>Takeshi</forenames></author><author><keyname>Uehara</keyname><forenames>Ryuhei</forenames></author></authors><title>Shortest Reconfiguration of Sliding Tokens on a Caterpillar</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that we are given two independent sets I_b and I_r of a graph such
that |I_b|=|I_r|, and imagine that a token is placed on each vertex in |I_b|.
Then, the sliding token problem is to determine whether there exists a sequence
of independent sets which transforms I_b into I_r so that each independent set
in the sequence results from the previous one by sliding exactly one token
along an edge in the graph. The sliding token problem is one of the
reconfiguration problems that attract the attention from the viewpoint of
theoretical computer science. The reconfiguration problems tend to be
PSPACE-complete in general, and some polynomial time algorithms are shown in
restricted cases. Recently, the problems that aim at finding a shortest
reconfiguration sequence are investigated. For the 3SAT problem, a trichotomy
for the complexity of finding the shortest sequence has been shown, that is, it
is in P, NP-complete, or PSPACE-complete in certain conditions. In general,
even if it is polynomial time solvable to decide whether two instances are
reconfigured with each other, it can be NP-complete to find a shortest sequence
between them. Namely, finding a shortest sequence between two independent sets
can be more difficult than the decision problem of reconfigurability between
them. In this paper, we show that the problem for finding a shortest sequence
between two independent sets is polynomial time solvable for some graph classes
which are subclasses of the class of interval graphs. More precisely, we can
find a shortest sequence between two independent sets on a graph G in
polynomial time if either G is a proper interval graph, a trivially perfect
graph, or a caterpillar. As far as the authors know, this is the first
polynomial time algorithm for the shortest sliding token problem for a graph
class that requires detours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00270</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00270</id><created>2015-11-01</created><authors><author><keyname>Markstr&#xf6;m</keyname><forenames>Klas</forenames></author></authors><title>Problem collection from the IML programme: Graphs, Hypergraphs, and
  Computing</title><categories>math.CO cs.DM</categories><comments>This problem collection is published as part of the IML preprint
  series for the research programme and also available there
  http://www.mittag-leffler.se/research-programs/preprint-series?course_id=4401.
  arXiv admin note: text overlap with arXiv:1403.5975, arXiv:0706.4101 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This collection of problems and conjectures is based on a subset of the open
problems from the seminar series and the problem sessions of the Institut
Mitag-Leffler programme Graphs, Hypergraphs, and Computing. Each problem
contributor has provided a write up of their proposed problem and the
collection has been edited by Klas Markstr\&quot;om.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00271</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00271</id><created>2015-11-01</created><authors><author><keyname>Luo</keyname><forenames>Tianyi</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Liu</keyname><forenames>Rong</forenames></author><author><keyname>Pan</keyname><forenames>Yiqiao</forenames></author></authors><title>Stochastic Top-k ListNet</title><categories>cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ListNet is a well-known listwise learning to rank model and has gained much
attention in recent years. A particular problem of ListNet, however, is the
high computation complexity in model training, mainly due to the large number
of object permutations involved in computing the gradients. This paper proposes
a stochastic ListNet approach which computes the gradient within a bounded
permutation subset. It significantly reduces the computation complexity of
model training and allows extension to Top-k models, which is impossible with
the conventional implementation based on full-set permutations. Meanwhile, the
new approach utilizes partial ranking information of human labels, which helps
improve model quality. Our experiments demonstrated that the stochastic ListNet
method indeed leads to better ranking performance and speeds up the model
training remarkably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00289</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00289</id><created>2015-11-01</created><authors><author><keyname>Kopczynski</keyname><forenames>Eryk</forenames></author></authors><title>Invisible pushdown languages</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context free languages allow one to express data with hierarchical structure,
at the cost of losing some of the useful properties of languages recognized by
finite automata on words. However, it is possible to restore some of these
properties by making the structure of the tree visible, such as is done by
visibly pushdown languages, or finite automata on trees. In this paper, we show
that the structure given by such approaches remains invisible when it is read
by a finite automaton (on word). In particular, we show that separability with
a regular language is undecidable for visibly pushdown languages, just as it is
undecidable for general context free languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00296</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00296</id><created>2015-11-01</created><updated>2016-02-16</updated><authors><author><keyname>Smerlak</keyname><forenames>Matteo</forenames></author><author><keyname>Youssef</keyname><forenames>Ahmed</forenames></author></authors><title>Limiting fitness distributions in evolutionary dynamics</title><categories>q-bio.PE cond-mat.stat-mech cs.NE</categories><comments>15 pages + appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Darwinian evolution can be modeled in general terms as a flow in the space of
fitness (i.e. reproductive rate) distributions. In the diffusion approximation,
Tsimring et al. have showed that this flow admits &quot;fitness wave&quot; solutions:
Gaussian-shape fitness distributions moving towards higher fitness values at
constant speed. Here we show more generally that evolving fitness distributions
are attracted to a one-parameter family of distributions with a fixed parabolic
relationship between skewness and kurtosis. Unlike fitness waves, this
statistical pattern encompasses both positive and negative (a.k.a. purifying)
selection and is not restricted to rapidly adapting populations. Moreover we
find that the mean fitness of a population under the selection of pre-existing
variation is a power-law function of time, as observed in microbiological
evolution experiments but at variance with fitness wave theory. At the
conceptual level, our results can be viewed as the resolution of the &quot;dynamic
insufficiency&quot; of Fisher's fundamental theorem of natural selection. Our
predictions are in good agreement with numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00310</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00310</id><created>2015-11-01</created><authors><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author></authors><title>Parameterized Integer Quadratic Programming: Variables and Coefficients</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Integer Quadratic Programming problem input is an n*n integer matrix
Q, an m*n integer matrix A and an m-dimensional integer vector b. The task is
to find a vector x in Z^n, nminimizing x^TQx, subject to Ax &lt;= b. We give a
fixed parameter tractable algorithm for Integer Quadratic Programming
parameterized by n+a, assuming that an optimal solution to the input instance
exists. Here a is the largest absolute value of an entry of Q and A. As an
application of our main result we show that Optimal Linear Arrangement is fixed
parameter tractable parameterized by the size of the smallest vertex cover of
the input graph. This resolves an open problem from the recent monograph by
Downey and Fellows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00319</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00319</id><created>2015-11-01</created><updated>2015-11-15</updated><authors><author><keyname>Jahnz</keyname><forenames>Christoph</forenames></author></authors><title>An introduction to the NMPC-Graph as general schema for causal modelling
  of nonlinear, multivariate, dynamic, and recursive systems with focus on
  time-series prediction</title><categories>cs.SY</categories><comments>29 pages, containing glossary</comments><msc-class>93-02</msc-class><acm-class>G.1.2; I.2.6; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the disciplines of physics and engineering sciences in many cases have
taken advantage from accurate time-series prediction of system behaviour by
applying ordinary differential equation systems upon precise basic physical
laws such approach hardly could be adopted by other scientific disciplines
where precise mathematical basic laws are unknown. A new modelling schema, the
NMPC-graph, opens the possibility of interdisciplinary and generic nonlinear,
multivariate, dynamic, and recursive causal modelling in domains where basic
laws are only known as qualitative relationships among parameters while their
precise mathematical nature remains undisclosed at modelling time. The
symbolism of NMPC-graph is kept simple and suited for analysts without advanced
mathematical skills. This article presents the definition of the NMPC-graph
modelling method and its six component types. Further, it shows how to solve
the inverse problem of deriving a nonlinear ordinary differential equation
system from any NMPC-graph in conjunction with historic calibration data by
means of machine learning. This article further discusses how such a derived
NMPC-model can be used for hypothesis testing and time-series prediction with
the expectation of gaining prediction accuracy in comparison to conventional
prediction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00321</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00321</id><created>2015-11-01</created><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author></authors><title>A Construction of Binary Linear Codes from Boolean Functions</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1503.06511; text overlap
  with arXiv:1505.07726 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean functions have important applications in cryptography and coding
theory. Two famous classes of binary codes derived from Boolean functions are
the Reed-Muller codes and Kerdock codes. In the past two decades, a lot of
progress on the study of applications of Boolean functions in coding theory has
been made. Two generic constructions of binary linear codes with Boolean
functions have been well investigated in the literature. The objective of this
paper is twofold. The first is to provide a survey on recent results, and the
other is to propose open problems on one of the two generic constructions of
binary linear codes with Boolean functions. These open problems are expected to
stimulate further research on binary linear codes from Boolean functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00322</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00322</id><created>2015-11-01</created><updated>2015-11-10</updated><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author><author><keyname>Yuan</keyname><forenames>Pingzhi</forenames></author></authors><title>Five Constructions of Permutation Polynomials over $\gf(q^2)$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Four recursive constructions of permutation polynomials over $\gf(q^2)$ with
those over $\gf(q)$ are developed and applied to a few famous classes of
permutation polynomials. They produce infinitely many new permutation
polynomials over $\gf(q^{2^\ell})$ for any positive integer $\ell$ with any
given permutation polynomial over $\gf(q)$. A generic construction of
permutation polynomials over $\gf(2^{2m})$ with o-polynomials over $\gf(2^m)$
is also presented, and a number of new classes of permutation polynomials over
$\gf(2^{2m})$ are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00329</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00329</id><created>2015-11-01</created><authors><author><keyname>Rizzo</keyname><forenames>Nicholas</forenames></author><author><keyname>Sprissler</keyname><forenames>Ethan</forenames></author><author><keyname>Hong</keyname><forenames>Yuan</forenames></author><author><keyname>Goel</keyname><forenames>Sanjay</forenames></author></authors><title>Privacy Preserving Driving Style Recognition</title><categories>cs.CR</categories><comments>International Conference on Connected Vehicles and Expo 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to better manage the premiums and encourage safe driving, many
commercial insurance companies (e.g., Geico, Progressive) are providing options
for their customers to install sensors on their vehicles which collect
individual vehicle's traveling data. The driver's insurance is linked to
his/her driving behavior. At the other end, through analyzing the historical
traveling data from a large number of vehicles, the insurance company could
build a classifier to predict a new driver's driving style: aggressive or
defensive. However, collection of such vehicle traveling data explicitly
breaches the drivers' personal privacy. To tackle such privacy concerns, this
paper presents a privacy-preserving driving style recognition technique to
securely predict aggressive and defensive drivers for the insurance company
without compromising the privacy of all the participating parties. The
insurance company cannot learn any private information from the vehicles, and
vice-versa. Finally, the effectiveness and efficiency of the privacy-preserving
driving style recognition technique are validated with experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00341</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00341</id><created>2015-11-01</created><updated>2016-01-24</updated><authors><author><keyname>Holz</keyname><forenames>Ralph</forenames></author><author><keyname>Amann</keyname><forenames>Johanna</forenames></author><author><keyname>Mehani</keyname><forenames>Olivier</forenames></author><author><keyname>Wachs</keyname><forenames>Matthias</forenames></author><author><keyname>Kaafar</keyname><forenames>Mohamed Ali</forenames></author></authors><title>TLS in the wild: an Internet-wide analysis of TLS-based protocols for
  electronic communication</title><categories>cs.CR</categories><comments>NDSS 2016</comments><doi>10.14722/ndss.2016.23055</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of electronic communication today happens either via email or
chat. Thanks to the use of standardised protocols electronic mail (SMTP, IMAP,
POP3) and instant chat (XMPP, IRC) servers can be deployed in a decentralised
but interoperable fashion. These protocols can be secured by providing
encryption with the use of TLS---directly or via the STARTTLS extension---and
leverage X.509 PKIs or ad hoc methods to authenticate communication peers.
However, many combination of these mechanisms lead to insecure deployments.
  We present the largest study to date that investigates the security of the
email and chat infrastructures. We used active Internet-wide scans to determine
the amount of secure service deployments, and passive monitoring to investigate
if user agents actually use this opportunity to secure their communications. We
addressed both the client-to-server interactions as well as server-to-server
forwarding mechanisms that these protocols offer, and the use of encryption and
authentication methods in the process.
  Our findings shed light on an insofar unexplored area of the Internet. The
truly frightening result is that most of our communication is poorly secured in
transit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00346</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00346</id><created>2015-11-01</created><updated>2016-01-09</updated><authors><author><keyname>Hasuo</keyname><forenames>Ichiro</forenames></author><author><keyname>Shimizu</keyname><forenames>Shunsuke</forenames></author><author><keyname>Cirstea</keyname><forenames>Corina</forenames></author></authors><title>Lattice-Theoretic Progress Measures and Coalgebraic Model Checking (with
  Appendices)</title><categories>cs.LO cs.PL</categories><comments>24 pages, Extended version with appendices of a paper accepted to
  POPL 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of formal verification in general and model checking in
particular, parity games serve as a mighty vehicle: many problems are encoded
as parity games, which are then solved by the seminal algorithm by Jurdzinski.
In this paper we identify the essence of this workflow to be the notion of
progress measure, and formalize it in general, possibly infinitary,
lattice-theoretic terms. Our view on progress measures is that they are to
nested/alternating fixed points what invariants are to safety/greatest fixed
points, and what ranking functions are to liveness/least fixed points. That is,
progress measures are combination of the latter two notions (invariant and
ranking function) that have been extensively studied in the context of
(program) verification.
  We then apply our theory of progress measures to a general model-checking
framework, where systems are categorically presented as coalgebras. The
framework's theoretical robustness is witnessed by a smooth transfer from the
branching-time setting to the linear-time one. Although the framework can be
used to derive some decision procedures for finite settings, we also expect the
proposed framework to form a basis for sound proof methods for some
undecidable/infinitary problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00347</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00347</id><created>2015-11-01</created><authors><author><keyname>Sadraddini</keyname><forenames>Sadra</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Robust Temporal Logic Model Predictive Control</title><categories>cs.SY</categories><comments>This work has been accepted to appear in the proceedings of 53rd
  Annual Allerton Conference on Communication, Control and Computing,
  Urbana-Champaign, IL (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Control synthesis from temporal logic specifications has gained popularity in
recent years. In this paper, we use a model predictive approach to control
discrete time linear systems with additive bounded disturbances subject to
constraints given as formulas of signal temporal logic (STL). We introduce a
(conservative) computationally efficient framework to synthesize control
strategies based on mixed integer programs. The designed controllers satisfy
the temporal logic requirements, are robust to all possible realizations of the
disturbances, and optimal with respect to a cost function. In case the temporal
logic constraint is infeasible, the controller satisfies a relaxed, minimally
violating constraint. An illustrative case study is included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00352</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00352</id><created>2015-11-01</created><updated>2016-02-15</updated><authors><author><keyname>Maurya</keyname><forenames>Abhinav</forenames></author></authors><title>Spatial Semantic Scan: Detecting Subtle, Spatially Localized Events in
  Text Streams</title><categories>cs.LG cs.CL stat.ML</categories><comments>This paper has been withdrawn by the author due to substantial
  changes in methodology and experimental setup</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many methods have been proposed for detecting emerging events in text streams
using topic modeling. However, these methods have shortcomings that make them
unsuitable for rapid detection of locally emerging events on massive text
streams. We describe Spatially Compact Semantic Scan (SCSS) that has been
developed specifically to overcome the shortcomings of current methods in
detecting new spatially compact events in text streams. SCSS employs
alternating optimization between using semantic scan to estimate contrastive
foreground topics in documents, and discovering spatial neighborhoods with high
occurrence of documents containing the foreground topics. We evaluate our
method on Emergency Department chief complaints dataset (ED dataset) to verify
the effectiveness of our method in detecting real-world disease outbreaks from
free-text ED chief complaint data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00353</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00353</id><created>2015-11-01</created><updated>2016-01-12</updated><authors><author><keyname>Shaviv</keyname><forenames>Dor</forenames></author><author><keyname>&#xd6;zg&#xfc;r</keyname><forenames>Ayfer</forenames></author></authors><title>Universally Near Optimal Online Power Control for Energy Harvesting
  Nodes</title><categories>cs.IT cs.NI math.IT</categories><comments>the proposed scheme is shown to be optimal both within constant
  additive and multiplicative gaps; submitted to Journal on Selected Areas in
  Communications - Series on Green Communications and Networking (Issue 3).
  arXiv admin note: text overlap with arXiv:1506.02024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider online power control for an energy harvesting system with random
i.i.d. energy arrivals and a finite size battery. We propose a simple online
power control policy for this channel that requires minimal information
regarding the distribution of the energy arrivals and prove that it is
universally near-optimal for all parameter values. In particular, the policy
depends on the distribution of the energy arrival process only through its mean
and it achieves the optimal long-term average throughput of the channel within
both constant additive and multiplicative gaps. Existing heuristics for online
power control fail to achieve such universal performance. This result also
allows us to approximate the long-term average throughput of the system with a
simple formula, which sheds some light on the qualitative behavior of the
throughput, namely how it depends on the distribution of the energy arrivals
and the size of the battery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00360</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00360</id><created>2015-11-01</created><authors><author><keyname>Ding</keyname><forenames>Chuang</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author><author><keyname>Yan</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Weini</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author></authors><title>Automatic Prosody Prediction for Chinese Speech Synthesis using
  BLSTM-RNN and Embedding Features</title><categories>cs.CL cs.SD</categories><comments>5 pages, 4 figures, ASRU 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prosody affects the naturalness and intelligibility of speech. However,
automatic prosody prediction from text for Chinese speech synthesis is still a
great challenge and the traditional conditional random fields (CRF) based
method always heavily relies on feature engineering. In this paper, we propose
to use neural networks to predict prosodic boundary labels directly from
Chinese characters without any feature engineering. Experimental results show
that stacking feed-forward and bidirectional long short-term memory (BLSTM)
recurrent network layers achieves superior performance over the CRF-based
method. The embedding features learned from raw text further enhance the
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00363</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00363</id><created>2015-11-01</created><updated>2015-11-12</updated><authors><author><keyname>Courbariaux</keyname><forenames>Matthieu</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>David</keyname><forenames>Jean-Pierre</forenames></author></authors><title>BinaryConnect: Training Deep Neural Networks with binary weights during
  propagations</title><categories>cs.LG cs.CV cs.NE</categories><comments>Accepted at NIPS 2015, 9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide
range of tasks, with the best results obtained with large training sets and
large models. In the past, GPUs enabled these breakthroughs because of their
greater computational speed. In the future, faster computation at both training
and test time is likely to be crucial for further progress and for consumer
applications on low-power devices. As a result, there is much interest in
research and development of dedicated hardware for Deep Learning (DL). Binary
weights, i.e., weights which are constrained to only two possible values (e.g.
-1 or 1), would bring great benefits to specialized DL hardware by replacing
many multiply-accumulate operations by simple accumulations, as multipliers are
the most space and power-hungry components of the digital implementation of
neural networks. We introduce BinaryConnect, a method which consists in
training a DNN with binary weights during the forward and backward
propagations, while retaining precision of the stored weights in which
gradients are accumulated. Like other dropout schemes, we show that
BinaryConnect acts as regularizer and we obtain near state-of-the-art results
with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00367</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00367</id><created>2015-11-01</created><authors><author><keyname>Wen</keyname><forenames>Dong</forenames></author><author><keyname>Qin</keyname><forenames>Lu</forenames></author><author><keyname>Zhang</keyname><forenames>Ying</forenames></author><author><keyname>Lin</keyname><forenames>Xuemin</forenames></author><author><keyname>Yu</keyname><forenames>Jeffrey Xu</forenames></author></authors><title>I/O Efficient Core Graph Decomposition at Web Scale</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Core decomposition is a fundamental graph problem with a large number of
applications. Most existing approaches for core decomposition assume that the
graph is kept in memory of a machine. Nevertheless, many real-world graphs are
big and may not reside in memory. In the literature, there is only one work for
I/O efficient core decomposition that avoids loading the whole graph in memory.
However, this approach is not scalable to handle big graphs because it cannot
bound the memory size and may load most parts of the graph in memory. In
addition, this approach can hardly handle graph updates. In this paper, we
study I/O efficient core decomposition following a semi-external model, which
only allows node information to be loaded in memory. This model works well in
many web-scale graphs. We propose a semi-external algorithm and two optimized
algorithms for I/O efficient core decomposition using very simple structures
and data access model. To handle dynamic graph updates, we show that our
algorithm can be naturally extended to handle edge deletion. We also propose an
I/O efficient core maintenance algorithm to handle edge insertion, and an
improved algorithm to further reduce I/O and CPU cost by investigating some new
graph properties. We conduct extensive experiments on 12 real large graphs. Our
optimal algorithm significantly outperform the existing I/O efficient algorithm
in terms of both processing time and memory consumption. In many
memory-resident graphs, our algorithms for both core decomposition and
maintenance can even outperform the in-memory algorithm due to the simple
structures and data access model used. Our algorithms are very scalable to
handle web-scale graphs. As an example, we are the first to handle a web graph
with 978.5 million nodes and 42.6 billion edges using less than 4.2 GB memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00378</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00378</id><created>2015-11-02</created><authors><author><keyname>Lu</keyname><forenames>Jiaxun</forenames></author><author><keyname>Chen</keyname><forenames>Zhengchuan</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Subcarrier Grouping for MIMO-OFDM Systems over Correlated
  Double-selective Fading Channels</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 figures, journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increase of physical antenna and subcarrier numbers in MIMO-OFDM
systems, channel side information feedback amount and signal precoding
complexity overburden will consume much more system resource, even become
intolerable. To solve this problem, previous works mainly focused on fixed
subcarrier grouping size and precoded MIMO signals in the same group with
unitary channel state information (CSI). It could reduce the system overburden,
but such a process would lead to system capacity loss due to the channel
mismatch in precoding procedure. In this paper, we consider a MIMOOFDM system
over double-selective i.i.d. Rayleigh channels and investigate the quantitative
relation between group size and capacity loss theoretically. By exploiting our
developed theoretical results, we also propose an adaptive subcarrier grouping
algorithm, which not only enables to have a good control of system service
quality but also to reduce system overburden significantly. Numerical results
are shown to provide valuable insights on the system design of MIMO-OFDM
systems and indicate that the proposed subcarrier grouping scheme is extremely
efficient in some common scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00379</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00379</id><created>2015-11-02</created><authors><author><keyname>Demirtas</keyname><forenames>Sefa</forenames></author><author><keyname>Oppenheim</keyname><forenames>Alan V.</forenames></author></authors><title>A Functional Composition Approach to Filter Sharpening and Modular
  Filter Design</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing and implementing systems as an interconnection of smaller
subsystems is a common practice for modularity and standardization of
components and design algorithms. Although not typically cast in this
framework, many of these approaches can be viewed within the mathematical
context of functional composition. This paper re-interprets and generalizes one
such approach known as filter sharpening, i.e. interconnecting filter modules
with significant approximation error to obtain improved filter characteristics,
within the framework of functional composition. More specifically, filter
sharpening is approached by determining the composing polynomial to minimize
the infinity-norm of the approximation error, utilizing the First Algorithm of
Remez. This is applied both to sharpening for FIR, even-symmetric filters and
for the more general case of subfilters that have complex-valued frequency
responses including causal IIR filters and for continuous-time filters. Within
the framework of functional composition, this paper also explores the use of
functional decomposition to approximate a desired system as a composition of
simpler functions based on a two-norm on the approximation error. Among the
potential advantages of this decomposition is the ability for modular
implementation in which the inner component of the functional decomposition
represents the subfilters and the outer the interconnection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00382</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00382</id><created>2015-11-02</created><updated>2015-11-04</updated><authors><author><keyname>Heilman</keyname><forenames>Steven</forenames></author></authors><title>Low Correlation Noise Stability of Symmetric Sets</title><categories>math.PR cs.CC math.FA</categories><comments>43 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Gaussian noise stability of subsets A of Euclidean space
satisfying A=-A. It is shown that an interval centered at the origin, or its
complement, maximizes noise stability for small correlation, among symmetric
subsets of the real line of fixed Gaussian measure. In the plane, we show that
a ball or its complement locally maximizes noise stability for small
correlation, among symmetric sets of fixed Gaussian volume. Some asymptotic
results are proven in higher-dimensional Euclidean space. In summary, we
provide the first known positive results for the Symmetric Gaussian Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00384</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00384</id><created>2015-11-02</created><authors><author><keyname>Ryman</keyname><forenames>Arthur</forenames></author></authors><title>Z Specification for the W3C Editor's Draft Core SHACL Semantics</title><categories>cs.DB cs.AI cs.LO</categories><comments>57 pages, Invited Expert contribution to the W3C RDF Data Shapes
  Working Group</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides a formalization of the W3C Draft Core SHACL Semantics
specification using Z notation. This formalization exercise has identified a
number of quality issues in the draft. It has also established that the
recursive definitions in the draft are well-founded. Further formal validation
of the draft will require the use of an executable specification technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00393</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00393</id><created>2015-11-02</created><authors><author><keyname>Ding</keyname><forenames>Yin</forenames></author><author><keyname>He</keyname><forenames>Wangpeng</forenames></author><author><keyname>Chen</keyname><forenames>Binqiang</forenames></author><author><keyname>Zi</keyname><forenames>Yanyang</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan W.</forenames></author></authors><title>Fault Detection of Rotation Machinery using Periodic Time-Frequency
  Sparsity</title><categories>cs.SD</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses the problem of extracting periodic oscillatory features
in vibration signals for detecting faults in rotation machinery. To extract the
feature, we propose an approach in the short-time Fourier transform (STFT)
domain, where the periodic oscillatory feature manifests itself as a relatively
sparse grid.To estimate the sparse grid, we formulate an optimization problem
using customized binary weights in the regularizer, where the weights are
formulated to promote periodicity. As examples, the proposed approach is
applied to simulated data, and used as a tool for diagnosing faults in bearings
and gearboxes for real data, and compared to some to some state-of-the-art
methods. The results show the proposed approach can effectively detect and
extract the periodical oscillatory features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00394</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00394</id><created>2015-11-02</created><updated>2016-02-23</updated><authors><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, SIERRA</affiliation></author></authors><title>Submodular Functions: from Discrete to Continous Domains</title><categories>cs.LG math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular set-functions have many applications in combinatorial
optimization, as they can be minimized and approximately maximized in
polynomial time. A key element in many of the algorithms and analyses is the
possibility of extending the submodular set-function to a convex function,
which opens up tools from convex optimization. Submodularity goes beyond
set-functions and has naturally been considered for problems with multiple
labels or for functions defined on continuous domains, where it corresponds
essentially to cross second-derivatives being nonpositive. In this paper, we
show that most results relating submodularity and convexity for set-functions
can be extended to all submodular functions. In particular, (a) we naturally
define a continuous extension in a set of probability measures, (b) show that
the extension is convex if and only if the original function is submodular, (c)
prove that the problem of minimizing a submodular function is equivalent to a
typically non-smooth convex optimization problem, and (d) propose another
convex optimization problem with better computational properties (e.g., a
smooth dual problem). Most of these extensions from the set-function situation
are obtained by drawing links with the theory of multi-marginal optimal
transport, which provides also a new interpretation of existing results for
set-functions. We then provide practical algorithms to minimize generic
submodular functions on discrete domains, with associated convergence rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00412</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00412</id><created>2015-11-02</created><authors><author><keyname>Araiza-Illan</keyname><forenames>Dejanira</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>Evaluating Model Checking Approaches to Verify Stability of Control
  Systems in Simulink</title><categories>cs.SY</categories><comments>Submitted for review to European Control Conference 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the verification of stability, a control requirement,
over discrete control systems represented as Simulink diagrams, using different
model checking approaches and tools. Model checking comprises the (exhaustive)
exploration of a model of a system, to determine if a requirement is satisfied.
If that is not the case, examples of the requirement's violation within the
system's model are provided, as witnesses. These examples are potentially
complementary to previous work on automatic theorem proving, when a system is
not proven to be stable, but no proof of instability can be provided.
  We experimentally evaluated the suitability of four model checking approaches
to verify stability on a set of benchmarks including linear and nonlinear,
controlled and uncontrolled, discrete systems, via Lyapunov's second method or
Lyapunov's direct method. Our study included symbolic, bounded, statistical and
hybrid model checking, through the open-source tools NuSMV, UCLID, S-TaLiRo and
SpaceEx, respectively. Our experiments and results provide an insight on the
strengths and limitations of these model checking approaches for the
verification of control requirements for discrete systems at Simulink level. We
found that statistical model checking with S-TaLiRo is the most suitable option
to complement our previous work on automatic theorem proving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00414</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00414</id><created>2015-11-02</created><authors><author><keyname>Hanel</keyname><forenames>Rudolf</forenames></author><author><keyname>Corominas-Murtra</keyname><forenames>Bernat</forenames></author><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author></authors><title>Analytical computation of frequency distributions of path-dependent
  processes by means of a non-multinomial maximum entropy approach</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path-dependent stochastic processes are often non-ergodic and the ensemble
picture can no longer be used to compute observables. The resulting
mathematical difficulties impose limits to the analytical understanding of
path-dependent processes. The statistics of path-dependent processes is often
not multinomial, in the sense that the multiplicities for the occurrence of
events is not a multinomial factor. The popular maximum entropy principle is
tightly related to multinomial processes, non-interacting systems and to the
ensemble picture, and loses its meaning for path-dependent processes. Here we
show that an equivalent to the ensemble picture exists for path-dependent
processes, and can be constructed if the non-multinomial statistics of the
underlying process is captured correctly in a functional that plays the role of
an entropy. We demonstrate this for P\'olya urn processes that serve as a well
known prototype for path-dependent processes. P\'olya urns, which have been
used in a huge range of practical applications, are self-reinforcing processes
that explicitly break multinomial structure. We show the predictive power of
the resulting analytical method by computing the frequency and rank
distributions of P\'olya urn processes. For the first time we are able to use
microscopic update rules of a path-dependent process to construct a
non-multinomial entropy functional, that, when maximized, yields details of
time-dependent distribution functions, as we show numerically. We discuss the
implications on the limits of max-ent based statistical inference when the
statistics of a data source is unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00418</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00418</id><created>2015-11-02</created><authors><author><keyname>Ivanov</keyname><forenames>Mikhail</forenames></author><author><keyname>Brannstrom</keyname><forenames>Fredrik</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Broadcast Coded Slotted ALOHA for Vehicular Communications: A Finite
  Frame Length Analysis</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1501.03389</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an uncoordinated medium access control (MAC) protocol, called
all-to-all broadcast coded slotted ALOHA (B-CSA) for reliable all-to-all
broadcast in vehicular networks. Unlike unicast coded slotted ALOHA, in B-CSA
each user acts as both transmitter and receiver in a half-duplex mode. The
half-duplex mode gives rise to a double unequal error protection (DUEP)
phenomenon: the more the user repeats its packet, the higher the probability
that this packet is decoded by other users, but the lower the probability for
this user to decode packets from others. We analyze the performance of B-CSA
over the packet erasure channel for a finite frame length. In particular, we
provide a general analysis of stopping sets for B-CSA and derive an analytical
approximation of the performance in the error floor (EF) region, which captures
the DUEP feature of B-CSA. Simulation results reveal that the proposed
approximation predicts very well the performance of B-CSA in the EF region.
Finally, we compare the proposed B-CSA protocol with carrier sense multiple
access (CSMA), the current MAC mechanism in vehicular networks. The results
show that B-CSA is able to support a much lager number of users than CSMA with
the same reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00422</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00422</id><created>2015-11-02</created><authors><author><keyname>Holroyd</keyname><forenames>Alexander E.</forenames></author><author><keyname>Levine</keyname><forenames>Lionel</forenames></author><author><keyname>Winkler</keyname><forenames>Peter</forenames></author></authors><title>Abelian logic gates</title><categories>cs.DM cs.FL math.CO</categories><comments>33 pages, many figures</comments><msc-class>68Q10, 68Q45, 68Q85, 90B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An abelian processor is an automaton whose output is independent of the order
of its inputs. Bond and Levine have proved that a network of abelian processors
performs the same computation regardless of processing order (subject only to a
halting condition). We prove that any finite abelian processor can be emulated
by a network of certain very simple abelian processors, which we call gates.
The most fundamental gate is a &quot;toppler&quot;, which absorbs input particles until
their number exceeds some given threshold, at which point it topples, emitting
one particle and returning to its initial state. With the exception of an adder
gate, which simply combines two streams of particles, each of our gates has
only one input wire. Our results can be reformulated in terms of the functions
computed by processors, and one consequence is that any increasing function
from N^k to N^l that is the sum of a linear function and a periodic function
can be expressed in terms of floors of quotients by integers, and addition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00423</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00423</id><created>2015-11-02</created><authors><author><keyname>Li</keyname><forenames>Xiaobai</forenames></author><author><keyname>Hong</keyname><forenames>Xiaopeng</forenames></author><author><keyname>Moilanen</keyname><forenames>Antti</forenames></author><author><keyname>Huang</keyname><forenames>Xiaohua</forenames></author><author><keyname>Pfister</keyname><forenames>Tomas</forenames></author><author><keyname>Zhao</keyname><forenames>Guoying</forenames></author><author><keyname>Pietik&#xe4;inen</keyname><forenames>Matti</forenames></author></authors><title>Reading Hidden Emotions: Spontaneous Micro-expression Spotting and
  Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Micro-expressions (MEs) are rapid, involuntary facial expressions which
reveal emotions that people do not intend to show. Studying MEs is valuable as
recognizing them has many important applications, particularly in forensic
science and psychotherapy. However, analyzing spontaneous MEs is very
challenging due to their short duration and low intensity. Automatic ME
analysis includes two tasks: ME spotting and ME recognition.For ME spotting,
previous studies have focused on posed rather than spontaneous videos. For ME
recognition, the performance of previous studies is low. To address these
challenges, we make the following contributions: (i) We propose the first
method for spotting spontaneous MEs in long videos (by exploiting feature
difference contrast). This method is training free and works on arbitrary
unseen videos. (ii) We present an advanced ME recognition framework, which
outperforms previous work by a large margin on two challenging spontaneous ME
databases (SMIC and CASMEII). (iii) We propose the first automatic ME analysis
system (MESR), which can spot and recognize MEs from spontaneous video data.
Finally, we show that our method achieves comparable performance to humans at
this very challenging task, and outperforms humans in the ME recognition task
by a large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00428</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00428</id><created>2015-11-02</created><authors><author><keyname>Gajbhiye</keyname><forenames>Sneha</forenames></author><author><keyname>Banavar</keyname><forenames>Ravi N.</forenames></author></authors><title>Geometric approach to tracking and stabilization for a spherical robot
  actuated by internal rotors</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper adopts a geometric approach to stabilization and tracking of a
spherical robot actuated by three internal rotors mounted on three mutually
orthogonal axes inside the robot. The system is underactuated and subject to
nonholonomic constraints. Initially, the equations of motion are derived
through Euler-Poincar\'{e} reduction. Then two feedback control laws are
synthesized: the first control law addresses orientation of the robot alone,
keeping the contact point arbitrary; the second control law is a tracking law
that addresses the contact position and angular velocity tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00438</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00438</id><created>2015-11-02</created><updated>2015-11-20</updated><authors><author><keyname>Lidon</keyname><forenames>Aniol</forenames></author><author><keyname>Bola&#xf1;os</keyname><forenames>Marc</forenames></author><author><keyname>Dimiccoli</keyname><forenames>Mariella</forenames></author><author><keyname>Radeva</keyname><forenames>Petia</forenames></author><author><keyname>Garolera</keyname><forenames>Maite</forenames></author><author><keyname>Gir&#xf3;-i-Nieto</keyname><forenames>Xavier</forenames></author></authors><title>Semantic Summarization of Egocentric Photo Stream Events</title><categories>cs.CV</categories><comments>12 pages, 16 figures, 2 tables. Submitted to Transactions on Human
  Machine Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid increase of users of wearable cameras in recent years and of
the amount of data they produce, there is a strong need for automatic retrieval
and summarization techniques. This work addresses the problem of automatically
summarizing egocentric photo streams captured through a wearable camera by
taking an image retrieval perspective. After removing non-informative images by
a new CNN-based filter, images are ranked by relevance to ensure semantic
diversity and finally re-ranked by a novelty criterion to reduce redundancy. To
assess the results, a new evaluation metric is proposed which takes into
account the non-uniqueness of the solution. Experimental results applied on a
database of 7,110 images from 6 different subjects and evaluated by experts
gave 95.74% of experts satisfaction and a Mean Opinion Score of 4.57 out of
5.0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00440</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00440</id><created>2015-11-02</created><authors><author><keyname>Zhao</keyname><forenames>Bo</forenames></author><author><keyname>Zhou</keyname><forenames>Hucheng</forenames></author><author><keyname>Li</keyname><forenames>Guoqiang</forenames></author><author><keyname>Huang</keyname><forenames>Yihua</forenames></author></authors><title>ZenLDA: An Efficient and Scalable Topic Model Training System on
  Distributed Data-Parallel Platform</title><categories>cs.DC</categories><comments>11 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:1412.4986 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents our recent efforts, zenLDA, an efficient and scalable
Collapsed Gibbs Sampling system for Latent Dirichlet Allocation training, which
is thought to be challenging that both data parallelism and model parallelism
are required because of the Big sampling data with up to billions of documents
and Big model size with up to trillions of parameters. zenLDA combines both
algorithm level improvements and system level optimizations. It first presents
a novel CGS algorithm that balances the time complexity, model accuracy and
parallelization flexibility. The input corpus in zenLDA is represented as a
directed graph and model parameters are annotated as the corresponding vertex
attributes. The distributed training is parallelized by partitioning the graph
that in each iteration it first applies CGS step for all partitions in
parallel, followed by synchronizing the computed model each other. In this way,
both data parallelism and model parallelism are achieved by converting them to
graph parallelism. We revisited the tradeoff between system efficiency and
model accuracy and presented approximations such as unsynchronized model,
sparse model initialization and &quot;converged&quot; token exclusion. zenLDA is built on
GraphX in Spark that provides distributed data abstraction (RDD) and expressive
APIs to simplify the programming efforts and simultaneously hides the system
complexities. This enables us to implement other CGS algorithm with a few lines
of code change. To better fit in distributed data-parallel framework and
achieve comparable performance with contemporary systems, we also presented
several system level optimizations to push the performance limit. zenLDA was
evaluated it against web-scale corpus, and the result indicates that zenLDA can
achieve about much better performance than other CGS algorithm we implemented,
and simultaneously achieve better model accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00441</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00441</id><created>2015-11-02</created><authors><author><keyname>Liu</keyname><forenames>Ying</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Do</keyname><forenames>Younghae</forenames></author></authors><title>Identify influential spreaders in complex networks, the role of
  neighborhood</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the most influential spreaders is an important issue in
controlling the spreading processes in complex networks. Centrality measures
are used to rank node influence in a spreading dynamics. Here we propose a node
influence measure based on the centrality of a node and its neighbors'
centrality, which we call the neighborhood centrality. By simulating the
spreading processes in six real-world networks, we find that the neighborhood
centrality greatly outperforms the basic centrality of a node such as the
degree and coreness in ranking node influence and identifying the most
influential spreaders. Interestingly, we discover a saturation effect in
considering the neighborhood of a node, which is not the case of the larger the
better. Specifically speaking, considering the 2-step neighborhood of nodes is
a good choice that balances the cost and performance. If further step of
neighborhood is taken into consideration, there is no obvious improvement and
even decrease in the ranking performance. The saturation effect may be
informative for studies that make use of the local structure of a node to
determine its importance in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00442</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00442</id><created>2015-11-02</created><updated>2016-02-15</updated><authors><author><keyname>Lutz</keyname><forenames>Jack H.</forenames></author><author><keyname>Lutz</keyname><forenames>Neil</forenames></author></authors><title>Algorithmic information, plane Kakeya sets, and conditional dimension</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate the conditional Kolmogorov complexity of x given y at precision
r, where x and y are points in Euclidean spaces and r is a natural number. We
demonstrate the utility of this notion in two
  1. We prove a point-to-set principle that enables one to use the
(relativized, constructive) dimension of a single point in a set E in a
Euclidean space to establish a lower bound on the (classical) Hausdorff
dimension of E. We then use this principle, together with conditional
Kolmogorov complexity in Euclidean spaces, to give a new proof of the known,
two-dimensional case of the Kakeya conjecture. This theorem of geometric
measure theory, proved by Davies in 1971, says that every plane set containing
a unit line segment in every direction has Hausdorff dimension 2.
  2. We use conditional Kolmogorov complexity in Euclidean spaces to develop
the lower and upper conditional dimensions dim(x|y) and Dim(x|y) of x given y,
where x and y are points in Euclidean spaces. Intuitively these are the lower
and upper asymptotic algorithmic information densities of x conditioned on the
information in y. We prove that these conditional dimensions are robust and
that they have the correct information-theoretic relationships with the well
studied dimensions dim(x) and Dim(x) and mutual dimensions mdim(x:y) and
Mdim(x:y).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00444</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00444</id><created>2015-11-02</created><updated>2015-11-04</updated><authors><author><keyname>Brussee</keyname><forenames>Paul</forenames></author><author><keyname>Pouwelse</keyname><forenames>Johan</forenames></author></authors><title>Autonomous smartphone apps: self-compilation, mutation, and viral
  spreading</title><categories>cs.CY cs.CR</categories><comments>7 pages, 5 figures; addition section 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first smart phone tool that is capable of self-compilation,
mutation and viral spreading. Our autonomous app does not require a host
computer to alter its functionality, change its appearance and lacks the normal
necessity of a central app store to spread among hosts. We pioneered survival
skills for mobile software in order to overcome disrupted Internet access due
to natural disasters and human made interference, like Internet kill switches
or censored networks. Internet kill switches have proven to be an effective
tool to eradicate open Internet access and all forms of digital communication
within an hour on a country-wide basis. We present the first operational tool
that is capable of surviving such digital eradication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00446</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00446</id><created>2015-11-02</created><updated>2015-11-03</updated><authors><author><keyname>Jung</keyname><forenames>Jaehoon</forenames></author><author><keyname>Lee</keyname><forenames>Sang-Rim</forenames></author><author><keyname>Lee</keyname><forenames>Inkyu</forenames></author></authors><title>Saturation Power based Simple Energy Efficiency Maximization Schemes for
  MU-MISO Systems</title><categories>cs.IT math.IT</categories><comments>21 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate an energy efficiency (EE) maximization problem
in multi-user multiple input single output downlink channels. The optimization
problem in this system model is difficult to solve in general, since it is in
non-convex fractional form. Hence, conventional algorithms have addressed the
problem in an iterative manner for each channel realization, which leads to
high computational complexity. To tackle this complexity issue, we propose a
new simple method by utilizing the fact that the EE maximization is identical
to the spectral efficiency (SE) maximization for the region of the power below
a certain transmit power referred to as saturation power. In order to calculate
the saturation power, we first introduce upper and lower bounds of the EE
performance by adopting a maximal ratio transmission beamforming strategy.
Then, we propose an efficient way to compute the saturation power for the EE
maximization problem. Once we determine the saturation power corresponding to
the maximum EE in advance, we can solve the EE maximization problem with SE
maximization schemes with low complexity. The derived saturation power is
parameterized by employing random matrix theory, which relies only on the
second order channel statistics. Hence, this approach requires much lower
computational complexity compared to a conventional scheme which exploits
instantaneous channel state information, and provides insight on the saturation
power. Numerical results validate that the proposed algorithm achieves near
optimal EE performance with significantly reduced complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00452</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00452</id><created>2015-11-02</created><updated>2016-02-24</updated><authors><author><keyname>Ashlagi</keyname><forenames>Itai</forenames></author><author><keyname>Gonczarowski</keyname><forenames>Yannai A.</forenames></author></authors><title>No Stable Matching Mechanism is Obviously Strategy-Proof</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The (men-proposing) deferred acceptance algorithm, which finds the
men-optimal stable matching in a two-sided marriage market, is known to be
strategy-proof for all men. This algorithm is now used extensively by many
centralized matching markets. Although applicants are being advised that it is
in their best interest to state their true preferences, empirical evidence
suggests that a significant fraction are nonetheless attempting to
strategically misreport their preferences. We show that while strategy-proof
for all men, no mechanism implementing the men-optimal stable matching is
&quot;obviously strategy-proof&quot; for all men, a term recently defined by Li (2015).
Consequently, no stable matching mechanism is obviously strategy-proof for all
men. This implies that in any implementation of the men-optimal stable
matching, significant cognitive effort and contingent reasoning are needed to
be convinced that no strategic opportunities exist for any man.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00457</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00457</id><created>2015-11-02</created><updated>2016-02-12</updated><authors><author><keyname>Kozlov</keyname><forenames>Dmitry N.</forenames></author></authors><title>Structure theory of flip graphs with applications to Weak Symmetry
  Breaking</title><categories>cs.DC math.CO</categories><msc-class>68Q85, 05C70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a~class of graphs which we call flip graphs. These
graphs encode adjacency structure in certain subcomplexes of iterated standard
chromatic subdivisions of a~simplex. While keeping the geometric background in
mind we develop the structure theory of matchings in flip graphs in a~purely
combinatorial way. The main applications of our theory lie in the field of
theoretical distributed computing, as we use our combinatorial results to prove
new estimates on the symmetry breaking number, introduced in~\cite{wsb6}.
  Specifically, we are able to make significant progress in understanding the
complexity of the so-called Weak Symmetry Breaking task. We prove several
results concerning this complexity, with the rather unexpected main theorem
stating that there exists infinitely many values of $n$, for which the Weak
Symmetry Breaking is solvable in $3$ rounds. In particular, the minimal number
of rounds which a~distributed protocol needs to solve the Weak Symmetry
Breaking task does not go to infinity when the number of processes goes to
infinity.
  As a consequence of our result, we suggest to change the paradigm. We think,
that the bounds on the complexity of solving WSB for $n$ processes should be
formulated in terms of the size of the solutions of the associated Diophantine
equation, rather the value $n$ itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00461</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00461</id><created>2015-11-02</created><authors><author><keyname>Zhang</keyname><forenames>Hanqing</forenames></author><author><keyname>Wiklund</keyname><forenames>Krister</forenames></author><author><keyname>Andersson</keyname><forenames>Magnus</forenames></author></authors><title>Circle detection using isosceles triangles sampling</title><categories>cs.CV</categories><comments>Manuscript, 31 pages, 11 figures</comments><msc-class>I.5.4</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection of circular objects in digital images is an important problem in
several vision applications. Circle detection using randomized sampling has
been developed in recent years to reduce the computational intensity.
Randomized sampling, however, is sensitive to noise that can lead to reduced
accuracy and false-positive candidates. This paper presents a new circle
detection method based upon randomized isosceles triangles sampling to improve
the robustness of randomized circle detection in noisy conditions. It is shown
that the geometrical property of isosceles triangles provide a robust criterion
to find relevant edge pixels and thereby efficiently provide an estimation of
the circle center and radii. The estimated results given by the isosceles
triangles sampling from each connected component of edge map were analyzed
using a simple clustering approach for efficiency. To further improve on the
accuracy we applied a two-step refinement process using chords and linear error
compensation with gradient information of the edge pixels. Extensive
experiments using both synthetic and real images were presented and results
were compared to leading state-of-the-art algorithms and showed that the
proposed algorithm: are efficient in finding circles with a low number of
iterations; has high rejection rate of false-positive circle candidates; and
has high robustness against noise, making it adaptive and useful in many vision
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00472</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00472</id><created>2015-11-02</created><updated>2015-11-03</updated><authors><author><keyname>Mettes</keyname><forenames>Pascal</forenames></author><author><keyname>Tan</keyname><forenames>Robby T.</forenames></author><author><keyname>Veltkamp</keyname><forenames>Remco C.</forenames></author></authors><title>Water Detection through Spatio-Temporal Invariant Descriptors</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we aim to segment and detect water in videos. Water detection
is beneficial for appllications such as video search, outdoor surveillance, and
systems such as unmanned ground vehicles and unmanned aerial vehicles. The
specific problem, however, is less discussed compared to general texture
recognition. Here, we analyze several motion properties of water. First, we
describe a video pre-processing step, to increase invariance against water
reflections and water colours. Second, we investigate the temporal and spatial
properties of water and derive corresponding local descriptors. The descriptors
are used to locally classify the presence of water and a binary water detection
mask is generated through spatio-temporal Markov Random Field regularization of
the local classifications. Third, we introduce the Video Water Database,
containing several hours of water and non-water videos, to validate our
algorithm. Experimental evaluation on the Video Water Database and the DynTex
database indicates the effectiveness of the proposed algorithm, outperforming
multiple algorithms for dynamic texture recognition and material recognition by
ca. 5% and 15% respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00486</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00486</id><created>2015-11-02</created><authors><author><keyname>Fraigniaud</keyname><forenames>Pierre</forenames></author><author><keyname>Korman</keyname><forenames>Amos</forenames></author><author><keyname>Rodeh</keyname><forenames>Yoav</forenames></author></authors><title>Parallel Exhaustive Search without Coordination</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze parallel algorithms in the context of exhaustive search over
totally ordered sets. Imagine an infinite list of &quot;boxes&quot;, with a &quot;treasure&quot;
hidden in one of them, where the boxes' order reflects the importance of
finding the treasure in a given box. At each time step, a search protocol
executed by a searcher has the ability to peek into one box, and see whether
the treasure is present or not. By equally dividing the workload between them,
$k$ searchers can find the treasure $k$ times faster than one searcher.
However, this straightforward strategy is very sensitive to failures (e.g.,
crashes of processors), and overcoming this issue seems to require a large
amount of communication. We therefore address the question of designing
parallel search algorithms maximizing their speed-up and maintaining high
levels of robustness, while minimizing the amount of resources for
coordination. Based on the observation that algorithms that avoid communication
are inherently robust, we analyze the best running time performance of
non-coordinating algorithms. Specifically, we devise non-coordinating
algorithms that achieve a speed-up of $9/8$ for two searchers, a speed-up of
$4/3$ for three searchers, and in general, a speed-up of $\frac{k}{4}(1+1/k)^2$
for any $k\geq 1$ searchers. Thus, asymptotically, the speed-up is only four
times worse compared to the case of full-coordination, and our algorithms are
surprisingly simple and hence applicable. Moreover, these bounds are tight in a
strong sense as no non-coordinating search algorithm can achieve better
speed-ups. Overall, we highlight that, in faulty contexts in which coordination
between the searchers is technically difficult to implement, intrusive with
respect to privacy, and/or costly in term of resources, it might well be worth
giving up on coordination, and simply run our non-coordinating exhaustive
search algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00493</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00493</id><created>2015-11-02</created><updated>2016-02-19</updated><authors><author><keyname>Guo</keyname><forenames>Heng</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author></authors><title>Uniqueness, Spatial Mixing, and Approximation for Ferromagnetic 2-Spin
  Systems</title><categories>cs.DS cs.DM</categories><comments>v2: improved presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For anti-ferromagnetic 2-spin systems, a beautiful connection has been
established, namely that the following three notions align perfectly: the
uniqueness in infinite regular trees, the decay of correlations (also known as
spatial mixing), and the approximability of the partition function. The
uniqueness condition implies spatial mixing, and an FPTAS for the partition
function exists based on spatial mixing. On the other hand, non-uniqueness
implies some long range correlation, based on which NP-hardness reductions are
built. These connections for ferromagnetic 2-spin systems are much less clear,
despite their similarities to anti-ferromagnetic systems. The celebrated
Jerrum-Sinclair Markov chain [JS93] works even if spatial mixing or uniqueness
fails.
  We provide some partial answers. We use $(\beta,\gamma)$ to denote the
$(+,+)$ and $(-,-)$ edge interactions and $\lambda$ the external field, where
$\beta\gamma&gt;1$. If all fields satisfy $\lambda&lt;\lambda_c$ (assuming
$\beta\le\gamma$), where
$\lambda_c=\left(\gamma/\beta\right)^\frac{\Delta_c+1}{2}$ and
$\Delta_c=\frac{\sqrt{\beta\gamma}+1}{\sqrt{\beta\gamma}-1}$, then a weaker
version of spatial mixing holds in all trees. Moreover, if $\beta\le 1$, then
$\lambda&lt;\lambda_c$ is sufficient to guarantee strong spatial mixing and FPTAS.
This improves the previous best algorithm, which is an FPRAS based on Markov
chains and works for $\lambda&lt;\gamma/\beta$ [LLZ14a]. The bound $\lambda_c$ is
almost optimal. When $\beta\le 1$, uniqueness holds in all infinite regular
trees, if and only if $\lambda\le\lambda_c^{int}$, where
$\lambda_c^{int}=\left(\gamma/\beta\right)^\frac{\lceil\Delta_c\rceil+1}{2}$.
If we allow fields $\lambda&gt;\lambda_c^{int'}$, where
$\lambda_c^{int'}=\left(\gamma/\beta\right)^\frac{\lfloor\Delta_c\rfloor+2}{2}$,
then approximating the partition function is #BIS-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00509</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00509</id><created>2015-11-02</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Swami</keyname><forenames>Ananthram</forenames></author><author><keyname>McDaniel</keyname><forenames>Patrick</forenames></author></authors><title>Six Potential Game-Changers in Cyber Security: Towards Priorities in
  Cyber Science and Engineering</title><categories>cs.CR</categories><comments>A version of this paper was presented as a keynote talk at the NATO
  Symposium on Cyber Security Science and Engineering, 13-14 October 2014,
  Tallinn, Estonia. A much shorter version was published in IEEE Computer
  (Kott, Alexander, Ananthram Swami, and Patrick McDaniel. &quot;Security Outlook:
  Six Cyber Game Changers for the Next 15 Years.&quot; Computer 47.12 (2014):
  104-106)</comments><msc-class>c.2.0, k.6.5</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fields of study encompassed by cyber science and engineering are broad
and poorly defined at this time. As national governments and research
communities increase their recognition of the importance, urgency and technical
richness of these disciplines, a question of priorities arises: what specific
sub-areas of research should be the foci of attention and funding? In this
paper we point to an approach to answering this question. We explore results of
a recent workshop that postulated possible game-changers or disruptive changes
that might occur in cyber security within the next 15 years. We suggest that
such game-changers may be useful in focusing attention of research communities
on high-priority topics. Indeed, if a drastic, important change is likely to
occur, should we not focus our research efforts on the nature and ramifications
of the phenomena pertaining to that change? We illustrate each of the
game-changers examples of related current research, and then offer
recommendations for advancement of cyber science and engineering with respect
to each of the six game-changers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00511</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00511</id><created>2015-11-02</created><authors><author><keyname>Haller</keyname><forenames>Philipp</forenames></author><author><keyname>Miller</keyname><forenames>Heather</forenames></author></authors><title>A Formal Model for Direct-style Asynchronous Observables</title><categories>cs.PL</categories><acm-class>D.3.1; D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Languages like F#, C#, and recently also Scala, provide &quot;Async&quot; programming
models which aim to make asynchronous programming easier by avoiding an
inversion of control that is inherent in callback-based programming models.
This paper presents a novel approach to integrate the Async model with
observable streams of the Reactive Extensions model. Reactive Extensions are
best-known from the .NET platform, and widely-used implementations of its
programming model exist also for Java, Ruby, and other languages. This paper
contributes a formalization of the unified &quot;Reactive Async&quot; model in the
context of an object-based core calculus. Our formal model captures the essence
of the protocol of asynchronous observables using a heap evolution property. We
prove a subject reduction theorem; the theorem implies that reduction preserves
the heap evolution property. Thus, for well-typed programs our calculus ensures
the protocol of asynchronous observables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00513</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00513</id><created>2015-11-02</created><authors><author><keyname>Bittel</keyname><forenames>Sebastian</forenames></author><author><keyname>Kaiser</keyname><forenames>Vitali</forenames></author><author><keyname>Teichmann</keyname><forenames>Marvin</forenames></author><author><keyname>Thoma</keyname><forenames>Martin</forenames></author></authors><title>Pixel-wise Segmentation of Street with Neural Networks</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Pixel-wise street segmentation of photographs taken from a drivers
perspective is important for self-driving cars and can also support other
object recognition tasks. A framework called SST was developed to examine the
accuracy and execution time of different neural networks. The best neural
network achieved an $F_1$-score of 89.5% with a simple feedforward neural
network which trained to solve a regression task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00523</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00523</id><created>2015-11-02</created><authors><author><keyname>Hunter</keyname><forenames>Paul</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Guillermo A.</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Minimizing Regret in Discounted-Sum Games</title><categories>cs.GT cs.FL cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1504.01708</comments><acm-class>F.1.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we look at the problem of minimizing regret in discounted-sum
games. We give algorithms for the general problem of computing the minimal
regret of the controller (Eve) as well as several variants depending on which
strategies the environment (Adam) is permitted to use. We also consider the
problem of synthesizing regret-free strategies for Eve in each of these
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00529</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00529</id><created>2015-11-02</created><authors><author><keyname>Santamar&#xed;a-Bonfil</keyname><forenames>Guillermo</forenames></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>Nelson</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Measuring the Complexity of Continuous Distributions</title><categories>nlin.AO cond-mat.stat-mech cs.CC</categories><comments>21 pages, 5 Tables, 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend previously proposed measures of complexity, emergence, and
self-organization to continuous distributions using differential entropy. This
allows us to calculate the complexity of phenomena for which distributions are
known. We find that a broad range of common parameters found in Gaussian and
scale-free distributions present high complexity values. We also explore the
relationship between our measure of complexity and information adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00540</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00540</id><created>2015-11-02</created><authors><author><keyname>Binas</keyname><forenames>Jonathan</forenames></author><author><keyname>Indiveri</keyname><forenames>Giacomo</forenames></author><author><keyname>Pfeiffer</keyname><forenames>Michael</forenames></author></authors><title>Spiking Analog VLSI Neuron Assemblies as Constraint Satisfaction Problem
  Solvers</title><categories>cs.NE</categories><comments>Submitted to ISCAS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving constraint satisfaction problems (CSPs) is a notoriously expensive
computational task. Recently, it has been proposed that efficient stochastic
solvers can be obtained via appropriately configured spiking neural networks
performing Markov Chain Monte Carlo (MCMC) sampling. The possibility to run
such models on massively parallel, low-power, neuromorphic hardware holds great
promise; however, previously proposed networks are based on probabilistic
spiking neurons, and thus rely on random number generators or external sources
of noise to achieve the necessary stochasticity, leading to significant
overhead in the implementation. Here we show how stochasticity can be achieved
by implementing deterministic models of integrate and fire neurons using
subthreshold analog circuits that are affected by thermal noise. We present an
efficient implementation of spike-based CSP solvers implemented on a
reconfigurable neural network VLSI device, which exploits the device's
intrinsic noise sources. We apply the overall concept to the solution of
generic Sudoku problems, and present experimental results obtained from the
neuromorphic device generating solutions to the problem at high rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00542</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00542</id><created>2015-11-02</created><authors><author><keyname>Vaddi</keyname><forenames>Mahesh Babu</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Optimal Vector Linear Index Codes for Some Symmetric Side Information
  Problems</title><categories>cs.IT math.IT</categories><comments>8 pages and 1 figure. arXiv admin note: text overlap with
  arXiv:1510.08592</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with vector linear index codes for multiple unicast index
coding problems where there is a source with K messages and there are K
receivers each wanting a unique message and having symmetric (with respect to
the receiver index) two-sided antidotes (side information). Optimal scalar
linear index codes for several such instances of this class of problems for
one-sided antidotes(not necessarily adjacent) have already been reported. These
codes can be viewed as special cases of the symmetric unicast index coding
problems discussed by Maleki, Cadambe and Jafar with one sided adjacent
antidotes. In this paper, starting from a given multiple unicast index coding
problem with with K messages and one-sided adjacent antidotes for which a
scalar linear index code $\mathfrak{C}$ is known, we give a construction
procedure which constructs a sequence (indexed by m) of multiple unicast index
problems with two-sided adjacent antidotes (for the same source) for all of
which a vector linear code $\mathfrak{C}^{(m)}$ is obtained from
$\mathfrak{C}.$ Also, it is shown that if $\mathfrak{C}$ is optimal then
$\mathfrak{C}^{(m)}$ is also optimal for all $m.$ We illustrate our
construction for some of the known optimal scalar linear codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00544</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00544</id><created>2015-11-02</created><authors><author><keyname>Luo</keyname><forenames>Yuan</forenames></author><author><keyname>Gao</keyname><forenames>Lin</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Spectrum Reservation Contract Design in TV White Space Networks</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a broker-based TV white space market, where
unlicensed white space devices (WSDs) purchase white space spectrum from TV
licensees via a third-party geo-location database (DB), which serves as a
spectrum broker, reserving spectrum from TV licensees and then reselling the
reserved spectrum to WSDs. We propose a contract-theoretic framework for the
database's spectrum reservation under demand stochasticity and information
asymmetry. In such a framework, the database offers a set of contract items in
the form of reservation amount and the corresponding payment, and each WSD
chooses the best contract item based on its private information. We
systematically study the optimal reservation contract design (that maximizes
the database's expected profit) under two different risk-bearing schemes:
DB-bearing-risk and WSD-bearing-risk, depending on who (the database or the
WSDs) will bear the risk of over reservation. Counter-intuitively, we show that
the optimal contract under DB-bearing-risk leads to a higher profit for the
database and a higher total network profit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00561</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00561</id><created>2015-11-02</created><updated>2015-12-08</updated><authors><author><keyname>Badrinarayanan</keyname><forenames>Vijay</forenames></author><author><keyname>Kendall</keyname><forenames>Alex</forenames></author><author><keyname>Cipolla</keyname><forenames>Roberto</forenames></author></authors><title>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image
  Segmentation</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel and practical deep fully convolutional neural network
architecture for semantic pixel-wise segmentation termed SegNet. This core
trainable segmentation engine consists of an encoder network, a corresponding
decoder network followed by a pixel-wise classification layer. The architecture
of the encoder network is topologically identical to the 13 convolutional
layers in the VGG16 network . The role of the decoder network is to map the low
resolution encoder feature maps to full input resolution feature maps for
pixel-wise classification. The novelty of SegNet lies is in the manner in which
the decoder upsamples its lower resolution input feature map(s). Specifically,
the decoder uses pooling indices computed in the max-pooling step of the
corresponding encoder to perform non-linear upsampling. This eliminates the
need for learning to upsample. The upsampled maps are sparse and are then
convolved with trainable filters to produce dense feature maps. We compare our
proposed architecture with the fully convolutional network (FCN) architecture
and its variants. This comparison reveals the memory versus accuracy trade-off
involved in achieving good segmentation performance. The design of SegNet was
primarily motivated by road scene understanding applications. Hence, it is
efficient both in terms of memory and computational time during inference. It
is also significantly smaller in the number of trainable parameters than
competing architectures and can be trained end-to-end using stochastic gradient
descent. We also benchmark the performance of SegNet on Pascal VOC12 salient
object segmentation and the recent SUN RGB-D indoor scene understanding
challenge. We show that SegNet provides competitive performance although it is
significantly smaller than other architectures. We also provide a Caffe
implementation of SegNet and a webdemo at
http://mi.eng.cam.ac.uk/projects/segnet/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00562</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00562</id><created>2015-11-02</created><authors><author><keyname>L&#xe1;zaro</keyname><forenames>Francisco</forenames></author><author><keyname>Paolini</keyname><forenames>Enrico</forenames></author><author><keyname>Liva</keyname><forenames>Gianluigi</forenames></author><author><keyname>Bauch</keyname><forenames>Gerhard</forenames></author></authors><title>Distance Spectrum of Fixed-Rate Raptor Codes with Linear Random
  Precoders</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the IEEE Journal on Selected Areas in
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Raptor code ensembles with linear random outer codes in a fixed-rate setting
are considered. An expression for the average distance spectrum is derived and
this expression is used to obtain the asymptotic exponent of the weight
distribution. The asymptotic growth rate analysis is then exploited to develop
a necessary and sufficient condition under which the fixed-rate Raptor code
ensemble exhibits a strictly positive typical minimum distance. The condition
involves the rate of the outer code, the rate of the inner fixed-rate Luby
Transform (LT) code and the LT code degree distribution. Additionally, it is
shown that for ensembles fulfilling this condition, the minimum distance of a
code randomly drawn from the ensemble has a linear growth with the block
length. The analytical results can be used to make accurate predictions of the
performance of finite length Raptor codes. These results are particularly
useful for fixed-rate Raptor codes under maximum likelihood erasure decoding,
whose performance is driven by their weight distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00567</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00567</id><created>2015-11-02</created><authors><author><keyname>Mercian</keyname><forenames>Anu</forenames></author><author><keyname>Gurrola</keyname><forenames>Elliot I.</forenames></author><author><keyname>Aurzada</keyname><forenames>Frank</forenames></author><author><keyname>McGarry</keyname><forenames>Michael P.</forenames></author><author><keyname>Reisslein</keyname><forenames>Martin</forenames></author></authors><title>Upstream Polling Protocols for Flow Control in PON/xDSL Hybrid Access
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a hybrid PON/xDSL access network, multiple Customer Premise Equipment
(CPE) nodes connect over individual Digital Subscriber Lines (DSLs) to a
drop-point device. The drop-point device, which is typically reverse powered
from the customer, is co-located with an Optical Network Unit (ONU) of the
Passive Optical Network (PON). We demonstrate that the drop-point experiences
very high buffer occupancies when no flow control or standard Ethernet PAUSE
frame flow control is employed. In order to reduce the buffer occupancies in
the drop-point, we introduce two gated flow control protocols that extend the
polling-based PON medium access control to the DSL segments between the CPEs
and the ONUs. We analyze the timing of the gated flow control mechanisms to
specify the latest possible time instant when CPEs can start the DSL upstream
transmissions so that the ONU can forward the upstream transmissions at the
full PON upstream transmission bit rate. Through extensive simulations for a
wide range of bursty traffic models, we find that the gated flow control
mechanisms, specifically, the ONU and CPE grant sizing policies, enable
effective control of the maximum drop-point buffer occupancies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00568</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00568</id><created>2015-11-02</created><authors><author><keyname>Kumar</keyname><forenames>Amit</forenames></author><author><keyname>Malhotra</keyname><forenames>Santosh</forenames></author></authors><title>Network Security Threats and Protection Models</title><categories>cs.CR</categories><comments>12 pages</comments><report-no>CSE-101507</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a brave new age of global connectivity and e-commerce, interconnections
via networks have heightened, creating for both individuals and organizations,
a state of complete dependence upon vulnerable systems for storage and transfer
of information. Never before, have so many people had power in their own hands.
The power to deface websites, access personal mail accounts, and worse more the
potential to bring down entire governments, and financial corporations through
openly documented software codes. This paper discusses the possible exploits on
typical network components, it will cite real life scenarios, and propose
practical measures that can be taken as safeguard. Then, it describes some of
the key efforts done by the research community to prevent such attacks, mainly
by using Firewall and Intrusion Detection Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00573</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00573</id><created>2015-11-02</created><authors><author><keyname>Hashimoto</keyname><forenames>Tatsunori B.</forenames></author><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author></authors><title>From random walks to distances on unweighted graphs</title><categories>stat.ML cs.AI cs.SI</categories><comments>To appear in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large unweighted directed graphs are commonly used to capture relations
between entities. A fundamental problem in the analysis of such networks is to
properly define the similarity or dissimilarity between any two vertices.
Despite the significance of this problem, statistical characterization of the
proposed metrics has been limited. We introduce and develop a class of
techniques for analyzing random walks on graphs using stochastic calculus.
Using these techniques we generalize results on the degeneracy of hitting times
and analyze a metric based on the Laplace transformed hitting time (LTHT). The
metric serves as a natural, provably well-behaved alternative to the expected
hitting time. We establish a general correspondence between hitting times of
the Brownian motion and analogous hitting times on the graph. We show that the
LTHT is consistent with respect to the underlying metric of a geometric graph,
preserves clustering tendency, and remains robust against random addition of
non-geometric edges. Tests on simulated and real-world data show that the LTHT
matches theoretical predictions and outperforms alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00575</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00575</id><created>2015-11-02</created><authors><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Lin</keyname><forenames>Xiaojun</forenames></author><author><keyname>Mohsenian-Rad</keyname><forenames>Hamed</forenames></author></authors><title>Proactive Demand Response for Data Centers: A Win-Win Solution</title><categories>cs.SY cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to reduce the energy cost of data centers, recent studies suggest
distributing computation workload among multiple geographically dispersed data
centers, by exploiting the electricity price difference. However, the impact of
data center load redistribution on the power grid is not well understood yet.
This paper takes the first step towards tackling this important issue, by
studying how the power grid can take advantage of the data centers' load
distribution proactively for the purpose of power load balancing. We model the
interactions between power grid and data centers as a two-stage problem, where
the utility company chooses proper pricing mechanisms to balance the electric
power load in the first stage, and the data centers seek to minimize their
total energy cost by responding to the prices in the second stage. We show that
the two-stage problem is a bilevel quadratic program, which is NP-hard and
cannot be solved using standard convex optimization techniques. We introduce
benchmark problems to derive upper and lower bounds for the solution of the
two-stage problem. We further propose a branch and bound algorithm to attain
the globally optimal solution, and propose a heuristic algorithm with low
computational complexity to obtain an alternative close-to-optimal solution. We
also study the impact of background load prediction error using the theoretical
framework of robust optimization. The simulation results demonstrate that our
proposed scheme can not only improve the power grid reliability but also reduce
the energy cost of data centers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00576</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00576</id><created>2015-11-02</created><updated>2016-02-18</updated><authors><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author><author><keyname>Keusch</keyname><forenames>Ralph</forenames></author><author><keyname>Lengler</keyname><forenames>Johannes</forenames></author></authors><title>Geometric Inhomogeneous Random Graphs</title><categories>cs.SI cs.DM cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world networks, like social networks or the internet infrastructure,
have structural properties such as their large clustering coefficient that can
best be described in terms of an underlying geometry. This is why the focus of
the literature on theoretical models for real-world networks shifted from
classic models without geometry, such as Chung-Lu random graphs, to modern
geometry-based models, such as hyperbolic random graphs.
  With this paper we contribute to the theoretical analysis of these modern,
more realistic random graph models. However, we do not directly study
hyperbolic random graphs, but replace them by a more general model that we call
\emph{geometric inhomogeneous random graphs} (GIRGs). Since we ignore constant
factors in the edge probabilities, our model is technically simpler
(specifically, we avoid hyperbolic cosines), while preserving the qualitative
behaviour of hyperbolic random graphs, and we suggest to replace hyperbolic
random graphs by our new model in future theoretical studies.
  We prove the following fundamental structural and algorithmic results on
GIRGs. (1) We provide a sampling algorithm that generates a random graph from
our model in expected linear time, improving the best-known sampling algorithm
for hyperbolic random graphs by a factor $O(\sqrt{n})$, (2) we establish that
GIRGs have a constant clustering coefficient, (3) we show that GIRGs have small
separators, i.e., it suffices to delete a sublinear number of edges to break
the giant component into two large pieces, and (4) we show how to compress
GIRGs using an expected linear number of bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00577</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00577</id><created>2015-10-18</created><authors><author><keyname>Che</keyname><forenames>Tiben</forenames></author><author><keyname>Xu</keyname><forenames>Jingwei</forenames></author><author><keyname>Choi</keyname><forenames>Gwan</forenames></author></authors><title>Overlapped List Successive Cancellation Approach for Hardware Efficient
  Polar Code Decoder</title><categories>cs.IT math.IT</categories><comments>submitted to ISCAS2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an efficient hardware design approach for list successive
cancellation (LSC) decoding of polar codes. By applying path-overlapping
scheme, the l instances of (l &gt; 1) successive cancellation (SC) decoder for LSC
with list size l can be cut down to only one. This results in a dramatic
reduction of the hardware complexity without any decoding performance loss. We
also develop novel approaches to reduce the latencyassociated with the pipeline
scheme. Simulation results show that with proposed design approach the hardware
efficiency is increased significantly over the recently proposed LSC decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00613</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00613</id><created>2015-11-02</created><authors><author><keyname>Chua</keyname><forenames>Freddy C.</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>A Bayesian Approach to the Partitioning of Workflows</title><categories>cs.DC physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When partitioning workflows in realistic scenarios, the knowledge of the
processing units is often vague or unknown. A naive approach to addressing this
issue is to perform many controlled experiments for different workloads, each
consisting of multiple number of trials in order to estimate the mean and
variance of the specific workload. Since this controlled experimental approach
can be quite costly in terms of time and resources, we propose a variant of the
Gibbs Sampling algorithm that uses a sequence of Bayesian inference updates to
estimate the processing characteristics of the processing units. Using the
inferred characteristics of the processing units, we are able to determine the
best way to split a workflow for processing it in parallel with the lowest
expected completion time and least variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00615</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00615</id><created>2015-11-02</created><authors><author><keyname>Vazifeh</keyname><forenames>Mohammad M.</forenames></author><author><keyname>Zhang</keyname><forenames>Hongmou</forenames></author><author><keyname>Santi</keyname><forenames>Paolo</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Optimizing the Deployment of Electric Vehicle Charging Stations Using
  Pervasive Mobility Data</title><categories>cs.SY cs.SI</categories><comments>11 pages, 8 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With recent advances in battery technology and the resulting decrease in the
charging times, public charging stations are becoming a viable option for
Electric Vehicle (EV) drivers. Concurrently, wide-spread use of
location-tracking devices in mobile phones and wearable devices makes it
possible to track individual-level human movements to an unprecedented spatial
and temporal grain. Motivated by these developments, we propose a novel
methodology to perform data-driven optimization of EV charging stations
location. We formulate the problem as a discrete optimization problem on a
geographical grid, with the objective of covering the entire demand region
while minimizing a measure of drivers' discomfort. Since optimally solving the
problem is computationally infeasible, we present computationally efficient,
near-optimal solutions based on greedy and genetic algorithms. We then apply
the proposed methodology to optimize EV charging stations location in the city
of Boston, starting from a massive cellular phone data sets covering 1 million
users over 4 months. Results show that genetic algorithm based optimization
provides the best solutions in terms of drivers' discomfort and the number of
charging stations required, which are both reduced about 10 percent as compared
to a randomized solution. We further investigate robustness of the proposed
data-driven methodology, showing that, building upon well-known regularity of
aggregate human mobility patterns, the near-optimal solution computed using
single day movements preserves its properties also in later months. When
collectively considered, the results presented in this paper clearly indicate
the potential of data-driven approaches for optimally locating public charging
facilities at the urban scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00619</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00619</id><created>2015-11-02</created><authors><author><keyname>Libert</keyname><forenames>Timothy</forenames></author></authors><title>Exposing the Hidden Web: An Analysis of Third-Party HTTP Requests on 1
  Million Websites</title><categories>cs.CR cs.CY</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This article provides a quantitative analysis of privacy-compromising
mechanisms on 1 million popular websites. Findings indicate that nearly 9 in 10
websites leak user data to parties of which the user is likely unaware; more
than 6 in 10 websites spawn third- party cookies; and more than 8 in 10
websites load Javascript code from external parties onto users' computers.
Sites that leak user data contact an average of nine external domains,
indicating that users may be tracked by multiple entities in tandem. By tracing
the unintended disclosure of personal browsing histories on the Web, it is
revealed that a handful of U.S. companies receive the vast bulk of user data.
Finally, roughly 1 in 5 websites are potentially vulnerable to known National
Security Agency spying techniques at the time of analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00622</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00622</id><created>2015-11-02</created><authors><author><keyname>Eger</keyname><forenames>Steffen</forenames></author></authors><title>On the Number of Many-to-Many Alignments of N Sequences</title><categories>math.CO cs.CL cs.DM</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We count the number of alignments of $N \ge 1$ sequences when match-up types
are from a specified set $S\subseteq \mathbb{N}^N$. Equivalently, we count the
number of nonnegative integer matrices whose rows sum to a given fixed vector
and each of whose columns lie in $S$. We provide a new asymptotic formula for
the case $S=\{(s_1,\ldots,s_N) \:|\: 1\le s_i\le 2\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00628</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00628</id><created>2015-11-02</created><authors><author><keyname>Dolatshah</keyname><forenames>Mohamad</forenames></author><author><keyname>Hadian</keyname><forenames>Ali</forenames></author><author><keyname>Minaei-Bidgoli</keyname><forenames>Behrouz</forenames></author></authors><title>Ball*-tree: Efficient spatial indexing for constrained nearest-neighbor
  search in metric spaces</title><categories>cs.DB cs.CG cs.DS</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging location-based systems and data analysis frameworks requires
efficient management of spatial data for approximate and exact search. Exact
similarity search can be done using space partitioning data structures, such as
Kd-tree, R*-tree, and Ball-tree. In this paper, we focus on Ball-tree, an
efficient search tree that is specific for spatial queries which use euclidean
distance. Each node of a Ball-tree defines a ball, i.e. a hypersphere that
contains a subset of the points to be searched.
  In this paper, we propose Ball*-tree, an improved Ball-tree that is more
efficient for spatial queries. Ball*-tree enjoys a modified space partitioning
algorithm that considers the distribution of the data points in order to find
an efficient splitting hyperplane. Also, we propose a new algorithm for KNN
queries with restricted range using Ball*-tree, which performs better than both
KNN and range search for such queries. Results show that Ball*-tree performs
39%-57% faster than the original Ball-tree algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00635</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00635</id><created>2015-10-21</created><authors><author><keyname>Oskouei</keyname><forenames>Samad Khabbazi</forenames></author></authors><title>Gacs Algorithmic Complexity on Hilbert Spaces and Some of its
  Applications</title><categories>quant-ph cs.IT math.IT</categories><comments>102 pages, PhD Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the notion of Gacs quantum algorithmic entropy, originally
formulated for finitely many qubits, to infinite dimensional quantum spin
chains and investigate the relation of this extension with two quantum
dynamical entropies that have been proposed in recent years. Further, we prove
an extension of Brudno's theorem in quantum spin chains with shift dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00647</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00647</id><created>2015-11-02</created><authors><author><keyname>Wen</keyname><forenames>Min</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author></authors><title>Strategy Synthesis for Stochastic Rabin Games with Discounted Reward</title><categories>cs.SY cs.FL</categories><comments>17 pages, 3 figures, submitted to TACAS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic games are often used to model reactive processes. We consider the
problem of synthesizing an optimal almost-sure winning strategy in a two-player
(namely a system and its environment) turn-based stochastic game with both a
qualitative objective as a Rabin winning condition, and a quantitative
objective as a discounted reward. Optimality is considered only over the
almost-sure winning strategies, i.e., system strategies that guarantee the
satisfaction of the Rabin condition with probability 1 regardless of the
environment's strategy. We show that optimal almost-sure winning strategies may
need infinite memory, but epsilon-optimal almost-sure winning strategies can
always be finite-memory or even memoryless. We identify a sufficient and
necessary condition of the existence of memoryless epsilon-optimal almost-sure
winning strategies and propose an algorithm to compute one when this condition
is satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00648</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00648</id><created>2015-11-02</created><authors><author><keyname>Chen</keyname><forenames>Xue</forenames></author><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author></authors><title>Parameterized Algorithms for Constraint Satisfaction Problems Above
  Average with Global Cardinality Constraints</title><categories>cs.DS</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a constraint satisfaction problem (CSP) on $n$ variables, $x_1, x_2,
\dots, x_n \in \{\pm 1\}$, and $m$ constraints, a global cardinality constraint
is in the form of $\sum_{i = 1}^{n} x_i = (1-2p)n$, where $p \in (\Omega(1), 1
- \Omega(1))$ and $pn$ is an integer. Let $AVG$ be the expected number of
constraints satisfied by randomly choosing an assignment to $x_1, x_2, \dots,
x_n$, complying with the global cardinality constraint. The CSP above average
with the global cardinality constraint problem asks whether there is an
assignment (complying with the cardinality constraint) that satisfies more than
$(AVG+t)$ constraints, where $t$ is an input parameter.
  In this paper, we present an algorithm that finds out a valid assignment
satisfying more than $(AVG+t)$ constraints (if there exists one) in time
$(2^{O(t^2)} + n^{O(d)})$. Therefore, the CSP above average with the global
cardinality constraint problem is fixed-parameter tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00661</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00661</id><created>2015-11-02</created><authors><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Chestnut</keyname><forenames>Stephen R.</forenames></author><author><keyname>Ivkin</keyname><forenames>Nikita</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Beating CountSketch for Heavy Hitters in Insertion Streams</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a stream $p_1, \ldots, p_m$ of items from a universe $\mathcal{U}$,
which, without loss of generality we identify with the set of integers $\{1, 2,
\ldots, n\}$, we consider the problem of returning all $\ell_2$-heavy hitters,
i.e., those items $j$ for which $f_j \geq \epsilon \sqrt{F_2}$, where $f_j$ is
the number of occurrences of item $j$ in the stream, and $F_2 = \sum_{i \in
[n]} f_i^2$. Such a guarantee is considerably stronger than the
$\ell_1$-guarantee, which finds those $j$ for which $f_j \geq \epsilon m$. In
2002, Charikar, Chen, and Farach-Colton suggested the {\sf CountSketch} data
structure, which finds all such $j$ using $\Theta(\log^2 n)$ bits of space (for
constant $\epsilon &gt; 0$). The only known lower bound is $\Omega(\log n)$ bits
of space, which comes from the need to specify the identities of the items
found. In this paper we show it is possible to achieve $O(\log n \log \log n)$
bits of space for this problem. Our techniques, based on Gaussian processes,
lead to a number of other new results for data streams, including
  (1) The first algorithm for estimating $F_2$ simultaneously at all points in
a stream using only $O(\log n\log\log n)$ bits of space, improving a natural
union bound and the algorithm of Huang, Tai, and Yi (2014).
  (2) A way to estimate the $\ell_{\infty}$ norm of a stream up to additive
error $\epsilon \sqrt{F_2}$ with $O(\log n\log\log n)$ bits of space, resolving
Open Question 3 from the IITK 2006 list for insertion only streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00700</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00700</id><created>2015-11-02</created><authors><author><keyname>Abboud</keyname><forenames>Amir</forenames></author><author><keyname>Bodwin</keyname><forenames>Greg</forenames></author></authors><title>The 4/3 Additive Spanner Exponent is Tight</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A spanner is a sparse subgraph that approximately preserves the pairwise
distances of the original graph. It is well known that there is a smooth
tradeoff between the sparsity of a spanner and the quality of its
approximation, so long as distance error is measured multiplicatively. A
central open question in the field is to prove or disprove whether such a
tradeoff exists also in the regime of additive error. That is, is it true that
for all $e&gt;0$, there is a constant $k_e$ such that every graph has a spanner on
$O(n^{1+e})$ edges that preserves its pairwise distances up to $+k_e$? Previous
lower bounds are consistent with a positive resolution to this question, while
previous upper bounds exhibit the beginning of a tradeoff curve: all graphs
have $+2$ spanners on $O(n^{3/2})$ edges, $+4$ spanners on $\tilde{O}(n^{7/5})$
edges, and $+6$ spanners on $O(n^{4/3})$ edges. However, progress has
mysteriously halted at the $n^{4/3}$ bound, and despite significant effort from
the community, the question has remained open for all $0&lt;e&lt;1/3$.
  Our main result is a surprising negative resolution of the open question,
even in a highly generalized setting. We show a new information theoretic
incompressibility bound: there is no mathematical function that compresses
graphs into $O(n^{4/3 - e})$ bits so that distance information can be recovered
within $+n^{o(1)}$ error. As a special case of our theorem, we get a tight
lower bound on the sparsity of additive spanners: the $+6$ spanner on
$O(n^{4/3})$ edges cannot be improved in the exponent, even if any
subpolynomial amount of additive error is allowed. Our theorem implies new
lower bounds for related objects as well; for example, the twenty-year-old $+4$
emulator on $O(n^{4/3})$ edges also cannot be improved in the exponent unless
the error allowance is polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00715</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00715</id><created>2015-11-02</created><authors><author><keyname>Berman</keyname><forenames>Piotr</forenames></author><author><keyname>Fukuyama</keyname><forenames>Junichiro</forenames></author></authors><title>The Distributed Selection Problem and the AKS Sorting Network</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the selection problem on a completely connected network of $n$
processors with no shared memory. Each processor initially holds a given
numeric item of $b$ bits, allowed to send a $b$-bit message to another
processor at a time. On such a communication network ${\cal G}$, we show that
the $k^{th}$ smallest of the $n$ inputs can be detected in $O( \log n )$ time
with $O ( n \log \log n )$ messages. The possibility of such a parallel
algorithm for this distributed $k$-selection problem has been unknown despite
the intensive investigation on many variations of the selection problem carried
out since 1970s. Satisfying the constraint of total $O ( n \log \log n )$
messages, it improves on ${\cal G}$ the asymptotic running time of Kuhn, Locker
and Wattenhofer's algorithm. Our parallel algorithm simulates the comparisons
and swaps performed by the AKS sorting network, the one of logarithmic depth
discovered by Ajtai, Koml\'os and Szemer\'edi in 1983. Simulation of such a
sorting network is our main trick to achieve $O ( \log n )$ time and $O ( n
\log \log n )$ messages simultaneously. Extending its correctness proof, we
will be able improve by about 47% the upper bound found by Seiferas on the
constant factor of the $O ( \log n )$ depth of an $n$-input sorting network.
Furthermore, we show the universal time lower bound $\lg n$ for many basic data
aggregation problems on ${\cal G}$. The class of problems having this lower
bound is huge including the selection problem, the problem of finding the sum
of $n$ items, and that of counting items exceeding a threshold. Thus the
universal lower bound means the asymptotic time optimality of our parallel
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00716</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00716</id><created>2015-11-02</created><authors><author><keyname>Moreira</keyname><forenames>Jo&#xe3;o A. G.</forenames></author><author><keyname>Zeng</keyname><forenames>Xiao Han T.</forenames></author><author><keyname>Amaral</keyname><forenames>Lu&#xed;s A. Nunes</forenames></author></authors><title>The Distribution of the Asymptotic Number of Citations to Sets of
  Publications by a Researcher or From an Academic Department Are Consistent
  With a Discrete Lognormal Model</title><categories>physics.soc-ph cs.DL</categories><comments>20 pages, 11 figures, 3 tables</comments><doi>10.1371/journal.pone.0143108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to quantify the impact of a researcher's or an institution's body of work
is a matter of increasing importance to scientists, funding agencies, and
hiring committees. The use of bibliometric indicators, such as the h-index or
the Journal Impact Factor, have become widespread despite their known
limitations. We argue that most existing bibliometric indicators are
inconsistent, biased, and, worst of all, susceptible to manipulation. Here, we
pursue a principled approach to the development of an indicator to quantify the
scientific impact of both individual researchers and research institutions
grounded on the functional form of the distribution of the asymptotic number of
citations. We validate our approach using the publication records of 1,283
researchers from seven scientific and engineering disciplines and the chemistry
departments at the 106 U.S. research institutions classified as &quot;very high
research activity&quot;. Our approach has three distinct advantages. First, it
accurately captures the overall scientific impact of researchers at all career
stages, as measured by asymptotic citation counts. Second, unlike other
measures, our indicator is resistant to manipulation and rewards publication
quality over quantity. Third, our approach captures the time-evolution of the
scientific impact of research institutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00722</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00722</id><created>2015-11-02</created><authors><author><keyname>Spasojevic</keyname><forenames>Nemanja</forenames></author><author><keyname>Rao</keyname><forenames>Adithya</forenames></author></authors><title>Identifying Actionable Messages on Social Media</title><categories>cs.IR cs.SI</categories><comments>9 pages, 2015 IEEE International Big Data Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text actionability detection is the problem of classifying user authored
natural language text, according to whether it can be acted upon by a
responding agent. In this paper, we propose a supervised learning framework for
domain-aware, large-scale actionability classification of social media
messages. We derive lexicons, perform an in-depth analysis for over 25 text
based features, and explore strategies to handle domains that have limited
training data. We apply these methods to over 46 million messages spanning 75
companies and 35 languages, from both Facebook and Twitter. The models achieve
an aggregate population-weighted F measure of 0.78 and accuracy of 0.74, with
values of over 0.9 in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00725</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00725</id><created>2015-11-02</created><updated>2016-01-24</updated><authors><author><keyname>Dhifli</keyname><forenames>Wajdi</forenames></author><author><keyname>Diallo</keyname><forenames>Abdoulaye Banir&#xe9;</forenames></author></authors><title>Galaxy-X: A Novel Approach for Multi-class Classification in an Open
  Universe</title><categories>cs.LG cs.AI cs.DB cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification is a fundamental task in machine learning and artificial
intelligence. Existing classification methods are designed to classify unknown
instances within a set of previously known classes that are seen in training.
Such classification takes the form of prediction within a closed-set. However,
a more realistic scenario that fits the ground truth of real world applications
is to consider the possibility of encountering instances that do not belong to
any of the classes that are seen in training, $i.e.$, an open-set
classification. In such situation, existing closed-set classification methods
will assign a training label to these instances resulting in a
misclassification. In this paper, we introduce Galaxy-X, a novel multi-class
classification method for open-set problem. For each class of the training set,
Galaxy-X creates a minimum bounding hyper-sphere that encompasses the
distribution of the class by enclosing all of its instances. In such manner,
our method is able to distinguish instances resembling previously seen classes
from those that are of unseen classes. To adequately evaluate open-set
classification, we introduce a novel evaluation procedure. Experimental results
on benchmark datasets as well as on synthetic datasets show the efficiency of
our approach in classifying novel instances from known as well as unknown
classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00735</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00735</id><created>2015-11-02</created><updated>2016-01-24</updated><authors><author><keyname>Wang</keyname><forenames>Qi</forenames></author><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author></authors><title>Large-Scale Analysis of the Accuracy of the Journal Classification
  Systems of Web of Science and Scopus</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Journal classification systems play an important role in bibliometric
analyses. The two most important bibliographic databases, Web of Science and
Scopus, each provide a journal classification system. However, no study has
systematically investigated the accuracy of these classification systems. To
examine and compare the accuracy of journal classification systems, we define
two criteria on the basis of direct citation relations between journals and
categories. We use Criterion I to select journals that have weak connections
with their assigned categories, and we use Criterion II to identify journals
that are not assigned to categories with which they have strong connections. If
a journal satisfies either of the two criteria, we conclude that its assignment
to categories may be questionable. Accordingly, we identify all journals with
questionable classifications in Web of Science and Scopus. Furthermore, we
perform a more in-depth analysis for the field of Library and Information
Science to assess whether our proposed criteria are appropriate and whether
they yield meaningful results. It turns out that according to our
citation-based criteria Web of Science performs significantly better than
Scopus in terms of the accuracy of its journal classification system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00736</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00736</id><created>2015-11-02</created><updated>2016-01-24</updated><authors><author><keyname>Dhifli</keyname><forenames>Wajdi</forenames></author><author><keyname>Diallo</keyname><forenames>Abdoulaye Banir&#xe9;</forenames></author></authors><title>ProtNN: Fast and Accurate Nearest Neighbor Protein Function Prediction
  based on Graph Embedding in Structural and Topological Space</title><categories>cs.LG cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studying the function of proteins is important for understanding the
molecular mechanisms of life. The number of publicly available protein
structures has increasingly become extremely large. Still, the determination of
the function of a protein structure remains a difficult, costly, and time
consuming task. The difficulties are often due to the essential role of spatial
and topological structures in the determination of protein functions in living
cells. In this paper, we propose ProtNN, a novel approach for protein function
prediction. Given an unannotated protein structure and a set of annotated
proteins, ProtNN finds the nearest neighbor annotated structures based on
protein-graph pairwise similarities. Given a query protein, ProtNN finds the
nearest neighbor reference proteins based on a graph representation model and a
pairwise similarity between vector embedding of both query and reference
protein-graphs in structural and topological spaces. ProtNN assigns to the
query protein the function with the highest number of votes across the set of k
nearest neighbor reference proteins, where k is a user-defined parameter.
Experimental evaluation demonstrates that ProtNN is able to accurately classify
several datasets in an extremely fast runtime compared to state-of-the-art
approaches. We further show that ProtNN is able to scale up to a whole PDB
dataset in a single-process mode with no parallelization, with a gain of
thousands order of magnitude of runtime compared to state-of-the-art
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00740</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00740</id><created>2015-11-02</created><authors><author><keyname>Mart&#xed;nez-Miranda</keyname><forenames>Enrique</forenames></author><author><keyname>McBurney</keyname><forenames>Peter</forenames></author><author><keyname>Howard</keyname><forenames>Matthew J.</forenames></author></authors><title>Learning Unfair Trading: a Market Manipulation Analysis From the
  Reinforcement Learning Perspective</title><categories>q-fin.TR cs.LG</categories><comments>7 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Market manipulation is a strategy used by traders to alter the price of
financial securities. One type of manipulation is based on the process of
buying or selling assets by using several trading strategies, among them
spoofing is a popular strategy and is considered illegal by market regulators.
Some promising tools have been developed to detect manipulation, but cases can
still be found in the markets. In this paper we model spoofing and pinging
trading, two strategies that differ in the legal background but share the same
elemental concept of market manipulation. We use a reinforcement learning
framework within the full and partial observability of Markov decision
processes and analyse the underlying behaviour of the manipulators by finding
the causes of what encourages the traders to perform fraudulent activities.
This reveals procedures to counter the problem that may be helpful to market
regulators as our model predicts the activity of spoofers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00750</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00750</id><created>2015-11-02</created><authors><author><keyname>Berbeglia</keyname><forenames>Franco</forenames></author><author><keyname>Berbeglia</keyname><forenames>Gerardo</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>The Benefits of Segmentation in Trial-Offer Markets with Social
  Influence and Position Bias</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purchasing behaviour of consumers is often influenced by numerous
factors, including the visibility of the products and the influence of other
customers through their own purchases or their recommendations.
  Motivated by trial-offer and freemium markets and a number of online markets
for cultural products, leisure services, and retail, this paper studies the
dynamics of a marketplace ran by a single firm and which is visited by
heterogeneous consumers whose choice preferences can be modeled using a Mixed
Multinomial Logit. In this marketplace, consumers are influenced by past
purchases, the inherent appeal of the products, and the visibility of each
product. The resulting market generalizes recent models already verified in
cultural markets.
  We examine various marketing policies for this market and analyze their
long-term dynamics and the potential benefits of social influence. In
particular, we show that the heterogeneity of the customers complicates the
market significantly: Many of the optimality and computational properties of
the corresponding homogeneous market no longer hold. To remedy these
limitations, we explore a market segmentation strategy and quantify its
benefits. The theoretical results are complemented by Monte Carlo simulations
conducted on examples of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00754</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00754</id><created>2015-11-02</created><authors><author><keyname>Chen</keyname><forenames>Yu-Fang</forenames></author><author><keyname>Hsieh</keyname><forenames>Chiao</forenames></author><author><keyname>Leng&#xe1;l</keyname><forenames>Ond&#x159;ej</forenames></author><author><keyname>Lii</keyname><forenames>Tsung-Ju</forenames></author><author><keyname>Tsai</keyname><forenames>Ming-Hsien</forenames></author><author><keyname>Wang</keyname><forenames>Bow-Yaw</forenames></author><author><keyname>Wang</keyname><forenames>Farn</forenames></author></authors><title>PAC Learning-Based Verification and Model Synthesis</title><categories>cs.SE cs.LG cs.LO</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel technique for verification and model synthesis of
sequential programs. Our technique is based on learning a regular model of the
set of feasible paths in a program, and testing whether this model contains an
incorrect behavior. Exact learning algorithms require checking equivalence
between the model and the program, which is a difficult problem, in general
undecidable. Our learning procedure is therefore based on the framework of
probably approximately correct (PAC) learning, which uses sampling instead and
provides correctness guarantees expressed using the terms error probability and
confidence. Besides the verification result, our procedure also outputs the
model with the said correctness guarantees. Obtained preliminary experiments
show encouraging results, in some cases even outperforming mature software
verifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00758</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00758</id><created>2015-11-02</created><updated>2016-02-17</updated><authors><author><keyname>Pillai</keyname><forenames>Sudeep</forenames></author><author><keyname>Ramalingam</keyname><forenames>Srikumar</forenames></author><author><keyname>Leonard</keyname><forenames>John J.</forenames></author></authors><title>High-Performance and Tunable Stereo Reconstruction</title><categories>cs.RO cs.CV</categories><comments>Accepted to International Conference on Robotics and Automation
  (ICRA) 2016; 8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional stereo algorithms have focused their efforts on reconstruction
quality and have largely avoided prioritizing for run time performance. Robots,
on the other hand, require quick maneuverability and effective computation to
observe its immediate environment and perform tasks within it. In this work, we
propose a high-performance and tunable stereo disparity estimation method, with
a peak frame-rate of 120Hz (VGA resolution, on a single CPU-thread), that can
potentially enable robots to quickly reconstruct their immediate surroundings
and maneuver at high-speeds. Our key contribution is a disparity estimation
algorithm that iteratively approximates the scene depth via a piece-wise planar
mesh from stereo imagery, with a fast depth validation step for semi-dense
reconstruction. The mesh is initially seeded with sparsely matched keypoints,
and is recursively tessellated and refined as needed (via a resampling stage),
to provide the desired stereo disparity accuracy. The inherent simplicity and
speed of our approach, with the ability to tune it to a desired reconstruction
quality and runtime performance makes it a compelling solution for applications
in high-speed vehicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00776</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00776</id><created>2015-11-02</created><authors><author><keyname>Vasudevan</keyname><forenames>K.</forenames></author></authors><title>Coherent Detection of Turbo-Coded OFDM Signals Transmitted through
  Frequency Selective Rayleigh Fading Channels with Receiver Diversity and
  Increased Throughput</title><categories>cs.IT math.IT</categories><comments>15 pages, 16 figures, 3 tables</comments><doi>10.1007/s11277-015-2303-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we discuss techniques for coherently detecting turbo coded
orthogonal frequency division multiplexed (OFDM) signals, transmitted through
frequency selective Rayleigh (the magnitude of each channel tap is Rayleigh
distributed) fading channels having a uniform power delay profile. The channel
output is further distorted by a carrier frequency and phase offset, besides
additive white Gaussian noise (AWGN). A new frame structure for OFDM,
consisting of a known preamble, cyclic prefix, data and known postamble is
proposed, which has a higher throughput compared to the earlier work. A robust
turbo decoder is proposed, which functions effectively over a wide range of
signal-to-noise ratio (SNR). The key contribution to the good performance of
the practical coherent receiver is due to the use of a long preamble (512 QPSK
symbols), which is perhaps not specified in any of the current wireless
communication standards. We have also shown from computer simulations that, it
is possible to obtain even better BER performance, using a better code. A
simple and approximate Cramer-Rao bound on the variance of the frequency offset
estimation error for coherent detection, is derived. The proposed algorithms
are well suited for implementation on a DSP-platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00778</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00778</id><created>2015-11-02</created><authors><author><keyname>Chen</keyname><forenames>Sitan</forenames></author></authors><title>Basis Collapse for Holographic Algorithms Over All Domain Sizes</title><categories>cs.CC</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of holographic algorithms introduced by Valiant represents a novel
approach to achieving polynomial-time algorithms for seemingly intractable
counting problems via a reduction to counting planar perfect matchings and a
linear change of basis. Two fundamental parameters in holographic algorithms
are the \emph{domain size} and the \emph{basis size}. Roughly, the domain size
is the range of colors involved in the counting problem at hand (e.g. counting
graph $k$-colorings is a problem over domain size $k$), while the basis size
$\ell$ captures the dimensionality of the representation of those colors. A
major open problem has been: for a given $k$, what is the smallest $\ell$ for
which any holographic algorithm for a problem over domain size $k$ &quot;collapses
to&quot; (can be simulated by) a holographic algorithm with basis size $\ell$? Cai
and Lu showed in 2008 that over domain size 2, basis size 1 suffices, opening
the door to an extensive line of work on the structural theory of holographic
algorithms over the Boolean domain. Cai and Fu later showed for signatures of
full rank that over domain sizes 3 and 4, basis sizes 1 and 2, respectively,
suffice, and they conjectured that over domain size $k$ there is a collapse to
basis size $\lfloor\log_2 k\rfloor$. In this work, we resolve this conjecture
in the affirmative for signatures of full rank for all $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00781</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00781</id><created>2015-11-03</created><authors><author><keyname>Agarwal</keyname><forenames>Amit</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sudarshan</forenames></author><author><keyname>Mohammed</keyname><forenames>Saif Khan</forenames></author></authors><title>Impact of Underlaid Multi-antenna D2D on Cellular Downlink Users in
  Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE International Conference on Communications (ICC'
  2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a massive multiple-input multiple-output (MIMO)
downlink system underlaid with a device-to-device (D2D) network, where each D2D
user equipment (UE) has multiple antennas. Each D2D transmitter (Tx) uses all
its antennas to beamform information towards its desired D2D receiver, which
uses only a single antenna for reception. Beamforming at the D2D Tx allows us
to reduce D2D interference to neighbouring cellular UEs (CUEs). Due to highly
directional beamforming at the massive MIMO base station, the cellular-to-D2D
interference is negligible. For the above proposed system, we analyze the
average per-user spectral efficiency (SE) of CUEs as a function of the D2D area
spectral efficiency (ASE). Our analysis reveals that for a fixed D2D ASE and
fixed number of D2D antennas, $N$, with increasing density of D2D Txs,
$\lambda$, the average per-user SE of CUEs increases (for sufficiently large
$\lambda$) and approaches a fundamental limit as $\lambda \to \infty$. Also,
this fundamental limit depends on the D2D ASE, $R_{0}^{(d)}$ and $N$, only
through the ratio $R_{0}^{(d)}/(N-1)$. This suggests that for a given
fundamental limit on the per-user SE of CUEs, the D2D ASE can be approximately
doubled with every doubling in $N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00785</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00785</id><created>2015-11-03</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Tang</keyname><forenames>Bo</forenames></author></authors><title>Well-Supported versus Approximate Nash Equilibria: Query Complexity of
  Large Games</title><categories>cs.GT cs.CC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the randomized query complexity of approximate Nash equilibria (ANE)
in large games. We prove that, for some constant $\epsilon&gt;0$, any randomized
oracle algorithm that computes an $\epsilon$-ANE in a binary-action, $n$-player
game must make $2^{\Omega(n/\log n)}$ payoff queries. For the stronger solution
concept of well-supported Nash equilibria (WSNE), Babichenko previously gave an
exponential $2^{\Omega(n)}$ lower bound for the randomized query complexity of
$\epsilon$-WSNE, for some constant $\epsilon&gt;0$; the same lower bound was shown
to hold for $\epsilon$-ANE, but only when $\epsilon=O(1/n)$.
  Our result answers an open problem posed by Hart and Nisan and Babichenko and
is very close to the trivial upper bound of $2^n$. Our proof relies on a
generic reduction from the problem of finding an $\epsilon$-WSNE to the problem
of finding an $\epsilon/(4\alpha)$-ANE, in large games with $\alpha$ actions,
which might be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00787</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00787</id><created>2015-11-03</created><authors><author><keyname>Lavin</keyname><forenames>Alexander</forenames></author></authors><title>A Pareto Optimal D* Search Algorithm for Multiobjective Path Planning</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1505.05947</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path planning is one of the most vital elements of mobile robotics, providing
the agent with a collision-free route through the workspace. The global path
plan can be calculated with a variety of informed search algorithms, most
notably the A* search method, guaranteed to deliver a complete and optimal
solution that minimizes the path cost. D* is widely used for its dynamic
replanning capabilities. Path planning optimization typically looks to minimize
the distance traversed from start to goal, but many mobile robot applications
call for additional path planning objectives, presenting a multiobjective
optimization (MOO) problem. Common search algorithms, e.g. A* and D*, are not
well suited for MOO problems, yielding suboptimal results. The search algorithm
presented in this paper is designed for optimal MOO path planning. The
algorithm incorporates Pareto optimality into D*, and is thus named D*-PO.
Non-dominated solution paths are guaranteed by calculating the Pareto front at
each search step. Simulations were run to model a planetary exploration rover
in a Mars environment, with five path costs. The results show the new, Pareto
optimal D*-PO outperforms the traditional A* and D* algorithms for MOO path
planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00792</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00792</id><created>2015-11-03</created><updated>2016-02-06</updated><authors><author><keyname>Dasgupta</keyname><forenames>Sayantan</forenames></author></authors><title>Scalable Recommendation from Web Usage Mining using Method of Moments</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of mass-available Internet, twenty-first century observed a
steady growth in web based commercial services and technology companies. Most
of them are based on web applications that receive huge amount of user
traffics, and generate massive amount of web usage data containing user-item
interactions. We attempt to build a recommendation algorithm based on such web
usage data. It is essential that recommendation algorithms for such
applications are highly scalable in nature. Existing algorithms such as matrix
factorization run several iterations through the dataset, and therefore may not
be suitable for large web-scale datasets. Here we propose a highly scalable
recommendation algorithm based on recently proposed Method of Moments (also
known as Spectral Method). Our method takes only two to three passes through
the entire dataset to extract the model parameters during the training phase.
We demonstrate the competitive performance of our algorithm in comparison with
the existing algorithms on various publicly available datasets through several
empirical measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00796</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00796</id><created>2015-11-03</created><authors><author><keyname>Nayak</keyname><forenames>Aradhana</forenames></author><author><keyname>Banavar</keyname><forenames>Ravi</forenames></author></authors><title>Almost globally stable tracking on compact Riemannian manifolds</title><categories>cs.SY</categories><comments>6 pages, submitted to the ECC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locally stable feedback control laws to track a reference trajectory for
fully actuated, simple mechanical systems (SMSs) on Riemannian manifolds is
well studied. Almost globally stable tracking for fully actuated SMSs on
compact Lie groups is also addressed in the literature. In this note, we show
almost global stable tracking on the more general compact Riemannian manifolds.
Tracking error functions and velocity transport maps are used to design
feedback laws in SMSs to locally track a smooth reference trajectory having
bounded velocity with exponential convergence using proportional derivative
plus feed forward (PD+FF) control laws. We modify conditions on the choice of
tracking error functions to achieve almost global tracking in a fully actuated
SMS on a compact Riemannian manifold from all initial conditions in an open
dense set on the phase space using the known PD+FF control laws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00797</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00797</id><created>2015-11-03</created><authors><author><keyname>Park</keyname><forenames>Hyun-Seo</forenames></author><author><keyname>Choi</keyname><forenames>Yong-Seouk</forenames></author><author><keyname>Kim</keyname><forenames>Tae-Joong</forenames></author><author><keyname>Kim</keyname><forenames>Byung-Chul</forenames></author><author><keyname>Lee</keyname><forenames>Jae-Yong</forenames></author></authors><title>Is It Possible to Simultaneously Achieve Zero Handover Failure Rate and
  Ping-Pong Rate?</title><categories>cs.NI cs.CG</categories><comments>14 pages, 9 figures. This work has been submitted to IEEE
  Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network densification through the deployment of large number of small cells
has been considered as the dominant driver for wireless evolution into 5G.
However, it has increased the complexity of mobility management, and operators
have been facing the technical challenges in handover (HO) parameter
optimization. The trade-off between the HO failure (HOF) rate and the ping-pong
(PP) rate has further complicated the challenges. In this article, we proposed
ZEro handover failure with Unforced and automatic time-to-execute Scaling
(ZEUS) HO. ZEUS HO assures HO signaling when a user equipment (UE) is in a good
radio link condition and executes the HO at an optimal time. We analyzed the HO
performance of LTE and ZEUS theoretically using a geometry-based model,
considering the most important HO parameter, i.e., HO margin (HOM). We derived
the probabilities of HOF and PP from the analysis. The numerical results
demonstrated that ZEUS HO can achieve zero HOF rate without increasing the PP
rate, solving the trade-off. Furthermore, we showed that the ZEUS HO can
accomplish zero HOF rate and zero PP rate simultaneously with an extension of
keeping fast moving users out of small cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00799</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00799</id><created>2015-11-03</created><authors><author><keyname>Kadam</keyname><forenames>Sudin</forenames></author><author><keyname>Gajbhiye</keyname><forenames>Sneha</forenames></author><author><keyname>Banavar</keyname><forenames>Ravi</forenames></author></authors><title>A geometric approach to the dynamics of flapping wing micro aerial
  vehicles: Modelling and reduction</title><categories>cs.SY</categories><comments>7 pages, 3 figures, submitted to the European Control Conference,
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a geometric framework for analysis of dynamics of
flapping wing micro aerial vehicles (FWMAV) which achieve locomotion in the
special Euclidean group SE(3) using internal shape changes. We review the
special structure of the configuration manifold of such systems. This work
addresses to extend the work in geometric locomotion to the aerial locomotion
problem. Furthermore, there seems to be limited work in modelling of flapping
wing bodies in a geometric framework. We derive the dynamic model of the FWMAV
using Lagrangian reduction theory defined on symmetry groups. The reduction is
achieved by applying Hamilton's variation principle on a reduced Lagrangian.
The resultant dynamics is governed by the Euler-Poincare and Euler-Lagrange
equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00803</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00803</id><created>2015-11-03</created><authors><author><keyname>Borello</keyname><forenames>Martino</forenames></author><author><keyname>Mila</keyname><forenames>Olivier</forenames></author></authors><title>On the Stabilizer of Weight Enumerators of Linear Codes</title><categories>cs.IT math.CO math.IT</categories><comments>11 pages</comments><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the relation between linear codes and the stabilizer
in ${\rm GL}_2(\mathbb{C})$ of their weight enumerators. We prove a result on
the finiteness of stabilizers and give a complete classification of linear
codes with infinite stabilizer in the non-binary case. We present an efficient
algorithm to compute explicitly the stabilizer of weight enumerators and we
apply it to the family of Reed-Muller codes to show that some of their weight
enumerators have trivial stabilizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00813</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00813</id><created>2015-11-03</created><authors><author><keyname>Bailleux</keyname><forenames>Olivier</forenames></author></authors><title>SAT as a game</title><categories>cs.CC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a funny representation of SAT. While the primary interest is to
present propositional satisfiability in a playful way for pedagogical purposes,
it could also inspire new search heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00825</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00825</id><created>2015-11-03</created><authors><author><keyname>Kido</keyname><forenames>Kengo</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Swarat</forenames></author><author><keyname>Hasuo</keyname><forenames>Ichiro</forenames></author></authors><title>Abstract Interpretation with Infinitesimals: Towards Scalability in
  Nonstandard Static Analysis (Extended Version)</title><categories>cs.PL</categories><comments>28 pages, an extended version of a paper accepted in 17th
  International Conference on Verification, Model Checking, and Abstract
  Interpretation (VMCAI 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend abstract interpretation for the purpose of verifying hybrid
systems. Abstraction has been playing an important role in many verification
methodologies for hybrid systems, but some special care is needed for
abstraction of continuous dynamics defined by ODEs. We apply Cousot and
Cousot's framework of abstract interpretation to hybrid systems, almost as it
is, by regarding continuous dynamics as an infinite iteration of infinitesimal
discrete jumps. This extension follows the recent line of work by Suenaga,
Hasuo and Sekine, where deductive verification is extended for hybrid systems
by 1) introducing a constant dt for an infinitesimal value; and 2) employing
Robinson's nonstandard analysis (NSA) to define mathematically rigorous
semantics. Our theoretical results include soundness and termination via
uniform widening operators; and our prototype implementation successfully
verifies some benchmark examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00830</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00830</id><created>2015-11-03</created><updated>2016-02-04</updated><authors><author><keyname>Louizos</keyname><forenames>Christos</forenames></author><author><keyname>Swersky</keyname><forenames>Kevin</forenames></author><author><keyname>Li</keyname><forenames>Yujia</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author><author><keyname>Zemel</keyname><forenames>Richard</forenames></author></authors><title>The Variational Fair Autoencoder</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of learning representations that are invariant to
certain nuisance or sensitive factors of variation in the data while retaining
as much of the remaining information as possible. Our model is based on a
variational autoencoding architecture with priors that encourage independence
between sensitive and latent factors of variation. Any subsequent processing,
such as classification, can then be performed on this purged latent
representation. To remove any remaining dependencies we incorporate an
additional penalty term based on the &quot;Maximum Mean Discrepancy&quot; (MMD) measure.
We discuss how these architectures can be efficiently trained on data and show
in experiments that this method is more effective than previous work in
removing unwanted sources of variation while maintaining informative latent
representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00838</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00838</id><created>2015-11-03</created><authors><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Roytman</keyname><forenames>Alan</forenames></author><author><keyname>Vorsanger</keyname><forenames>Gregory</forenames></author></authors><title>Approximating Subadditive Hadamard Functions on Implicit Matrices</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important challenge in the streaming model is to maintain small-space
approximations of entrywise functions performed on a matrix that is generated
by the outer product of two vectors given as a stream. In other works, streams
typically define matrices in a standard way via a sequence of updates, as in
the work of Woodruff (2014) and others. We describe the matrix formed by the
outer product, and other matrices that do not fall into this category, as
implicit matrices. As such, we consider the general problem of computing over
such implicit matrices with Hadamard functions, which are functions applied
entrywise on a matrix. In this paper, we apply this generalization to provide
new techniques for identifying independence between two vectors in the
streaming model. The previous state of the art algorithm of Braverman and
Ostrovsky (2010) gave a $(1 \pm \epsilon)$-approximation for the $L_1$ distance
between the product and joint distributions, using space $O(\log^{1024}(nm)
\epsilon^{-1024})$, where $m$ is the length of the stream and $n$ denotes the
size of the universe from which stream elements are drawn. Our general
techniques include the $L_1$ distance as a special case, and we give an
improved space bound of $O(\log^{12}(n) \log^{2}({nm \over
\epsilon})\epsilon^{-7})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00840</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00840</id><created>2015-11-03</created><authors><author><keyname>Yakovlev</keyname><forenames>Konstantin</forenames></author><author><keyname>Baskin</keyname><forenames>Egor</forenames></author><author><keyname>Hramoin</keyname><forenames>Ivan</forenames></author></authors><title>Finetuning Randomized Heuristic Search For 2D Path Planning: Finding The
  Best Input Parameters For R* Algorithm Through Series Of Experiments</title><categories>cs.AI</categories><comments>8 pages, 2 figures, 18 references. As accepted to the 16th
  International Conference on Artificial Intelligence:Methodology, Systems,
  Applications (AIMSA 2014), Varna, Bulgaria, September 11-13, 2014</comments><doi>10.1007/978-3-319-10554-3_29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path planning is typically considered in Artificial Intelligence as a graph
searching problem and R* is state-of-the-art algorithm tailored to solve it.
The algorithm decomposes given path finding task into the series of subtasks
each of which can be easily (in computational sense) solved by well-known
methods (such as A*). Parameterized random choice is used to perform the
decomposition and as a result R* performance largely depends on the choice of
its input parameters. In our work we formulate a range of assumptions
concerning possible upper and lower bounds of R* parameters, their
interdependency and their influence on R* performance. Then we evaluate these
assumptions by running a large number of experiments. As a result we formulate
a set of heuristic rules which can be used to initialize the values of R*
parameters in a way that leads to algorithm's best performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00849</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00849</id><created>2015-11-03</created><updated>2016-02-24</updated><authors><author><keyname>van de Hoef</keyname><forenames>Sebastian</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Computing Feasible Vehicle Platooning Opportunities for Transport
  Assignments</title><categories>cs.SY</categories><comments>6 pages, 2 figures, to appear in CTS 2016, 14-th IFAC Symposium on
  Control in Transportation Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicle platooning facilitates the partial automation of vehicles and can
significantly reduce fuel consumption. Mobile communication infrastructure
makes it possible to dynamically coordinate the formation of platoons en route.
We consider a centralized system that provides trucks with routes and speed
profiles allowing them to dynamically form platoons during their journeys. For
this to work, all possible pairs of vehicles that can platoon based on their
location, destination, and other constraints have to be identified. The
presented approach scales well to large vehicle fleets and realistic road
networks by extracting features from the transport assignments of the vehicles
and rules out a majority of possible pairs based on these features only. Merely
a small number of remaining pairs are considered in depth by a complete and
computationally expensive algorithm. This algorithm conclusively decides if
platooning is possible for a pair based on the complete data associated with
the two vehicles. We derive appropriate features for the problem and
demonstrate the effectiveness of the approach in a simulation example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00852</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00852</id><created>2015-11-03</created><authors><author><keyname>Berger</keyname><forenames>Joram</forenames></author><author><keyname>Colombo</keyname><forenames>Fabio</forenames></author><author><keyname>Friese</keyname><forenames>Raphael</forenames></author><author><keyname>Haitz</keyname><forenames>Dominik</forenames></author><author><keyname>Hauth</keyname><forenames>Thomas</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Thomas</forenames></author><author><keyname>Quast</keyname><forenames>G&#xfc;nter</forenames></author><author><keyname>Sieber</keyname><forenames>Georg</forenames></author></authors><title>ARTUS - A Framework for Event-based Data Analysis in High Energy Physics</title><categories>hep-ex cs.SE</categories><comments>8 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ARTUS is an event-based data-processing framework for high energy physics
experiments. It is designed for large-scale data analysis in a collaborative
environment. The architecture design choices take into account typical
challenges and are based on experiences with similar applications. The
structure of the framework and its advantages are described. An example use
case and performance measurements are presented. The framework is well-tested
and successfully used by several analysis groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00856</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00856</id><created>2015-11-03</created><authors><author><keyname>Duda</keyname><forenames>Jarek</forenames></author><author><keyname>Korcyl</keyname><forenames>Grzegorz</forenames></author></authors><title>Designing dedicated data compression for physics experiments within FPGA
  already used for data acquisition</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physics experiments produce enormous amount of raw data, counted in petabytes
per day. Hence, there is large effort to reduce this amount, mainly by using
some filters. The situation can be improved by additionally applying some data
compression techniques: removing redundancy and optimally encoding the actual
information. Preferably, both filtering and data compression should fit in FPGA
already used for data acquisition - reducing requirements of both data storage
and networking architecture.
  We will briefly explain and discuss some basic techniques, for a better focus
applied to design a dedicated data compression system basing on a sample data
from a prototype of a tracking detector: 10000 events for 48 channels. We will
focus on the time data here, which after neglecting the headers and applying
data filtering, requires on average 1170 bits/event using the current coding.
Encoding relative times (differences) and grouping data by channels, reduces
this number to 798 bits/channel, still using fixed length coding: a fixed
number of bits used for a given value. Using variable length Huffman coding to
encode numbers of digital pulses for a channel and the most significant bits of
values (simple binning) reduces further this number to 552 bits/event. Using
adaptive binning: denser for frequent values, and an accurate entropy coder we
get further down to 455 bits/event - this option can easily fit unused
resources of FPGA currently used for data acquisition. Finally, using separate
probability distributions for different channels, what could be done by a
software compressor, leads to 437bits/event, what is 2.67 times less than the
original 1170 bits/event.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00863</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00863</id><created>2015-11-03</created><authors><author><keyname>Siro</keyname><forenames>Topi</forenames></author><author><keyname>Harju</keyname><forenames>Ari</forenames></author></authors><title>Exact diagonalization of quantum lattice models on coprocessors</title><categories>cond-mat.str-el cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We implement the Lanczos algorithm on an Intel Xeon Phi coprocessor and
compare its performance to a multi-core Intel Xeon CPU and an NVIDIA graphics
processor. The Xeon and the Xeon Phi are parallelized with OpenMP and the
graphics processor is programmed with CUDA. The performance is evaluated by
measuring the execution time of a single step in the Lanczos algorithm. We
study two quantum lattice models with different particle numbers, and conclude
that for small systems, the multi-core CPU is the fastest platform, while for
large systems, the graphics processor is the clear winner, reaching speedups of
up to 7.6 compared to the CPU. The Xeon Phi outperforms the CPU with
sufficiently large particle number, reaching a speedup of 2.5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00867</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00867</id><created>2015-11-03</created><updated>2016-02-02</updated><authors><author><keyname>van Ditmarsch</keyname><forenames>Hans</forenames></author><author><keyname>van Eijck</keyname><forenames>Jan</forenames></author><author><keyname>Pardo</keyname><forenames>Pere</forenames></author><author><keyname>Ramezanian</keyname><forenames>Rahim</forenames></author><author><keyname>Schwarzentruber</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Dynamic Gossip</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A gossip protocol is a procedure for spreading secrets among a group of
agents, using a connection graph. We consider distributed gossip protocols
wherein the agents themselves instead of a global scheduler determine whom to
call. In this paper the problem of designing and analyzing gossip protocols is
given a dynamic twist by assuming that when a call is established not only
secrets are exchanged but also telephone numbers. Both numbers and secrets can
be represented by edges in a gossip graph. Thus, each call may change both the
number relation and the secret relation of the graph. We define various such
distributed dynamic gossip protocols, and we characterize them in terms of the
class of graphs where they terminate successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00869</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00869</id><created>2015-11-03</created><authors><author><keyname>Vazquez</keyname><forenames>Rafael</forenames></author><author><keyname>Gavilan</keyname><forenames>Francisco</forenames></author><author><keyname>Camacho</keyname><forenames>Eduardo F.</forenames></author></authors><title>Pulse-Width Predictive Control for LTV Systems with Application to
  Spacecraft Rendezvous</title><categories>math.OC cs.SY</categories><comments>Preprint submitted to Control Engineering Practice</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a model predictive controller (MPC) that is able to handle
linear time-varying (LTV) plants with PWM control. The MPC is based on a
planner that employs a PAM or impulsive approximation as a hot-start and then
uses explicit linearization around successive PWM solutions for rapidly
improving the solution by means of linear programming. As an example, the
problem of rendezvous of spacecraft for eccentric target orbits is considered.
The problem is modeled by the LTV Tschauner-Hempel equations, whose transition
matrix is explicit; this is exploited by the algorithm for rapid convergence.
The efficacy of the method is shown in a simulation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00871</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00871</id><created>2015-11-03</created><authors><author><keyname>Jain</keyname><forenames>Brijnesh J.</forenames></author></authors><title>Properties of the Sample Mean in Graph Spaces and the
  Majorize-Minimize-Mean Algorithm</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most fundamental concepts in statistics is the concept of sample
mean. Properties of the sample mean that are well-defined in Euclidean spaces
become unwieldy or even unclear in graph spaces. Open problems related to the
sample mean of graphs include: non-existence, non-uniqueness, statistical
inconsistency, lack of convergence results of mean algorithms, non-existence of
midpoints, and disparity to midpoints. We present conditions to resolve all six
problems and propose a Majorize-Minimize-Mean (MMM) Algorithm. Experiments on
graph datasets representing images and molecules show that the MMM-Algorithm
best approximates a sample mean of graphs compared to six other mean
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00873</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00873</id><created>2015-11-03</created><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author><author><keyname>Derka</keyname><forenames>Martin</forenames></author></authors><title>The (3,1)-ordering for 4-connected planar triangulations</title><categories>cs.CG cs.DS</categories><acm-class>G.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Canonical orderings of planar graphs have frequently been used in graph
drawing and other graph algorithms. In this paper we introduce the notion of an
$(r,s)$-canonical order, which unifies many of the existing variants of
canonical orderings. We then show that $(3,1)$-canonical ordering for
4-connected triangulations always exist; to our knowledge this variant of
canonical ordering was not previously known. We use it to give much simpler
proofs of two previously known graph drawing results for 4-connected planar
triangulations, namely, rectangular duals and rectangle-of-influence drawings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00876</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00876</id><created>2015-11-03</created><updated>2016-02-18</updated><authors><author><keyname>Heydrich</keyname><forenames>Sandy</forenames></author><author><keyname>van Stee</keyname><forenames>Rob</forenames></author></authors><title>Beating the Harmonic lower bound for online bin packing</title><categories>cs.DS</categories><comments>24 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an online bin packing algorithm with asymptotic performance ratio
of 1.5815, which constitutes the first improvement over the algorithm
Harmonic++ in fifteen years. This algorithm achieved a competitive ratio of
1.58889 and is one instance of the Super Harmonic framework; a lower bound of
Ramanan et al. shows that within this framework, no competitive ratio below
1.58333 can be achieved. We make two crucial changes to that framework. First,
some of the decisions of the algorithm will depend on exact sizes of items,
instead of only their types. In particular, for item pairs where the size of
one item is in (1/3,1/2] and the other is larger than 1/2 (a large item), when
deciding whether to pack such a pair together in one bin, our algorithm does
not consider their types, but only checks whether their total size is at most
1. Second, for items with sizes in (1/3,1/2] (medium items), we try to pack the
larger items of every type in pairs, while combining the smallest items with
large items whenever possible. To do this, we postpone the coloring of medium
items (i.e., the decision which items to pack in pairs and which to pack alone)
where possible, and later select the smallest ones to be reserved for combining
with large items. Additionally, in case such large items arrive early, we pack
medium items with them whenever possible. This is a highly unusual idea in the
context of Harmonic-like algorithms, which initially seems to preclude analysis
(technically, the ratio of items reserved for combining with large items is no
longer a fixed constant). We give a lower bound of 1.5766 for any interval
classification algorithm, including ones that pack medium and large items like
our algorithm. This shows that fundamentally different ideas will be required
to make further improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00898</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00898</id><created>2015-11-03</created><updated>2016-01-14</updated><authors><author><keyname>Sir&#xe9;n</keyname><forenames>Jouni</forenames></author></authors><title>Burrows-Wheeler transform for terabases</title><categories>cs.DS</categories><comments>This is the full version of the paper that was accepted to DCC 2016.
  The implementation is available at https://github.com/jltsiren/bwt-merge</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to avoid the reference bias introduced by mapping reads to a
reference genome, bioinformaticians are investigating reference-free methods
for analyzing sequenced genomes. With large projects sequencing thousands of
individuals, this raises the need for tools capable of handling terabases of
sequence data. A key method is the Burrows-Wheeler transform (BWT), which is
widely used for compressing and indexing reads. We propose a practical
algorithm for building the BWT of a large read collection by merging the BWTs
of subcollections. With our 2.4 Tbp datasets, the algorithm can merge 600
Gbp/day on a single system, using 30 gigabytes of memory overhead on top of the
run-length encoded BWTs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00900</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00900</id><created>2015-11-03</created><authors><author><keyname>Brandt</keyname><forenames>Sebastian</forenames></author><author><keyname>Fischer</keyname><forenames>Orr</forenames></author><author><keyname>Hirvonen</keyname><forenames>Juho</forenames></author><author><keyname>Keller</keyname><forenames>Barbara</forenames></author><author><keyname>Lempi&#xe4;inen</keyname><forenames>Tuomo</forenames></author><author><keyname>Rybicki</keyname><forenames>Joel</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author><author><keyname>Uitto</keyname><forenames>Jara</forenames></author></authors><title>A Lower Bound for the Distributed Lov\'asz Local Lemma</title><categories>cs.DC cs.CC</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that any randomised Monte Carlo distributed algorithm for the
Lov\'asz local lemma requires $\Omega(\log \log n)$ communication rounds,
assuming that it finds a correct assignment with high probability. Our result
holds even in the special case of $d = O(1)$, where $d$ is the maximum degree
of the dependency graph. By prior work, there are distributed algorithms for
the Lov\'asz local lemma with a running time of $O(\log n)$ rounds in
bounded-degree graphs, and the best lower bound before our work was
$\Omega(\log^* n)$ rounds [Chung et al. 2014].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00905</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00905</id><created>2015-11-03</created><authors><author><keyname>Shrestha</keyname><forenames>Babins</forenames></author><author><keyname>Saxena</keyname><forenames>Nitesh</forenames></author><author><keyname>Truong</keyname><forenames>Hien Thi Thu</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author></authors><title>Contextual Proximity Detection in the Face of Context-Manipulating
  Adversaries</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contextual proximity detection (or, co-presence detection) is a promising
approach to defend against relay attacks in many mobile authentication systems.
We present a systematic assessment of co-presence detection in the presence of
a context-manipulating attacker. First, we show that it is feasible to
manipulate, consistently control and stabilize the readings of different
acoustic and physical environment sensors (and even multiple sensors
simultaneously) using low-cost, off-the-shelf equipment. Second, based on these
capabilities, we show that an attacker who can manipulate the context gains a
significant advantage in defeating context-based co-presence detection. For
systems that use multiple sensors, we investigate two sensor fusion approaches
based on machine learning techniques: features-fusion and decisions-fusion, and
show that both are vulnerable to contextual attacks but the latter approach can
be more resistant in some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00906</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00906</id><created>2015-11-03</created><authors><author><keyname>Ochab</keyname><forenames>Jeremi K.</forenames></author></authors><title>Reinventing the Triangles: Rule of Thumb for Assessing Detectability</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>6 pages, 4 figures. Accepted to IEEE Computer Society. Presented at
  The 4th International Workshop on Complex Networks and their Applications,
  November 23-27, 2015 Bangkok, Thailand</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical significance of network clustering has been an unresolved problem
since it was observed that community detection algorithms produce false
positives even in random graphs. After a phase transition between undetectable
and detectable cluster structures was discovered, the connection between
spectra of adjacency matrices and detectability limits were shown, and both
were calculated for a wide range of networks with arbitrary degree
distributions and community structure. In practice the full eigenspectrum is
not known, and whether a given network has any communities within detectability
regime cannot be easily established. Based on the global clustering coefficient
we construct a criterion telling whether in an undirected, unweighted network
there is some/no detectable community structure, or if the network is in a
transient regime. The method is simple and faster than methods involving
bootstrapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00909</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00909</id><created>2015-11-03</created><authors><author><keyname>Sakharov</keyname><forenames>Alexander</forenames></author><author><keyname>Sakharov</keyname><forenames>Timothy</forenames></author></authors><title>Data Language Specification via Terminal Attribution</title><categories>cs.FL</categories><comments>This is a detailed description of tier grammar properties including
  proofs. Tier grammars are defined in paper 'Data Parsing Using Tier Grammars'
  by the same authors, appearing in the Proceedings of the 7th International
  Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge
  Management, Lisbon, Portugal (2015), 463-468</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unstructured data have to be parsed in order to become usable. The complexity
of grammar notations and the difficulty of grammar debugging limit the use of
parsers for data preprocessing. We introduce a notation in which grammars are
defined by simply dividing terminals into predefined classes and then splitting
elements of some classes into multiple layered sub-groups. These LL(1) grammars
are designed for data languages. They simplify the task of developing data
parsers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00915</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00915</id><created>2015-11-03</created><authors><author><keyname>Wielemaker</keyname><forenames>Jan</forenames></author><author><keyname>Lager</keyname><forenames>Torbj&#xf6;rn</forenames></author><author><keyname>Riguzzi</keyname><forenames>Fabrizio</forenames></author></authors><title>SWISH: SWI-Prolog for Sharing</title><categories>cs.PL cs.AI</categories><comments>International Workshop on User-Oriented Logic Programming (IULP
  2015), co-located with the 31st International Conference on Logic Programming
  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic
  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,
  pages 99-113, August 2015</comments><proxy>Stefan Ellmauthaler</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, we see a new type of interfaces for programmers based on web
technology. For example, JSFiddle, IPython Notebook and R-studio. Web
technology enables cloud-based solutions, embedding in tutorial web pages,
atractive rendering of results, web-scale cooperative development, etc. This
article describes SWISH, a web front-end for Prolog. A public website exposes
SWI-Prolog using SWISH, which is used to run small Prolog programs for
demonstration, experimentation and education. We connected SWISH to the
ClioPatria semantic web toolkit, where it allows for collaborative development
of programs and queries related to a dataset as well as performing maintenance
tasks on the running server and we embedded SWISH in the Learn Prolog Now!
online Prolog book.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00916</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00916</id><created>2015-11-03</created><authors><author><keyname>Vennekens</keyname><forenames>Joost</forenames></author></authors><title>Lowering the learning curve for declarative programming: a Python API
  for the IDP system</title><categories>cs.PL cs.AI</categories><comments>International Workshop on User-Oriented Logic Programming (IULP
  2015), co-located with the 31st International Conference on Logic Programming
  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic
  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,
  pages 83-98, August 2015</comments><proxy>Stefan Ellmauthaler</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programmers may be hesitant to use declarative systems, because of the
associated learning curve. In this paper, we present an API that integrates the
IDP Knowledge Base system into the Python programming language. IDP is a
state-of-the-art logical system, which uses SAT, SMT, Logic Programming and
Answer Set Programming technology. Python is currently one of the most widely
used (teaching) languages for programming. The first goal of our API is to
allow a Python programmer to use the declarative power of IDP, without needing
to learn any new syntax or semantics. The second goal is allow IDP to be added
to/removed from an existing code base with minimal changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00920</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00920</id><created>2015-11-03</created><authors><author><keyname>Dasseville</keyname><forenames>Ingmar</forenames></author><author><keyname>Janssens</keyname><forenames>Gerda</forenames></author></authors><title>A web-based IDE for IDP</title><categories>cs.PL cs.AI</categories><comments>International Workshop on User-Oriented Logic Programming (IULP
  2015), co-located with the 31st International Conference on Logic Programming
  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic
  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,
  pages 21-32, August 2015</comments><proxy>Stefan Ellmauthaler</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IDP is a knowledge base system based on first order logic. It is finding its
way to a larger public but is still facing practical challenges. Adoption of
new languages requires a newcomer-friendly way for users to interact with it.
Both an online presence to try to convince potential users to download the
system and offline availability to develop larger applications are essential.
We developed an IDE which can serve both purposes through the use of web
technology. It enables us to provide the user with a modern IDE with relatively
little effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00924</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00924</id><created>2015-11-03</created><authors><author><keyname>Gaggl</keyname><forenames>Sarah Alice</forenames></author><author><keyname>Rudolph</keyname><forenames>Sebastian</forenames></author><author><keyname>Schweizer</keyname><forenames>Lukas</forenames></author></authors><title>Bound Your Models! How to Make OWL an ASP Modeling Language</title><categories>cs.PL cs.AI</categories><comments>International Workshop on User-Oriented Logic Programming (IULP
  2015), co-located with the 31st International Conference on Logic Programming
  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic
  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,
  pages 33-49, August 2015</comments><proxy>Stefan Ellmauthaler</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To exploit the Web Ontology Language OWL as an answer set programming (ASP)
language, we introduce the notion of bounded model semantics, as an intuitive
and computationally advantageous alternative to its classical semantics. We
show that a translation into ASP allows for solving a wide range of
bounded-model reasoning tasks, including satisfiability and axiom entailment
but also novel ones such as model extraction and enumeration. Ultimately, our
work facilitates harnessing advanced semantic web modeling environments for the
logic programming community through an &quot;off-label use&quot; of OWL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00925</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00925</id><created>2015-11-03</created><updated>2015-12-02</updated><authors><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Morgenstern</keyname><forenames>Jamie</forenames></author><author><keyname>Rogers</keyname><forenames>Ryan</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Vohra</keyname><forenames>Rakesh</forenames></author></authors><title>Do Prices Coordinate Markets?</title><categories>cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Walrasian equilibrium prices can be said to coordinate markets: They support
a welfare optimal allocation in which each buyer is buying bundle of goods that
is individually most preferred. However, this clean story has two caveats.
First, the prices alone are not sufficient to coordinate the market, and buyers
may need to select among their most preferred bundles in a coordinated way to
find a feasible allocation. Second, we don't in practice expect to encounter
exact equilibrium prices tailored to the market, but instead only approximate
prices, somehow encoding &quot;distributional&quot; information about the market. How
well do prices work to coordinate markets when tie-breaking is not coordinated,
and they encode only distributional information?
  We answer this question. First, we provide a genericity condition such that
for buyers with Matroid Based Valuations, overdemand with respect to
equilibrium prices is at most 1, independent of the supply of goods, even when
tie-breaking is done in an uncoordinated fashion. Second, we provide
learning-theoretic results that show that such prices are robust to changing
the buyers in the market, so long as all buyers are sampled from the same
(unknown) distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00928</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00928</id><created>2015-11-03</created><authors><author><keyname>Lapauw</keyname><forenames>Ruben</forenames></author><author><keyname>Dasseville</keyname><forenames>Ingmar</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Visualising interactive inferences with IDPD3</title><categories>cs.PL cs.AI</categories><comments>International Workshop on User-Oriented Logic Programming (IULP
  2015), co-located with the 31st International Conference on Logic Programming
  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic
  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,
  pages 67-81, August 2015</comments><proxy>Stefan Ellmauthaler</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large part of the use of knowledge base systems is the interpretation of
the output by the end-users and the interaction with these users. Even during
the development process visualisations can be a great help to the developer. We
created IDPD3 as a library to visualise models of logic theories. IDPD3 is a
new version of $ID^{P}_{Draw}$ and adds support for visualised interactive
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00938</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00938</id><created>2015-11-03</created><updated>2015-12-20</updated><authors><author><keyname>Francis</keyname><forenames>Nadime</forenames><affiliation>ENS Cachan and INRIA</affiliation></author><author><keyname>Segoufin</keyname><forenames>Luc</forenames><affiliation>INRIA &amp; ENS Cachan</affiliation></author><author><keyname>Sirangelo</keyname><forenames>Cristina</forenames><affiliation>ENS Cachan, CNRS and INRIA</affiliation></author></authors><title>Datalog Rewritings of Regular Path Queries using Views</title><categories>cs.DB</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:14) 2015</journal-ref><doi>10.2168/LMCS-11(4:14)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider query answering using views on graph databases, i.e. databases
structured as edge-labeled graphs. We mainly consider views and queries
specified by Regular Path Queries (RPQ). These are queries selecting pairs of
nodes in a graph database that are connected via a path whose sequence of edge
labels belongs to some regular language. We say that a view V determines a
query Q if for all graph databases D, the view image V(D) always contains
enough information to answer Q on D. In other words, there is a well defined
function from V(D) to Q(D). Our main result shows that when this function is
monotone, there exists a rewriting of Q as a Datalog query over the view
instance V(D). In particular the rewriting query can be evaluated in time
polynomial in the size of V(D). Moreover this implies that it is decidable
whether an RPQ query can be rewritten in Datalog using RPQ views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00971</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00971</id><created>2015-11-03</created><authors><author><keyname>Marr&#xf3;n</keyname><forenames>Diego</forenames><affiliation>dmarron@ac.upc.edu</affiliation></author><author><keyname>Read</keyname><forenames>Jesse</forenames><affiliation>jesse.read@aalto.fi</affiliation></author><author><keyname>Bifet</keyname><forenames>Albert</forenames><affiliation>albert.bifet@telecom-paristech.fr</affiliation></author><author><keyname>Navarro</keyname><forenames>Nacho</forenames><affiliation>nacho@ac.upc.edu</affiliation></author></authors><title>Data Stream Classification using Random Feature Functions and Novel
  Method Combinations</title><categories>cs.LG cs.NE</categories><comments>20 pages, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big Data streams are being generated in a faster, bigger, and more
commonplace. In this scenario, Hoeffding Trees are an established method for
classification. Several extensions exist, including high-performing ensemble
setups such as online and leveraging bagging. Also, $k$-nearest neighbors is a
popular choice, with most extensions dealing with the inherent performance
limitations over a potentially-infinite stream.
  At the same time, gradient descent methods are becoming increasingly popular,
owing in part to the successes of deep learning. Although deep neural networks
can learn incrementally, they have so far proved too sensitive to
hyper-parameter options and initial conditions to be considered an effective
`off-the-shelf' data-streams solution.
  In this work, we look at combinations of Hoeffding-trees, nearest neighbour,
and gradient descent methods with a streaming preprocessing approach in the
form of a random feature functions filter for additional predictive power.
  We further extend the investigation to implementing methods on GPUs, which we
test on some large real-world datasets, and show the benefits of using GPUs for
data-stream learning due to their high scalability.
  Our empirical evaluation yields positive results for the novel approaches
that we experiment with, highlighting important issues, and shed light on
promising future directions in approaches to data-stream classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.00984</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.00984</id><created>2015-11-03</created><authors><author><keyname>Huq</keyname><forenames>Arefin</forenames></author></authors><title>Undirected Cat-and-Mouse is P-complete</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cat-and-mouse is a two-player game on a finite graph. Chandra and Stockmeyer
showed cat-and-mouse is P-complete on directed graphs. We show cat-and-mouse is
P-complete on undirected graphs. To our knowledge, no proof of the directed
case was ever published. To fill this gap we give a proof for directed graphs
and extend it to undirected graphs. The proof is a reduction from a variant of
the circuit value problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01016</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01016</id><created>2015-11-03</created><authors><author><keyname>Monge-N&#xe1;jera</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Corrales</keyname><forenames>Karla Vega</forenames></author></authors><title>Sexual videos in Internet: a test of 11 hypotheses about intimate
  practices and gender interactions in Latin America</title><categories>cs.CY</categories><comments>6 pages, 1 figure</comments><journal-ref>Cuadernos de Investigaci\'on UNED (ISSN: 1659-4266) Vol. 5(2),
  Diciembre, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a marked lack of literature on user-submitted sexual videos from
Latin America. To start filling that gap, we present a formal statistical
testing of several hypotheses about the characteristics of 214 videos from
Nereliatube.com posted from the inauguration of the site until December 2010.
We found that in most cases the video was made consensually and the camera was
operated by the man. The most frequent practice shown was fellatio, followed by
vaginal penetration. The great majority of videos showed the sexual
interactions of one woman with one man; group sex was rare. Violence and
manifestations of power were rare and when there was violence it was mostly
simulated. Latin American user-submitted sexual videos in Nereliatube generally
reflect a society in which women and men have a variety of sexual practices
that are mostly consensual and that do not differ from the biologically and
anthropologically expected patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01017</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01017</id><created>2015-11-03</created><updated>2015-11-04</updated><authors><author><keyname>Mousavi</keyname><forenames>Ali</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Consistent Parameter Estimation for LASSO and Approximate Message
  Passing</title><categories>math.ST cs.IT math.IT math.OC stat.ML stat.TH</categories><comments>arXiv admin note: text overlap with arXiv:1309.5979</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a vector $\beta_o \in \mathbb{R}^p$
from $n$ random and noisy linear observations $y= X\beta_o + w$, where $X$ is
the measurement matrix and $w$ is noise. The LASSO estimate is given by the
solution to the optimization problem $\hat{\beta}_{\lambda} = \arg \min_{\beta}
\frac{1}{2} \|y-X\beta\|_2^2 + \lambda \| \beta \|_1$. Among the iterative
algorithms that have been proposed for solving this optimization problem,
approximate message passing (AMP) has attracted attention for its fast
convergence. Despite significant progress in the theoretical analysis of the
estimates of LASSO and AMP, little is known about their behavior as a function
of the regularization parameter $\lambda$, or the thereshold parameters
$\tau^t$. For instance the following basic questions have not yet been studied
in the literature: (i) How does the size of the active set
$\|\hat{\beta}^\lambda\|_0/p$ behave as a function of $\lambda$? (ii) How does
the mean square error $\|\hat{\beta}_{\lambda} - \beta_o\|_2^2/p$ behave as a
function of $\lambda$? (iii) How does $\|\beta^t - \beta_o \|_2^2/p$ behave as
a function of $\tau^1, \ldots, \tau^{t-1}$? Answering these questions will help
in addressing practical challenges regarding the optimal tuning of $\lambda$ or
$\tau^1, \tau^2, \ldots$. This paper answers these questions in the asymptotic
setting and shows how these results can be employed in deriving simple and
theoretically optimal approaches for tuning the parameters $\tau^1, \ldots,
\tau^t$ for AMP or $\lambda$ for LASSO. It also explores the connection between
the optimal tuning of the parameters of AMP and the optimal tuning of LASSO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01025</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01025</id><created>2015-11-03</created><authors><author><keyname>Le</keyname><forenames>Van Bang</forenames></author><author><keyname>Pfender</keyname><forenames>Florian</forenames></author></authors><title>Color-line and Proper Color-line Graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by investigations of rainbow matchings in edge colored graphs, we
introduce the notion of color-line graphs that generalizes the classical
concept of line graphs in a natural way. Let $H$ be a (properly) edge-colored
graph. The (proper) color-line graph $C\!L(H)$ of $H$ has edges of $H$ as
vertices, and two edges of $H$ are adjacent in $C\!L(H)$ if they are incident
in $H$ or have the same color. We give Krausz-type characterizations for
(proper) color-line graphs, and point out that, for any fixed $k\ge 2$,
recognizing if a graph is the color-line graph of some graph $H$ in which the
edges are colored with at most $k$ colors is NP-complete. In contrast, we show
that, for any fixed $k$, recognizing color-line graphs of properly edge colored
graphs $H$ with at most $k$ colors is polynomially. Moreover, we give a good
characterization for proper $2$-color line graphs that yields a linear time
recognition algorithm in this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01029</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01029</id><created>2015-11-03</created><authors><author><keyname>Badrinarayanan</keyname><forenames>Vijay</forenames></author><author><keyname>Mishra</keyname><forenames>Bamdev</forenames></author><author><keyname>Cipolla</keyname><forenames>Roberto</forenames></author></authors><title>Understanding symmetries in deep networks</title><categories>cs.LG cs.AI cs.CV</categories><comments>Accepted at the 8th NIPS Workshop on Optimization for Machine
  Learning (OPT2015) to be held at Montreal, Canada on December 11, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works have highlighted scale invariance or symmetry present in the
weight space of a typical deep network and the adverse effect it has on the
Euclidean gradient based stochastic gradient descent optimization. In this
work, we show that a commonly used deep network, which uses convolution, batch
normalization, reLU, max-pooling, and sub-sampling pipeline, possess more
complex forms of symmetry arising from scaling-based reparameterization of the
network weights. We propose to tackle the issue of the weight space symmetry by
constraining the filters to lie on the unit-norm manifold. Consequently,
training the network boils down to using stochastic gradient descent updates on
the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows
that the proposed updates improve the test performance beyond what is achieved
with batch normalization and without sacrificing the computational efficiency
of the weight updates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01032</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01032</id><created>2015-11-03</created><updated>2016-02-19</updated><authors><author><keyname>Figueiredo</keyname><forenames>Flavio</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Almeida</keyname><forenames>Jussara</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>TribeFlow: Mining &amp; Predicting User Trajectories</title><categories>cs.SI physics.data-an physics.soc-ph stat.ML</categories><comments>To Appear at WWW 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Which song will Smith listen to next? Which restaurant will Alice go to
tomorrow? Which product will John click next? These applications have in common
the prediction of user trajectories that are in a constant state of flux over a
hidden network (e.g. website links, geographic location). What users are doing
now may be unrelated to what they will be doing in an hour from now. Mindful of
these challenges we propose TribeFlow, a method designed to cope with the
complex challenges of learning personalized predictive models of
non-stationary, transient, and time-heterogeneous user trajectories. TribeFlow
is a general method that can perform next product recommendation, next song
recommendation, next location prediction, and general arbitrary-length user
trajectory prediction without domain-specific knowledge. TribeFlow is more
accurate and up to 413x faster than top competitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01038</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01038</id><created>2015-11-03</created><authors><author><keyname>Blelloch</keyname><forenames>Guy E.</forenames></author><author><keyname>Fineman</keyname><forenames>Jeremy T.</forenames></author><author><keyname>Gibbons</keyname><forenames>Phillip B.</forenames></author><author><keyname>Gu</keyname><forenames>Yan</forenames></author><author><keyname>Shun</keyname><forenames>Julian</forenames></author></authors><title>Efficient Algorithms under Asymmetric Read and Write Costs</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In several emerging technologies for computer memory (main memory) the cost
of reading is significantly cheaper than the cost of writing. Such asymmetry in
memory costs poses a fundamentally different model from the RAM for algorithm
design. In this paper we study lower and upper bounds for various problems
under such asymmetric read and write costs. We consider both the case in which
all but $O(1)$ memory has asymmetric cost, and the case of a small cache of
symmetric memory. We model both cases using the ARAM, in which there is a small
(symmetric) memory of size $M$, a large unbounded (asymmetric) memory, both
random access, and the cost of reading from the large memory is unit, but the
cost of writing is $w \geq 1$.
  For FFT and sorting networks we show a lower bound cost of $\Omega(w(n
\log_{w M} n))$. For the FFT we show a matching upper bound, which indicates it
is only possible to achieve asymptotic improvements with cheaper reads when $w
&gt; M$. For sorting networks we show an asymptotic gap between the cost of
sorting networks and comparison sorting in the model. We also show a lower
bound for computations on an $n \times n$ diamond DAG of $\Omega(w n^2/M)$
cost, which indicates no asymptotic improvement is achievable with fast reads.
However, we show that for the minimum edit distance problem (and related
problems), which would seem to be a diamond DAG, we can improve on this lower
bound getting $O(w n^2/ (M \min(w^{1/3},M^{1/2})))$ cost. To achieve this we
make use of a &quot;path sketch&quot; technique that is forbidden in a strict DAG
computation. Finally we show several interesting upper bounds for shortest path
problems, minimum-spanning trees, and other problems. A common theme in many of
the upper bounds is that they require redundant computation and a tradeoff
between reads and writes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01042</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01042</id><created>2015-11-03</created><updated>2015-11-15</updated><authors><author><keyname>Chung</keyname><forenames>Junyoung</forenames></author><author><keyname>Devlin</keyname><forenames>Jacob</forenames></author><author><keyname>Awadalla</keyname><forenames>Hany Hassan</forenames></author></authors><title>Detecting Interrogative Utterances with Recurrent Neural Networks</title><categories>cs.CL cs.LG cs.NE</categories><comments>6 pages, accepted to NIPS 2015 Workshop on Machine Learning for
  Spoken Language Understanding and Interaction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore different neural network architectures that can
predict if a speaker of a given utterance is asking a question or making a
statement. We com- pare the outcomes of regularization methods that are
popularly used to train deep neural networks and study how different context
functions can affect the classification performance. We also compare the
efficacy of gated activation functions that are favorably used in recurrent
neural networks and study how to combine multimodal inputs. We evaluate our
models on two multimodal datasets: MSR-Skype and CALLHOME.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01047</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01047</id><created>2015-06-10</created><authors><author><keyname>Qiu</keyname><forenames>Zhicong</forenames></author><author><keyname>Miller</keyname><forenames>David J.</forenames></author><author><keyname>Kesidis</keyname><forenames>George</forenames></author></authors><title>Detecting Clusters of Anomalies on Low-Dimensional Feature Subsets with
  Application to Network Traffic Flow Data</title><categories>cs.NI cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a variety of applications, one desires to detect groups of anomalous data
samples, with a group potentially manifesting its atypicality (relative to a
reference model) on a low-dimensional subset of the full measured set of
features. Samples may only be weakly atypical individually, whereas they may be
strongly atypical when considered jointly. What makes this group anomaly
detection problem quite challenging is that it is a priori unknown which subset
of features jointly manifests a particular group of anomalies. Moreover, it is
unknown how many anomalous groups are present in a given data batch. In this
work, we develop a group anomaly detection (GAD) scheme to identify the subset
of samples and subset of features that jointly specify an anomalous cluster. We
apply our approach to network intrusion detection to detect BotNet and
peer-to-peer flow clusters. Unlike previous studies, our approach captures and
exploits statistical dependencies that may exist between the measured features.
Experiments on real world network traffic data demonstrate the advantage of our
proposed system, and highlight the importance of exploiting feature dependency
structure, compared to the feature (or test) independence assumption made in
previous studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01050</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01050</id><created>2015-11-03</created><authors><author><keyname>Arbabjolfaei</keyname><forenames>Fatemeh</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>Three Stories on a Two-sided Coin: Index Coding, Locally Recoverable
  Distributed Storage, and Guessing Games on Graphs</title><categories>cs.IT math.IT</categories><comments>8 pages, 53rd Annual Allerton Conference on Communication, Control,
  and Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three science and engineering problems of recent interests -index coding,
locally recoverable distributed storage, and guessing games on graphs- are
discussed and the connection between their optimal solutions is elucidated. By
generalizing recent results by Shanmugam and Dimakis and by Mazumdar on the
complementarity between the optimal broadcast rate of an index coding problem
on a directed graph and the normalized rate of a locally recoverable
distributed storage problem on the same graph, it is shown that the capacity
region and the optimal rate region of these two problems are complementary. The
main ingredients in establishing this result are the notion of confusion graph
introduced by Alon et al. (2008), the vertex transitivity of a confusion graph,
the characterization of the index coding capacity region via the fractional
chromatic number of confusion graphs, and the characterization of the optimal
rate region of the locally recoverable distributed storage via the independence
number of confusion graphs. As the third and final facet of the
complementarity, guessing games on graphs by Riis are discussed as special
cases of the locally recoverable distributed storage problem, and it is shown
that the winning probability of the optimal strategy for a guessing game and
the ratio between the winning probabilities of the optimal strategy and a
random guess can be characterized, respectively, by the capacity region for
index coding and the optimal rate region for distributed storage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01061</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01061</id><created>2015-11-02</created><authors><author><keyname>Esteban</keyname><forenames>Juan Luis</forenames></author><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>A correction on Shiloach's algorithm for minimum linear arrangement of
  trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More than 30 years ago, Shiloach published an algorithm to solve the minimum
linear arrangement problem for undirected trees. Here we fix a small error in
the original version of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01064</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01064</id><created>2015-10-31</created><updated>2015-12-11</updated><authors><author><keyname>Karargyris</keyname><forenames>Alexandros</forenames></author></authors><title>Color Space Transformation Network</title><categories>cs.CV</categories><comments>Report</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Deep networks have become very popular over the past few years. The main
reason for this widespread use is their excellent ability to learn and predict
knowledge in a very easy and efficient way. Convolutional neural networks and
auto-encoders have become the normal in the area of imaging and computer vision
achieving unprecedented accuracy levels in many applications. The most common
strategy is to build and train networks with many layers by tuning their
hyper-parameters. While this approach has proven to be a successful way to
build robust deep learning schemes it suffers from high complexity. In this
paper we introduce a module that learns color space transformations within a
network. Given a large dataset of colored images the color space transformation
module tries to learn color space transformations that increase overall
classification accuracy. This module has shown to increase overall accuracy for
the same network design and to achieve faster convergence. It is part of a
broader family of image transformations (e.g. spatial transformer network).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01065</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01065</id><created>2015-11-01</created><authors><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Glass</keyname><forenames>Arnold</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Towards a Consumer-Centric Grid: A Behavioral Perspective</title><categories>cs.SY cs.IT math.IT</categories><comments>to appear, Proceedings of the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active consumer participation is seen as an integral part of the emerging
smart grid. Examples include demand-side management programs, incorporation of
consumer-owned energy storage or renewable energy units, and active energy
trading. However, despite the foreseen technological benefits of such
consumer-centric grid features, to date, their widespread adoption in practice
remains modest. To shed light on this challenge, this paper explores the
potential of prospect theory, a Nobel-prize winning theory, as a
decision-making framework that can help understand how risk and uncertainty can
impact the decisions of smart grid consumers. After introducing the basic
notions of prospect theory, several examples drawn from a number of smart grid
applications are developed. These results show that a better understanding of
the role of human decision-making within the smart grid is paramount for
optimizing its operation and expediting the deployment of its various
technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01080</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01080</id><created>2015-11-03</created><authors><author><keyname>Collavizza</keyname><forenames>H&#xe9;l&#xe8;ne</forenames></author><author><keyname>Michel</keyname><forenames>Claude</forenames></author><author><keyname>Rueher</keyname><forenames>Michel</forenames></author></authors><title>Searching input values hitting suspicious Intervals in programs with
  floating-point operations</title><categories>cs.PL cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programs with floating-point computations are often derived from mathematical
models or designed with the semantics of the real numbers in mind. However, for
a given input, the computed path with floating-point numbers may differ from
the path corresponding to the same computation with real numbers. A common
practice when validating such programs consists in estimating the accuracy of
floating-point computations with respect to the same sequence of operations in
an ide-alized semantics of real numbers. However, state-of-the-art tools
compute an over-approximation of the error introduced by floating-point
operations. As a consequence, totally inappropriate behaviors of a program may
be dreaded but the developer does not know whether these behaviors will
actually occur, or not. In this paper, we introduce a new constraint-based
approach that searches for test cases in the part of the over-approximation
where errors due to floating-point arithmetic would lead to inappropriate
behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01088</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01088</id><created>2015-11-03</created><authors><author><keyname>Merelo</keyname><forenames>Juan-J.</forenames></author><author><keyname>Garc&#xed;a-S&#xe1;nchez</keyname><forenames>Pablo</forenames></author><author><keyname>Garc&#xed;a-Valdez</keyname><forenames>Mario</forenames></author><author><keyname>Blancas</keyname><forenames>Israel</forenames></author></authors><title>There is no fast lunch: an examination of the running speed of
  evolutionary algorithms in several languages</title><categories>cs.NE cs.PF</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  It is quite usual when an evolutionary algorithm tool or library uses a
language other than C, C++, Java or Matlab that a reviewer or the audience
questions its usefulness based on the speed of those other languages,
purportedly slower than the aforementioned ones. Despite speed being not
everything needed to design a useful evolutionary algorithm application, in
this paper we will measure the speed for several very basic evolutionary
algorithm operations in several languages which use different virtual machines
and approaches, and prove that, in fact, there is no big difference in speed
between interpreted and compiled languages, and that in some cases, interpreted
languages such as JavaScript or Python can be faster than compiled languages
such as Scala, making them worthy of use for evolutionary algorithm
experimentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01111</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01111</id><created>2015-11-03</created><updated>2016-03-01</updated><authors><author><keyname>Blasiok</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Chestnut</keyname><forenames>Stephen R.</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Yang</keyname><forenames>Lin F.</forenames></author></authors><title>Streaming Symmetric Norms via Measure Concentration</title><categories>cs.DS</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the streaming space complexity of every symmetric norm l (a
norm on R^n invariant under sign-flips and coordinate-permutations), by
relating this space complexity to the measure-concentration characteristics of
l. Specifically, we provide matching upper and lower bounds (up to polylog n
factors) on the space complexity of approximating the norm of the stream, where
both bounds depend on the median of l(x). when x is drawn uniformly from the l2
unit sphere. The same quantity governs many phenomena in high-dimensional
spaces, such as large-deviation bounds and the critical dimension in
Dvoretzky's Theorem.
  The family of symmetric norms contains several well-studied norms, such as
all l_p norms, and indeed we provide a new explanation for the disparity in
space complexity between p &lt;= 2 and p&gt;2. In addition, we apply our general
results to easily derive bounds for several norms were not studied before in
the streaming model, including for example the top-k norm and the k-support
norm, which was recently shown to be effective for machine learning tasks.
  Overall, these results make progress on two outstanding problems in the area
of sublinear algorithms (Problems 5 and 30 in http://sublinear.info).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01123</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01123</id><created>2015-11-03</created><authors><author><keyname>Jaroschek</keyname><forenames>Maximilian</forenames></author><author><keyname>Dobal</keyname><forenames>Pablo Federico</forenames></author><author><keyname>Fontaine</keyname><forenames>Pascal</forenames></author></authors><title>Adapting Real Quantifier Elimination Methods for Conflict Set
  Computation</title><categories>cs.LO</categories><journal-ref>Frontiers of Combining Systems, 151--166, isbn 978-3-319-24245-3,
  2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The satisfiability problem in real closed fields is decidable. In the context
of satisfiability modulo theories, the problem restricted to conjunctive sets
of literals, that is, sets of polynomial constraints, is of particular
importance. One of the central problems is the computation of good explanations
of the unsatisfiability of such sets, i.e.\ obtaining a small subset of the
input constraints whose conjunction is already unsatisfiable. We adapt two
commonly used real quantifier elimination methods, cylindrical algebraic
decomposition and virtual substitution, to provide such conflict sets and
demonstrate the performance of our method in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01128</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01128</id><created>2015-11-03</created><authors><author><keyname>Jaroschek</keyname><forenames>Maximilian</forenames></author></authors><title>Improved Polynomial Remainder Sequences for Ore Polynomials</title><categories>cs.SC</categories><journal-ref>Journal of Symbolic Computation (50), 64 - 76, 2013</journal-ref><doi>10.1016/j.jsc.2013.05.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial remainder sequences contain the intermediate results of the
Euclidean algorithm when applied to (non-)commutative polynomials. The running
time of the algorithm is dependent on the size of the coefficients of the
remainders. Different ways have been studied to make these as small as
possible. The subresultant sequence of two polynomials is a polynomial
remainder sequence in which the size of the coefficients is optimal in the
generic case, but when taking the input from applications, the coefficients are
often larger than necessary. We generalize two improvements of the subresultant
sequence to Ore polynomials and derive a new bound for the minimal coefficient
size. Our approach also yields a new proof for the results in the commutative
case, providing a new point of view on the origin of the extraneous factors of
the coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01132</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01132</id><created>2015-11-03</created><authors><author><keyname>Azar</keyname><forenames>Yossi</forenames></author><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Roytman</keyname><forenames>Alan</forenames></author></authors><title>Liquid Price of Anarchy</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating budget constraints into the analysis of auctions has become
increasingly important, as they model practical settings more accurately. The
social welfare function, which is the standard measure of efficiency in
auctions, is inadequate for settings with budgets, since there may be a large
disconnect between the value a bidder derives from obtaining an item and what
can be liquidated from her. The Liquid Welfare objective function has been
suggested as a natural alternative for settings with budgets. Simple auctions,
like simultaneous item auctions, are evaluated by their performance at
equilibrium using the Price of Anarchy (PoA) measure -- the ratio of the
objective function value of the optimal outcome to the worst equilibrium.
Accordingly, we evaluate the performance of simultaneous item auctions in
budgeted settings by the Liquid Price of Anarchy (LPoA) measure -- the ratio of
the optimal Liquid Welfare to the Liquid Welfare obtained in the worst
equilibrium.
  Our main result is that the LPoA for mixed Nash equilibria is bounded by a
constant when bidders are additive and items can be divided into sufficiently
many discrete parts. Our proofs are robust, and can be extended to achieve
similar bounds for simultaneous second price auctions as well as Bayesian Nash
equilibria. For pure Nash equilibria, we establish tight bounds on the LPoA for
the larger class of fractionally-subadditive valuations. To derive our results,
we develop a new technique in which some bidders deviate (surprisingly) toward
a non-optimal solution. In particular, this technique does not fit into the
smoothness framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01137</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01137</id><created>2015-11-03</created><authors><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author><author><keyname>Williams</keyname><forenames>Virginia Vassilevska</forenames></author><author><keyname>V&#xe9;gh</keyname><forenames>L&#xe1;szl&#xf3; A.</forenames></author></authors><title>A 7/3-Approximation for Feedback Vertex Sets in Tournaments</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the minimum-weight feedback vertex set problem in tournaments:
given a tournament with non-negative vertex weights, remove a minimum-weight
set of vertices that intersects all cycles. This problem is $\mathsf{NP}$-hard
to solve exactly, and Unique Games-hard to approximate by a factor better than
2. We present the first $7/3$ approximation algorithm for this problem,
improving on the previously best known ratio $5/2$ given by Cai et al. [FOCS
1998, SICOMP 2001].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01138</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01138</id><created>2015-11-03</created><authors><author><keyname>Wild</keyname><forenames>Sebastian</forenames></author></authors><title>Why Is Dual-Pivot Quicksort Fast?</title><categories>cs.DS</categories><comments>extended abstract for Theorietage 2015
  (https://www.uni-trier.de/index.php?id=55089)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I discuss the new dual-pivot Quicksort that is nowadays used to sort arrays
of primitive types in Java. I sketch theoretical analyses of this algorithm
that offer a possible, and in my opinion plausible, explanation why (a)
dual-pivot Quicksort is faster than the previously used (classic) Quicksort and
(b) why this improvement was not already found much earlier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01154</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01154</id><created>2015-11-03</created><authors><author><keyname>Bogovic</keyname><forenames>John A.</forenames></author><author><keyname>Hanslovsky</keyname><forenames>Philipp</forenames></author><author><keyname>Wong</keyname><forenames>Allan</forenames></author><author><keyname>Saalfeld</keyname><forenames>Stephan</forenames></author></authors><title>Robust Registration of Calcium Images by Learned Contrast Synthesis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-modal image registration is a challenging task that is vital to fuse
complementary signals for subsequent analyses. Despite much research into cost
functions addressing this challenge, there exist cases in which these are
ineffective. In this work, we show that (1) this is true for the registration
of in-vivo Drosophila brain volumes visualizing genetically encoded calcium
indicators to an nc82 atlas and (2) that machine learning based contrast
synthesis can yield improvements. More specifically, the number of subjects for
which the registration outright failed was greatly reduced (from 40% to 15%) by
using a synthesized image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01156</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01156</id><created>2015-11-03</created><authors><author><keyname>Tschopp</keyname><forenames>Fabian</forenames></author><author><keyname>Zorzi</keyname><forenames>Marco</forenames></author></authors><title>Robust Large-Scale Localization in 3D Point Clouds Revisited</title><categories>cs.CV</categories><comments>6 pages; technical report</comments><acm-class>I.2.10; I.3.5; I.4.1; I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the problem of getting a full 6-DOF pose estimation of a query
image inside a given point cloud. This technical report re-evaluates the
algorithms proposed by Y. Li et al. &quot;Worldwide Pose Estimation using 3D Point
Cloud&quot;. Our code computes poses from 3 or 4 points, with both known and unknown
focal length. The results can easily be displayed and analyzed with Meshlab. We
found both advantages and shortcomings of the methods proposed. Furthermore,
additional priors and parameters for point selection, RANSAC and pose quality
estimate (inlier test) are proposed and applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01158</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01158</id><created>2015-11-03</created><authors><author><keyname>Feng</keyname><forenames>Minwei</forenames></author><author><keyname>Xiang</keyname><forenames>Bing</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>Distributed Deep Learning for Answer Selection</title><categories>cs.LG cs.CL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is an empirical study of the distributed deep learning for a
question answering subtask: answer selection. Comparison studies of SGD, MSGD,
DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results
show that the message passing interface based distributed framework can
accelerate the convergence speed at a sublinear scale. This paper demonstrates
the importance of distributed training: with 120 workers, an 83x speedup is
achievable and running time is decreased from 107.9 hours to 1.3 hours, which
will benefit the productivity significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01161</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01161</id><created>2015-11-03</created><authors><author><keyname>Hanslovsky</keyname><forenames>Philipp</forenames></author><author><keyname>Bogovic</keyname><forenames>John A.</forenames></author><author><keyname>Xu</keyname><forenames>C. Shan</forenames></author><author><keyname>Hayworth</keyname><forenames>Kenneth J.</forenames></author><author><keyname>Lu</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Hess</keyname><forenames>Harald F.</forenames></author><author><keyname>Saalfeld</keyname><forenames>Stephan</forenames></author></authors><title>Image based compensation for thickness variation in microscopy section
  series</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Serial block face scanning electron microscopy in combination with focused
ion beam milling (FIB-SEM) has become a popular method for nanometer-resolution
isotropic imaging of neural and other cellular tissue with a planar field of
view of up to 100um. While FIB-SEM is particularly attractive for its high
in-plane resolution, ion beam milling generates non-planar block faces and
inhomogeneous z-spacing leading to distorted volume acquisitions. We extend our
previous work on image-based z-spacing correction for serial section series to
determine a deformation field that varies within the xy-plane to account for
non-planarity. We show that our method identifies and corrects these
distortions in real world FIB-SEM acquisitions and quantitatively assess its
precision on virtual ground truth. Our method is available as an open source
implementation that is parallelized using the Spark framework enabling rapid
processing of very large volumes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01166</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01166</id><created>2015-11-03</created><authors><author><keyname>Clawson</keyname><forenames>Zachary</forenames></author><author><keyname>Ding</keyname><forenames>Xuchu</forenames></author><author><keyname>Englot</keyname><forenames>Brendan</forenames></author><author><keyname>Frewen</keyname><forenames>Thomas A.</forenames></author><author><keyname>Sisson</keyname><forenames>William M.</forenames></author><author><keyname>Vladimirsky</keyname><forenames>Alexander</forenames></author></authors><title>A bi-criteria path planning algorithm for robotics applications</title><categories>cs.RO cs.SY</categories><comments>18 pages, 11 figures; submitted for publication to The International
  J. of Robotics Research</comments><msc-class>68T40, 93C85, 68W25, 90C29, 90C39, 05C38, 05C85</msc-class><acm-class>I.2.8; I.2.9; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Realistic path planning applications often require optimizing with respect to
several criteria simultaneously. Here we introduce an efficient algorithm for
bi-criteria path planning on graphs. Our approach is based on augmenting the
state space to keep track of the &quot;budget&quot; remaining to satisfy the constraints
on secondary cost. The resulting augmented graph is acyclic and the primary
cost can be then minimized by a simple upward sweep through budget levels. The
efficiency and accuracy of our algorithm is tested on Probabilistic Roadmap
graphs to minimize the distance of travel subject to a constraint on the
overall threat exposure of the robot. We also present the results from field
experiments illustrating the use of this approach on realistic robotic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01168</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01168</id><created>2015-11-03</created><authors><author><keyname>Paciscopi</keyname><forenames>Marco</forenames></author><author><keyname>Silvestri</keyname><forenames>Ludovico</forenames></author><author><keyname>Pavone</keyname><forenames>Francesco Saverio</forenames></author><author><keyname>Frasconi</keyname><forenames>Paolo</forenames></author></authors><title>Cell identification in whole-brain multiview images of neural activation</title><categories>cs.CV</categories><acm-class>J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a scalable method for brain cell identification in multiview
confocal light sheet microscopy images. Our algorithmic pipeline includes a
hierarchical registration approach and a novel multiview version of semantic
deconvolution that simultaneously enhance visibility of fluorescent cell
bodies, equalize their contrast, and fuses adjacent views into a single 3D
images on which cell identification is performed with mean shift.
  We present empirical results on a whole-brain image of an adult Arc-dVenus
mouse acquired at 4micron resolution. Based on an annotated test volume
containing 3278 cells, our algorithm achieves an $F_1$ measure of 0.89.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01169</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01169</id><created>2015-11-03</created><updated>2016-02-23</updated><authors><author><keyname>Keskar</keyname><forenames>Nitish Shirish</forenames></author><author><keyname>Berahas</keyname><forenames>Albert S.</forenames></author></authors><title>adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNNs) are powerful models that achieve exceptional
performance on several pattern recognition problems. However, the training of
RNNs is a computationally difficult task owing to the well-known
&quot;vanishing/exploding&quot; gradient problem. Algorithms proposed for training RNNs
either exploit no (or limited) curvature information and have cheap
per-iteration complexity, or attempt to gain significant curvature information
at the cost of increased per-iteration cost. The former set includes
diagonally-scaled first-order methods such as ADAGRAD and ADAM, while the
latter consists of second-order algorithms like Hessian-Free Newton and K-FAC.
In this paper, we present adaQN, a stochastic quasi-Newton algorithm for
training RNNs. Our approach retains a low per-iteration cost while allowing for
non-diagonal scaling through a stochastic L-BFGS updating scheme. The method
uses a novel L-BFGS scaling initialization scheme and is judicious in storing
and retaining L-BFGS curvature pairs. We present numerical experiments on two
language modeling tasks and show that adaQN is competitive with popular RNN
training algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01175</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01175</id><created>2015-11-03</created><authors><author><keyname>Gao</keyname><forenames>Pu</forenames></author><author><keyname>Wormald</keyname><forenames>Nicholas</forenames></author></authors><title>Uniform generation of random regular graphs</title><categories>math.CO cs.DS math.PR</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new approach for uniform generation of combinatorial objects,
and apply it to derive a uniform sampler REG for d-regular graphs. REG can be
implemented such that each graph is generated in expected time O(nd^3),
provided that d=o(n^{1/2}). Our result significantly improves the previously
best uniform sampler, which works efficiently only when d=O(n^{1/3}), with
essentially the same running time for the same d. We also give a linear-time
approximate sampler REG*, which generates a random d-regular graph whose
distribution differs from the uniform by o(1) in total variation distance, when
d=o(n^{1/2}).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01186</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01186</id><created>2015-11-03</created><authors><author><keyname>Yang</keyname><forenames>Hongyu</forenames></author><author><keyname>Huang</keyname><forenames>Di</forenames></author><author><keyname>Wang</keyname><forenames>Yunhong</forenames></author><author><keyname>Wang</keyname><forenames>Heng</forenames></author><author><keyname>Tang</keyname><forenames>Yuanyan</forenames></author></authors><title>Face Aging Effect Simulation using Hidden Factor Analysis Joint Sparse
  Representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face aging simulation has received rising investigations nowadays, whereas it
still remains a challenge to generate convincing and natural age-progressed
face images. In this paper, we present a novel approach to such an issue by
using hidden factor analysis joint sparse representation. In contrast to the
majority of tasks in the literature that handle the facial texture integrally,
the proposed aging approach separately models the person-specific facial
properties that tend to be stable in a relatively long period and the
age-specific clues that change gradually over time. It then merely transforms
the age component to a target age group via sparse reconstruction, yielding
aging effects, which is finally combined with the identity component to achieve
the aged face. Experiments are carried out on three aging databases, and the
results achieved clearly demonstrate the effectiveness and robustness of the
proposed method in rendering a face with aging effects. Additionally, a series
of evaluations prove its validity with respect to identity preservation and
aging effect generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01211</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01211</id><created>2015-11-04</created><authors><author><keyname>Bottesch</keyname><forenames>Ralph C.</forenames></author><author><keyname>Gavinsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Klauck</keyname><forenames>Hartmut</forenames></author></authors><title>Equality, Revisited</title><categories>cs.CC quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new lower bound method for analysing the complexity of the
Equality function (EQ) in the Simultaneous Message Passing (SMP) model of
communication complexity. The new technique gives tight lower bounds of
$\Omega(\sqrt n)$ for both EQ and its negation NE in the non-deterministic
version of quantum-classical SMP, where Merlin is also quantum $-$ this is the
strongest known version of SMP where the complexity of both EQ and NE remain
high (previously known techniques seem to be insufficient for this).
  Besides, our analysis provides to a unified view of the communication
complexity of EQ and NE, allowing to obtain tight characterisation in all
previously studied and a few newly introduced versions of SMP, including all
possible combination of either quantum or randomised Alice, Bob and Merlin in
the non-deterministic case.
  Some of our results highlight that NE is easier than EQ in the presence of
classical proofs, whereas the problems have (roughly) the same complexity when
a quantum proof is present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01212</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01212</id><created>2015-11-04</created><authors><author><keyname>Si</keyname><forenames>Hongbo</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Hierarchical Polar Coding for Achieving Secrecy over Fading Wiretap
  Channels without any Instantaneous CSI</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a polar coding scheme to achieve secrecy in block fading
binary symmetric wiretap channels without the knowledge of instantaneous
channel state information (CSI) at the transmitter. For this model, a coding
scheme that hierarchically utilizes polar codes is presented. In particular, on
polarization of different binary symmetric channels over different fading
blocks, each channel use is modeled as an appropriate binary erasure channel
over fading blocks. Polar codes are constructed for both coding over channel
uses for each fading block and coding over fading blocks for certain channel
uses. In order to guarantee security, random bits are introduced at appropriate
places to exhaust the observations of the eavesdropper. It is shown that this
coding scheme, without instantaneous CSI at the transmitter, is secrecy
capacity achieving for the simultaneous fading scenario. For the independent
fading case, the capacity is achieved when the fading realizations for the
eavesdropper channel is always degraded with respect to the receiver. For the
remaining cases, the gap is analyzed by comparing lower and upper bounds.
Remarkably, for the scenarios where the secrecy capacity is achieved, the
results imply that instantaneous CSI does not increase the secrecy capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01214</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01214</id><created>2015-11-04</created><updated>2016-03-07</updated><authors><author><keyname>Gopalan</keyname><forenames>Giri</forenames></author></authors><title>Quantifying the information of the prior and likelihood in parametric
  Bayesian modeling</title><categories>stat.ML cs.IT math.IT stat.AP stat.ME</categories><comments>Experiment section cleaned up</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I suggest using a pair of metrics to quantify the information of the prior
and likelihood functions within a parametric Bayesian model, one of which is
closely related to the reference priors of Berger and Bernardo (Bernardo 1979,
Berger and Bernardo 2009) and information measure introduced by Lindley
(Lindley 1956). A Monte Carlo algorithm to estimate these metrics is developed
and their properties are explored via a combination of theoretical results,
simulations, and applications to public medical data sets. This combination of
theoretical, empirical, and computational support provides evidence that these
metrics may be useful diagnostic tools when performing a Bayesian analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01223</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01223</id><created>2015-11-04</created><authors><author><keyname>Wiener</keyname><forenames>Lucas</forenames></author><author><keyname>Ekholm</keyname><forenames>Tomas</forenames></author><author><keyname>Haller</keyname><forenames>Philipp</forenames></author></authors><title>Modular Responsive Web Design using Element Queries</title><categories>cs.SE cs.PL</categories><acm-class>D.2.13; I.7.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Responsive Web Design (RWD) enables web applications to adapt to the
characteristics of different devices such as screen size which is important for
mobile browsing. Today, the only W3C standard to support this adaptability is
CSS media queries. However, using media queries it is impossible to create
applications in a modular way, because responsive elements then always depend
on the global context. Hence, responsive elements can only be reused if the
global context is exactly the same, severely limiting their reusability. This
makes it extremely challenging to develop large responsive applications,
because the lack of true modularity makes certain requirement changes either
impossible or expensive to realize.
  In this paper we extend RWD to also include responsive modules, i.e., modules
that adapt their design based on their local context independently of the
global context. We present the ELQ project which implements our approach. ELQ
is a novel implementation of so-called element queries which generalize media
queries. Importantly, our design conforms to existing web specifications,
enabling adoption on a large scale. ELQ is designed to be heavily extensible
using plugins. Experimental results show speed-ups of the core algorithms of up
to 37x compared to previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01230</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01230</id><created>2015-11-04</created><authors><author><keyname>Xia</keyname><forenames>Mingji</forenames></author></authors><title>Base collapse of holographic algorithms</title><categories>cs.CC</categories><report-no>ISCAS-SKLCS-14-20</report-no><msc-class>68Q99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A holographic algorithm solves a problem in domain of size $n$, by reducing
it to counting perfect matchings in planar graphs. It may simulate a $n$-value
variable by a bunch of $t$ matchgate bits, which has $2^t$ values. The
transformation in the simulation can be expressed as a $n \times 2^t$ matrix
$M$, called the base of the holographic algorithm. We wonder whether more
matchgate bits bring us more powerful holographic algorithms. In another word,
whether we can solve the same original problem, with a collapsed base of size
$n \times 2^{r}$, where $r&lt;t$.
  Base collapse was discovered for small domain $n=2,3,4$. For $n=3, 4$, the
base collapse was proved under the condition that there is a full rank
generator. We prove for any $n$, the base collapse to a $r\leq \lfloor \log n
\rfloor$, with some similar conditions. One of them is that the original
problem is defined by one symmetric function. In the proof, we utilize
elementary matchgate transformations instead of matchgate identities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01232</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01232</id><created>2015-11-04</created><authors><author><keyname>Morabito</keyname><forenames>Roberto</forenames></author></authors><title>Power Consumption of Virtualization Technologies: an Empirical
  Investigation</title><categories>cs.DC cs.PF</categories><comments>Accepted to the IEEE/ACM UCC 2015 (SD3C Workshop) - IEEE Copyright</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization is growing rapidly as a result of the increasing number of
alternative solutions in this area, and of the wide range of application field.
Until now, hypervisor-based virtualization has been the de facto solution to
perform server virtualization. Recently, container-based virtualization - an
alternative to hypervisors - has gained more attention because of lightweight
characteristics, attracting cloud providers that have already made use of it to
deliver their services. However, a gap in the existing research on containers
exists in the area of power consumption. This paper presents the results of a
performance comparison in terms of power consumption of four different
virtualization technologies: KVM and Xen, which are based on hypervisor
virtualization, Docker and LXC which are based on container virtualization. The
aim of this empirical investigation, carried out by means of a testbed, is to
understand how these technologies react to particular workloads. Our initial
results show how, despite of the number of virtual entities running, both kinds
of virtualization alternatives behave similarly in idle state and in CPU/Memory
stress test. Contrarily, the results on network performance show differences
between the two technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01238</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01238</id><created>2015-11-04</created><updated>2015-11-05</updated><authors><author><keyname>Csermely</keyname><forenames>Peter</forenames></author></authors><title>Fast and slow thinking -- of networks: The complementary 'elite' and
  'wisdom of crowds' of amino acid, neuronal and social networks</title><categories>q-bio.MN cond-mat.dis-nn cs.SI nlin.AO physics.bio-ph</categories><comments>This a preprint of a future paper and book chapter, please find its
  illustrative videos here: http://networkdecisions.linkgroup.hu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex systems may have billion components making consensus formation slow
and difficult. Recently several overlapping stories emerged from various
disciplines, including protein structures, neuroscience and social networks,
showing that fast responses to known stimuli involve a network core of few,
strongly connected nodes. In unexpected situations the core may fail to provide
a coherent response, thus the stimulus propagates to the periphery of the
network. Here the final response is determined by a large number of weakly
connected nodes mobilizing the collective memory and opinion, i.e. the slow
democracy exercising the 'wisdom of crowds'. This mechanism resembles to
Kahneman's &quot;Thinking, Fast and Slow&quot; discriminating fast, pattern-based and
slow, contemplative decision making. The generality of the response also shows
that democracy is neither only a moral stance nor only a decision making
technique, but a very efficient general learning strategy developed by complex
systems during evolution. The duality of fast core and slow majority may
increase our understanding of metabolic, signaling, ecosystem, swarming or
market processes, as well as may help to construct novel methods to explore
unusual network responses, deep-learning neural network structures and
core-periphery targeting drug design strategies. (Illustrative videos can be
downloaded from here: http://networkdecisions.linkgroup.hu)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01239</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01239</id><created>2015-11-04</created><updated>2015-11-06</updated><authors><author><keyname>Csermely</keyname><forenames>Peter</forenames></author></authors><title>Plasticity-rigidity cycles: A general adaptation mechanism</title><categories>q-bio.MN cond-mat.dis-nn cs.SI nlin.AO physics.bio-ph</categories><comments>This is a preprint of a future paper and book chapter (with 280
  references total)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successful adaptation helped the emergence of complexity. Alternating
plastic- and rigid-like states were recurrently considered to play a role in
adaptive processes. However, this extensive knowledge remained fragmented. In
this paper I describe plasticity-rigidity cycles as a general adaptation
mechanism operating in molecular assemblies, assisted protein folding, cellular
differentiation, learning, memory formation, creative thinking, as well as the
organization of social groups and ecosystems. Plasticity-rigidity cycles enable
a novel understanding of aging, exploration/exploitation trade-off and
evolvability, as well as help the design of efficient interventions in medicine
and in crisis management of financial and biological ecosystems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01245</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01245</id><created>2015-11-04</created><updated>2015-11-18</updated><authors><author><keyname>Bouwmans</keyname><forenames>Thierry</forenames></author><author><keyname>Sobral</keyname><forenames>Andrews</forenames></author><author><keyname>Javed</keyname><forenames>Sajid</forenames></author><author><keyname>Jung</keyname><forenames>Soon Ki</forenames></author><author><keyname>Zahzah</keyname><forenames>El-Hadi</forenames></author></authors><title>Decomposition into Low-rank plus Additive Matrices for
  Background/Foreground Separation: A Review for a Comparative Evaluation with
  a Large-Scale Dataset</title><categories>cs.CV</categories><comments>121 pages, 5 figures, submitted to Computer Science Review. arXiv
  admin note: text overlap with arXiv:1312.7167, arXiv:1109.6297,
  arXiv:1207.3438, arXiv:1105.2126, arXiv:1404.7592, arXiv:1210.0805,
  arXiv:1403.8067 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research on problem formulations based on decomposition into low-rank
plus sparse matrices shows a suitable framework to separate moving objects from
the background. The most representative problem formulation is the Robust
Principal Component Analysis (RPCA) solved via Principal Component Pursuit
(PCP) which decomposes a data matrix in a low-rank matrix and a sparse matrix.
However, similar robust implicit or explicit decompositions can be made in the
following problem formulations: Robust Non-negative Matrix Factorization
(RNMF), Robust Matrix Completion (RMC), Robust Subspace Recovery (RSR), Robust
Subspace Tracking (RST) and Robust Low-Rank Minimization (RLRM). The main goal
of these similar problem formulations is to obtain explicitly or implicitly a
decomposition into low-rank matrix plus additive matrices. In this context,
this work aims to initiate a rigorous and comprehensive review of the similar
problem formulations in robust subspace learning and tracking based on
decomposition into low-rank plus additive matrices for testing and ranking
existing algorithms for background/foreground separation. For this, we first
provide a preliminary review of the recent developments in the different
problem formulations which allows us to define a unified view that we called
Decomposition into Low-rank plus Additive Matrices (DLAM). Then, we examine
carefully each method in each robust subspace learning/tracking frameworks with
their decomposition, their loss functions, their optimization problem and their
solvers. Furthermore, we investigate if incremental algorithms and real-time
implementations can be achieved for background/foreground separation. Finally,
experimental results on a large-scale dataset called Background Models
Challenge (BMC 2012) show the comparative performance of 32 different robust
subspace learning/tracking methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01249</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01249</id><created>2015-11-04</created><authors><author><keyname>Ta</keyname><forenames>Vinh Thong</forenames></author></authors><title>On the Systematic Design of Privacy Policies and Privacy Architectures</title><categories>cs.CR</categories><comments>27 pages LNCS format, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of systematic privacy policy and
privacy architecture design. We focus on two relevant aspects of privacy,
namely, accountability and personal data control. We propose a systematic
design approach of privacy policies adapting the current international data
protection regulations, as well as an automated privacy architectures
generation method from the corresponding policies. In particular, we propose a
high-level policy language and an architecture language, as well as a
systematic mapping procedure from policies to the corresponding architectures.
We demonstrate the usability of our proposed approach on real-world systems
such as Facebook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01258</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01258</id><created>2015-11-04</created><updated>2015-11-08</updated><authors><author><keyname>Segev</keyname><forenames>Noam</forenames></author><author><keyname>Harel</keyname><forenames>Maayan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author><author><keyname>El-Yaniv</keyname><forenames>Ran</forenames></author></authors><title>Learn on Source, Refine on Target:A Model Transfer Learning Framework
  with Random Forests</title><categories>cs.LG</categories><comments>2 columns, 14 pages, TPAMI submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose novel model transfer-learning methods that refine a decision
forest model M learned within a &quot;source&quot; domain using a training set sampled
from a &quot;target&quot; domain, assumed to be a variation of the source. We present two
random forest transfer algorithms. The first algorithm searches greedily for
locally optimal modifications of each tree structure by trying to locally
expand or reduce the tree around individual nodes. The second algorithm does
not modify structure, but only the parameter (thresholds) associated with
decision nodes. We also propose to combine both methods by considering an
ensemble that contains the union of the two forests. The proposed methods
exhibit impressive experimental results over a range of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01259</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01259</id><created>2015-11-04</created><authors><author><keyname>Grefenstette</keyname><forenames>Gregory</forenames><affiliation>TAO</affiliation></author><author><keyname>Rafes</keyname><forenames>Karima</forenames><affiliation>TAO</affiliation></author></authors><title>Transforming Wikipedia into a Search Engine for Local Experts</title><categories>cs.IR cs.CL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding experts for a given problem is recognized as a difficult task. Even
when a taxonomy of subject expertise exists, and is associated with a group of
experts, it can be hard to exploit by users who have not internalized the
taxonomy. Here we present a method for both attaching experts to a domain
ontology, and hiding this fact from the end user looking for an expert. By
linking Wikipedia to this same pivot ontology, we describe how a user can
browse Wikipedia, as they normally do to search for information, and use this
browsing behavior to find experts. Experts are characterized by their textual
productions (webpages, publications, reports), and these textual productions
are attached to concepts in the pivot ontology. When the user finds the
Wikipedia page characterizing their need, a list of experts is displayed. In
this way we transform Wikipedia into a search engine for experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01261</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01261</id><created>2015-11-04</created><authors><author><keyname>Gebser</keyname><forenames>Martin</forenames></author><author><keyname>Obermeier</keyname><forenames>Phillip</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>Interactive Answer Set Programming - Preliminary Report</title><categories>cs.PL</categories><comments>International Workshop on User-Oriented Logic Programming (IULP
  2015), co-located with the 31st International Conference on Logic Programming
  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic
  Programming (IULP 2015), invited talk, Editors: Stefan Ellmauthaler and
  Claudia Schulz, pages 3-17, August 2015</comments><proxy>Stefan Ellmauthaler</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional Answer Set Programming (ASP) rests upon one-shot solving. A logic
program is fed into an ASP system and its stable models are computed. The high
practical relevance of dynamic applications led to the development of
multi-shot solving systems. An operative system solves continuously changing
logic programs. Although this was primarily aiming at dynamic applications in
assisted living, robotics, or stream reasoning, where solvers interact with an
environment, it also opened up the opportunity of interactive ASP, where a
solver interacts with a user. We begin with a formal characterization of
interactive ASP in terms of states and operations on them. In turn, we describe
the interactive ASP shell aspic along with its basic functionalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01272</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01272</id><created>2015-11-04</created><authors><author><keyname>Ehrhard</keyname><forenames>Thomas</forenames><affiliation>PPS</affiliation></author><author><keyname>Pagani</keyname><forenames>Michele</forenames><affiliation>PPS</affiliation></author><author><keyname>Tasson</keyname><forenames>Christine</forenames><affiliation>PPS</affiliation></author></authors><title>Full abstraction for probabilistic PCF</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a probabilistic version of PCF, a well-known simply typed
universal functional language. The type hierarchy is based on a single ground
type of natural numbers. Even if the language is globally call-by-name, we
allow a call-by-value evaluation for ground type arguments in order to provide
the language with a suitable algorithmic expressiveness. We describe a
denotational semantics based on probabilistic coherence spaces, a model of
classical Linear Logic developed in previous works. We prove an adequacy and an
equational full abstraction theorem showing that equality in the model
coincides with a natural notion of observational equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01276</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01276</id><created>2015-11-04</created><authors><author><keyname>Fadlallah</keyname><forenames>Yasser</forenames><affiliation>SOCRATE</affiliation></author><author><keyname>Cardoso</keyname><forenames>Leonardo S.</forenames><affiliation>SOCRATE</affiliation></author><author><keyname>Gorce</keyname><forenames>Jean-Marie</forenames><affiliation>SOCRATE</affiliation></author></authors><title>Demo: Non-classic Interference Alignment for Downlink Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Joint NEWCOM/COST Workshop on Wireless Communications JNCW 2015, Oct
  2015, Barcelone, Spain. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our demo aims at proving the concept of a recent proposed interference
management scheme that reduces the inter-cell interference in downlink without
complex coordination, known as non-classic interference alignment (IA) scheme.
We assume a case where one main Base Station (BS) needs to serve three users
equipments (UE) while another BS is causing interference. The primary goal is
to construct the alignment scheme ; i.e. each UE estimates the main and
interfered channel coefficients, calculates the optimal interference free
directions dropped by the interfering BS and feeds them back to the main BS
which in turn applies a scheduling to select the best free inter-cell
interference directions. Once the scheme is build, we are able to measure the
total capacity of the downlink interference channel. We run the scheme in
CorteXlab ; a controlled hardware facility located in Lyon, France with
remotely programmable radios and multi-node processing capabilities, and we
illustrate the achievable capacity gain for different channel realizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01280</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01280</id><created>2015-11-04</created><authors><author><keyname>De Myttenaere</keyname><forenames>Arnaud</forenames><affiliation>SAMM, Viadeo</affiliation></author><author><keyname>Golden</keyname><forenames>Boris</forenames><affiliation>Viadeo</affiliation></author><author><keyname>Grand</keyname><forenames>B&#xe9;n&#xe9;dicte Le</forenames><affiliation>CRI</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Study of a bias in the offline evaluation of a recommendation algorithm</title><categories>cs.IR cs.LG stat.ML</categories><comments>arXiv admin note: substantial text overlap with arXiv:1407.0822</comments><proxy>ccsd</proxy><journal-ref>Petra Perner. 11th Industrial Conference on Data Mining, ICDM
  2015, Jul 2015, Hamburg, Germany. Ibai Publishing, pp.57-70, 2015, Advances
  in Data Mining</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation systems have been integrated into the majority of large online
systems to filter and rank information according to user profiles. It thus
influences the way users interact with the system and, as a consequence, bias
the evaluation of the performance of a recommendation algorithm computed using
historical data (via offline evaluation). This paper describes this bias and
discuss the relevance of a weighted offline evaluation to reduce this bias for
different classes of recommendation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01281</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01281</id><created>2015-11-04</created><authors><author><keyname>Mahrsi</keyname><forenames>Mohamed Khalil El</forenames><affiliation>LTCI, SAMM</affiliation></author><author><keyname>Guigour&#xe8;s</keyname><forenames>Romain</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author><author><keyname>Boull&#xe9;</keyname><forenames>Marc</forenames></author></authors><title>Co-Clustering Network-Constrained Trajectory Data</title><categories>stat.ML cs.DB cs.LG</categories><proxy>ccsd</proxy><journal-ref>Advances in Knowledge Discovery and Management, 615, Springer
  International Publishing, pp.19-32, 2015, Studies in Computational
  Intelligence, 978-3-319-23750-3</journal-ref><doi>10.1007/978-3-319-23751-0_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, clustering moving object trajectories kept gaining interest from
both the data mining and machine learning communities. This problem, however,
was studied mainly and extensively in the setting where moving objects can move
freely on the euclidean space. In this paper, we study the problem of
clustering trajectories of vehicles whose movement is restricted by the
underlying road network. We model relations between these trajectories and road
segments as a bipartite graph and we try to cluster its vertices. We
demonstrate our approaches on synthetic data and show how it could be useful in
inferring knowledge about the flow dynamics and the behavior of the drivers
using the road network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01282</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01282</id><created>2015-11-04</created><authors><author><keyname>Nguyen</keyname><forenames>Phong</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Kalousis</keyname><forenames>Alexandros</forenames></author></authors><title>Factorizing LambdaMART for cold start recommendations</title><categories>cs.LG cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation systems often rely on point-wise loss metrics such as the mean
squared error. However, in real recommendation settings only few items are
presented to a user. This observation has recently encouraged the use of
rank-based metrics. LambdaMART is the state-of-the-art algorithm in learning to
rank which relies on such a metric. Despite its success it does not have a
principled regularization mechanism relying in empirical approaches to control
model complexity leaving it thus prone to overfitting.
  Motivated by the fact that very often the users' and items' descriptions as
well as the preference behavior can be well summarized by a small number of
hidden factors, we propose a novel algorithm, LambdaMART Matrix Factorization
(LambdaMART-MF), that learns a low rank latent representation of users and
items using gradient boosted trees. The algorithm factorizes lambdaMART by
defining relevance scores as the inner product of the learned representations
of the users and items. The low rank is essentially a model complexity
controller; on top of it we propose additional regularizers to constraint the
learned latent representations that reflect the user and item manifolds as
these are defined by their original feature based descriptors and the
preference behavior. Finally we also propose to use a weighted variant of NDCG
to reduce the penalty for similar items with large rating discrepancy.
  We experiment on two very different recommendation datasets, meta-mining and
movies-users, and evaluate the performance of LambdaMART-MF, with and without
regularization, in the cold start setting as well as in the simpler matrix
completion setting. In both cases it outperforms in a significant manner
current state of the art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01287</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01287</id><created>2015-11-04</created><authors><author><keyname>Fraigniaud</keyname><forenames>Pierre</forenames><affiliation>GANG, LIAFA</affiliation></author><author><keyname>Heinrich</keyname><forenames>Marc</forenames><affiliation>GANG, LIAFA</affiliation></author><author><keyname>Kosowski</keyname><forenames>Adrian</forenames><affiliation>GANG, LIAFA</affiliation></author></authors><title>Local Conflict Coloring</title><categories>cs.DS cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locally finding a solution to symmetry-breaking tasks such as
vertex-coloring, edge-coloring, maximal matching, maximal independent set,
etc., is a long-standing challenge in distributed network computing. More
recently, it has also become a challenge in the framework of centralized local
computation. We introduce conflict coloring as a general symmetry-breaking task
that includes all the aforementioned tasks as specific instantiations ---
conflict coloring includes all locally checkable labeling tasks from
[Naor\\&amp;Stockmeyer, STOC 1993]. Conflict coloring is characterized by two
parameters $l$ and $d$, where the former measures the amount of freedom given
to the nodes for selecting their colors, and the latter measures the number of
constraints which colors of adjacent nodes are subject to. We show that, in the
standard \LOCAL model for distributed network computing, if $l/d \textgreater{}
\Delta$, then conflict coloring can be solved in
$\Otilde(\sqrt{\Delta})+\log^*n$ rounds in $n$-node graphs with maximum
degree~$\Delta$, where $\Otilde$ ignores the polylog factors in $\Delta$. The
dependency in~$n$ is optimal, as a consequence of the $\Omega(\log^*n)$ lower
bound by [Linial, SIAM J. Comp. 1992] for $(\Delta+1)$-coloring. An important
special case of our result is a significant improvement over the best known
algorithm for distributed $(\Delta+1)$-coloring due to [Barenboim, PODC 2015],
which required $\Otilde(\Delta^{3/4})+\log^*n$ rounds. Improvements for other
variants of coloring, including $(\Delta+1)$-list-coloring,
$(2\Delta-1)$-edge-coloring, $T$-coloring, etc., also follow from our general
result on conflict coloring. Likewise, in the framework of centralized local
computation algorithms (LCAs), our general result yields an LCA which requires
a smaller number of probes than the previously best known algorithm for
vertex-coloring, and works for a wide range of coloring problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01289</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01289</id><created>2015-11-04</created><authors><author><keyname>Ravishankar</keyname><forenames>Saiprasad</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Data-Driven Learning of a Union of Sparsifying Transforms Model for
  Blind Compressed Sensing</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1501.02923</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing is a powerful tool in applications such as magnetic
resonance imaging (MRI). It enables accurate recovery of images from highly
undersampled measurements by exploiting the sparsity of the images or image
patches in a transform domain or dictionary. In this work, we focus on blind
compressed sensing (BCS), where the underlying sparse signal model is a priori
unknown, and propose a framework to simultaneously reconstruct the underlying
image as well as the unknown model from highly undersampled measurements.
Specifically, our model is that the patches of the underlying image(s) are
approximately sparse in a transform domain. We also extend this model to a
union of transforms model that better captures the diversity of features in
natural images. The proposed block coordinate descent type algorithms for blind
compressed sensing are highly efficient, and are guaranteed to converge to at
least the partial global and partial local minimizers of the highly non-convex
BCS problems. Our numerical experiments show that the proposed framework
usually leads to better quality of image reconstructions in MRI compared to
several recent image reconstruction methods. Importantly, the learning of a
union of sparsifying transforms leads to better image reconstructions than a
single adaptive transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01291</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01291</id><created>2015-11-04</created><updated>2016-02-27</updated><authors><author><keyname>Diamantoulakis</keyname><forenames>Panagiotis D.</forenames></author><author><keyname>Pappi</keyname><forenames>Koralia N.</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Wireless Powered Communications with Non-Orthogonal Multiple Access</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a wireless-powered uplink communication system with non-orthogonal
multiple access (NOMA), consisting of one base station and multiple energy
harvesting users. More specifically, we focus on the individual data rate
optimization and fairness improvement and we show that the formulated problems
can be optimally and efficiently solved by either linear programming or convex
optimization. In the provided analysis, two types of decoding order strategies
are considered, namely fixed decoding order and time- sharing. Furthermore, we
propose an efficient greedy algorithm, which is suitable for the practical
implementation of the time-sharing strategy. Simulation results illustrate that
the proposed scheme outperforms the baseline orthogonal multiple access scheme.
More specifically, it is shown that NOMA offers a considerable improvement in
throughput, fairness, and energy efficiency. Also, the dependence among system
throughput, minimum individual data rate, and harvested energy is revealed, as
well as an interesting trade-off between rates and energy efficiency. Finally,
the convergence speed of the proposed greedy algorithm is evaluated, and it is
shown that the required number of iterations is linear with respect to the
number of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01293</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01293</id><created>2015-11-04</created><authors><author><keyname>Cavagna</keyname><forenames>Andrea</forenames></author><author><keyname>Creato</keyname><forenames>Chiara</forenames></author><author><keyname>Del Castello</keyname><forenames>Lorenzo</forenames></author><author><keyname>Melillo</keyname><forenames>Stefania</forenames></author><author><keyname>Parisi</keyname><forenames>Leonardo</forenames></author><author><keyname>Viale</keyname><forenames>Massimiliano</forenames></author></authors><title>Towards a tracking algorithm based on the clustering of spatio-temporal
  clouds of points</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interest in 3D dynamical tracking is growing in fields such as robotics,
biology and fluid dynamics. Recently, a major source of progress in 3D tracking
has been the study of collective behaviour in biological systems, where the
trajectories of individual animals moving within large and dense groups need to
be reconstructed to understand the behavioural interaction rules. Experimental
data in this field are generally noisy and at low spatial resolution, so that
individuals appear as small featureless objects and trajectories must be
retrieved by making use of epipolar information only. Moreover, optical
occlusions often occur: in a multi-camera system one or more objects become
indistinguishable in one view, potentially jeopardizing the conservation of
identity over long-time trajectories. The most advanced 3D tracking algorithms
overcome optical occlusions making use of set-cover techniques, which however
have to solve NP-hard optimization problems. Moreover, current methods are not
able to cope with occlusions arising from actual physical proximity of objects
in 3D space. Here, we present a new method designed to work directly in 3D
space and time, creating (3D+1) clouds of points representing the full
spatio-temporal evolution of the moving targets. We can then use a simple
connected components labeling routine, which is linear in time, to solve
optical occlusions, hence lowering from NP to P the complexity of the problem.
Finally, we use normalized cut spectral clustering to tackle 3D physical
proximity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01297</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01297</id><created>2015-11-04</created><authors><author><keyname>Lv</keyname><forenames>Yuezu</forenames></author><author><keyname>Li</keyname><forenames>Zhongkui</forenames></author><author><keyname>Duan</keyname><forenames>Zhisheng</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author></authors><title>Fully Distributed Adaptive Output Feedback Protocols for Linear
  Multi-Agent Systems with Directed Graphs: A Sequential Observer Design
  Approach</title><categories>cs.SY</categories><comments>14 pages, 8 figures, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies output feedback consensus protocol design problems for
linear multi-agent systems with directed graphs. We consider both leaderless
and leader-follower consensus with a leader whose control input is nonzero and
bounded. We propose a novel sequential observer design approach, which makes it
possible to design fully distributed adaptive output feedback protocols that
the existing methods fail to accomplish. With the sequential observer
architecture, we show that leaderless consensus can be achieved for any
strongly connected directed graph in a fully distributed manner, whenever the
agents are stabilizable and detectable. For the case with a leader of bounded
control input, we further present novel distributed adaptive output feedback
protocols, which include nonlinear functions to deal with the effect of the
leaders's nonzero control input and are able to achieve leader-follower
consensus for any directed graph containing a directed spanning tree with the
leader as the root.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01303</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01303</id><created>2015-11-04</created><authors><author><keyname>Durand</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LINCS, GANG</affiliation></author><author><keyname>Kloeckner</keyname><forenames>Beno&#xee;t</forenames><affiliation>LAMA</affiliation></author><author><keyname>Mathieu</keyname><forenames>Fabien</forenames><affiliation>LINCS</affiliation></author><author><keyname>Noirie</keyname><forenames>Ludovic</forenames><affiliation>LINCS</affiliation></author></authors><title>Geometry on the Utility Space</title><categories>cs.GT</categories><comments>in Fourth International Conference on Algorithmic Decision Theory,
  Sep 2015, Lexington, United States. pp.16, 2015, Fourth International
  Conference on Algorithmic Decision Theory</comments><proxy>ccsd</proxy><doi>10.1007/978-3-319-23114-3_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the geometrical properties of the utility space (the space of
expected utilities over a finite set of options), which is commonly used to
model the preferences of an agent in a situation of uncertainty. We focus on
the case where the model is neutral with respect to the available options, i.e.
treats them, a priori, as being symmetrical from one another. Specifically, we
prove that the only Riemannian metric that respects the geometrical properties
and the natural symmetries of the utility space is the round metric. This
canonical metric allows to define a uniform probability over the utility space
and to naturally generalize the Impartial Culture to a model with expected
utilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01306</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01306</id><created>2015-11-04</created><updated>2016-02-03</updated><authors><author><keyname>Cohen</keyname><forenames>Jeremy E.</forenames></author></authors><title>About Notations in Multiway Array Processing</title><categories>cs.NA</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This paper gives an overview of notations used in multiway array processing.
We redefine the vectorization and matricization operators to comply with some
properties of the Kronecker product. The tensor product and Kronecker product
are also represented with two different symbols, and it is shown how these
notations lead to clearer expressions for multiway array operations. Finally,
the paper recalls the useful yet widely unknown properties of the array normal
law with suggested notations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01315</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01315</id><created>2015-11-04</created><authors><author><keyname>Chappelon</keyname><forenames>Jonathan</forenames><affiliation>IMAG</affiliation></author><author><keyname>Mart&#xed;nez-Sandoval</keyname><forenames>Leonardo</forenames><affiliation>IMAG</affiliation></author><author><keyname>Montejano</keyname><forenames>Luis</forenames><affiliation>IMAG</affiliation></author><author><keyname>Montejano</keyname><forenames>Luis Pedro</forenames><affiliation>IMAG</affiliation></author><author><keyname>Alfons&#xed;n</keyname><forenames>Jorge Ram&#xed;rez</forenames><affiliation>IMAG</affiliation></author></authors><title>Complete Kneser Transversals</title><categories>math.CO cs.DM math.MG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $k,d,\lambda\geqslant1$ be integers with $d\geqslant\lambda $. In 2010,
the following function was
introduced:\par\smallskip$m(k,d,\lambda)\overset{\mathrm{def}}{=}$ the maximum
positive integer $n$ such that every set of $n$ points (not necessarily in
general position) in $\mathbb{R}^{d}$ has the property that the convex hulls of
all $k$-sets have a common transversal $(d-\lambda)$-plane.\smallskip\parIt
turns out that $m(k, d,\lambda)$ is strongly connected with other interesting
problems, for instance, the chromatic number of Kneser hypergraphs and a
discrete version of Rado's central Theorem.\par In the same spirit, we
introduce a natural discrete version $m^*$ of $m$ by considering the existence
of \emph{complete Kneser transversals}. We study the relation between them and
give a number of lower and upper bounds of $m^*$ as well as the exact value in
some cases. The main ingredient for the proofs are Radon's partition theorem as
well as oriented matroids tools. By studying the so-called \emph{alternating}
oriented matroid we provide the asymptotic behavior of the function $m^*$ for
the family of cyclic polytopes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01331</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01331</id><created>2015-11-04</created><authors><author><keyname>Lv</keyname><forenames>Yuezu</forenames></author><author><keyname>Li</keyname><forenames>Zhongkui</forenames></author><author><keyname>Duan</keyname><forenames>Zhisheng</forenames></author><author><keyname>Feng</keyname><forenames>Gang</forenames></author></authors><title>Novel Distributed Robust Adaptive Consensus Protocols for Linear
  Multi-agent Systems with Directed Graphs and External Disturbances</title><categories>cs.SY</categories><comments>9 pages, 5 figures. submitted for publication. arXiv admin note: text
  overlap with arXiv:1312.7377</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the distributed consensus protocol design problem for
linear multi-agent systems with directed graphs and external unmatched
disturbances. A novel distributed adaptive consensus protocol is proposed to
achieve leader-follower consensus for any directed graph containing a directed
spanning tree with the leader as the root node. It is noted that the adaptive
protocol might suffer from a problem of undesirable parameter drift phenomenon
when bounded external disturbances exist. To deal with this issue, a
distributed robust adaptive consensus protocol is designed to guarantee the
ultimate boundedness of both the consensus error and the adaptive coupling
weights in the presence of external disturbances. Both adaptive protocols are
fully distributed, relying on only the agent dynamics and the relative states
of neighboring agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01344</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01344</id><created>2015-11-04</created><updated>2016-01-22</updated><authors><author><keyname>Poojary</keyname><forenames>Sudheer</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Asymptotic Approximations for TCP Compound</title><categories>cs.NI</categories><comments>Longer version for NCC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive an approximation for throughput of TCP Compound
connections under random losses. Throughput expressions for TCP Compound under
a deterministic loss model exist in the literature. These are obtained assuming
the window sizes are continuous, i.e., a fluid behaviour is assumed. We
validate this model theoretically. We show that under the deterministic loss
model, the TCP window evolution for TCP Compound is periodic and is independent
of the initial window size. We then consider the case when packets are lost
randomly and independently of each other. We discuss Markov chain models to
analyze performance of TCP in this scenario. We use insights from the
deterministic loss model to get an appropriate scaling for the window size
process and show that these scaled processes, indexed by p, the packet error
rate, converge to a limit Markov chain process as p goes to 0. We show the
existence and uniqueness of the stationary distribution for this limit process.
Using the stationary distribution for the limit process, we obtain
approximations for throughput, under random losses, for TCP Compound when
packet error rates are small. We compare our results with ns2 simulations which
show a good match.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01353</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01353</id><created>2015-10-31</created><authors><author><keyname>Challacombe</keyname><forenames>Matt</forenames></author></authors><title>A N-Body Solver for Free Mesh Interpolation</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Factorization of the Gaussian RBF kernel is developed for free-mesh
interpolation in the flat, polynomial limit corresponding to Taylor expansion
and the Vandermonde basis of geometric moments. With this spectral
approximation, a top-down octree-scoping of an interpolant is found by
recursively decomposing the residual, similar to the work of Driscoll and
Heryudono (2007), except that in the current approach the grid is decoupled
from the low rank approximation, allowing partial separation of sampling errors
(the mesh) from representation errors (the polynomial order). Then, it is
possible to demonstrate roughly 5 orders of magnitude improvement in free-mesh
interpolation errors for the three-dimensional Franke function, relative to
previous benchmarks. As in related work on $N$-body methods for factorization
by square root iteration (Challacombe 2015), some emphasis is placed on
resolution of the identity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01354</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01354</id><created>2015-11-04</created><updated>2016-03-04</updated><authors><author><keyname>Araiza-Illan</keyname><forenames>Dejanira</forenames></author><author><keyname>Western</keyname><forenames>David</forenames></author><author><keyname>Pipe</keyname><forenames>Anthony</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>Systematic and Realistic Testing in Simulation of Control Code for
  Robots in Collaborative Human-Robot Interactions</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industries such as flexible manufacturing and home care will be transformed
by the presence of robotic assistants. Assurance of safety and functional
soundness for these robotic systems will require rigorous verification and
validation. We propose testing in simulation using Coverage-Driven Verification
(CDV) to guide the testing process in an automatic and systematic way. We use a
two-tiered test generation approach, where abstract test sequences are computed
first and then concretized (e.g., data and variables are instantiated), to
reduce the complexity of the test generation problem. To demonstrate the
effectiveness of our approach, we developed a testbench for robotic code,
running in ROS-Gazebo, that implements an object handover as part of a
human-robot interaction (HRI) task. Tests are generated to stimulate the
robot's code in a realistic manner, through stimulating the human, environment,
sensors, and actuators in simulation. We compare the merits of unconstrained,
constrained and model-based test generation in achieving thorough exploration
of the code under test, and interesting combinations of human-robot actions.
Our results show that CDV combined with systematic test generation achieves a
very high degree of automation in simulation-based verification of control code
for robots in HRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01363</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01363</id><created>2015-11-04</created><authors><author><keyname>Broadbent</keyname><forenames>Anne</forenames></author><author><keyname>Gharibian</keyname><forenames>Sevag</forenames></author><author><keyname>Zhou</keyname><forenames>Hong-Sheng</forenames></author></authors><title>Quantum One-Time Memories from Stateless Hardware</title><categories>quant-ph cs.CR</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central tenet of theoretical cryptography is the study of the minimal
assumptions required to implement a given cryptographic primitive. One such
primitive is the one-time memory (OTM), introduced by Goldwasser, Kalai, and
Rothblum [CRYPTO 2008], which is a classical functionality modeled after a
non-interactive 1-out-of-2 oblivious transfer, and which is complete for
one-time classical and quantum programs. It is known that secure OTMs do not
exist in the standard model in both the classical and quantum settings. Here,
we show how to use quantum information, together with the assumption of
stateless (i.e., reusable) hardware tokens, to build statistically secure OTMs.
This is in sharp contrast with the classical case, where stateless hardware
tokens alone cannot yield OTMs. In addition, our scheme is technologically
simple. We prove security in the quantum universal composability framework,
employing semi-definite programming results of Molina, Vidick and Watrous [TQC
2013] and combinatorial techniques of Pastawski et al. [Proc. Natl. Acad. Sci.
2012].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01368</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01368</id><created>2015-11-04</created><updated>2016-02-01</updated><authors><author><keyname>Goldberg</keyname><forenames>Eugene</forenames></author></authors><title>Equivalence Checking By Logic Relaxation</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new framework for Equivalence Checking (EC) of Boolean
circuits based on a general technique called Logic Relaxation (LoR). The
essence of LoR is to relax the formula to be solved and compute a superset S of
the set of new behaviors. Namely, S contains all new satisfying assignments
that appeared due to relaxation and does not contain assignments satisfying the
original formula. Set S is generated by a procedure called partial quantifier
elimination. If all possible bad behaviors are in S, the original formula
cannot have them and so the property described by this formula holds. The
appeal of EC by LoR is twofold. First, it facilitates generation of powerful
inductive proofs. Second, proving inequivalence comes down to checking the
presence of some bad behaviors in the relaxed formula, which simplifies bug
hunting. We give some experimental evidence that supports our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01379</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01379</id><created>2015-11-04</created><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author><author><keyname>Wrochna</keyname><forenames>Marcin</forenames></author></authors><title>Fully polynomial-time parameterized computations for graphs and matrices
  of low treewidth</title><categories>cs.DS cs.CC</categories><comments>43 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of several fundamental polynomial-time solvable
problems on graphs and on matrices, when the given instance has low treewidth;
in the case of matrices, we consider the treewidth of the graph formed by
non-zero entries. In each of the considered cases, the best known algorithms
working on general graphs run in polynomial time, however the exponent of the
polynomial is large. Therefore, our main goal is to construct algorithms with
running time of the form $\textrm{poly}(k)\cdot n$ or $\textrm{poly}(k)\cdot
n\log n$, where $k$ is the width of the tree decomposition given on the input.
Such procedures would outperform the best known algorithms for the considered
problems already for moderate values of the treewidth, like $O(n^{1/c})$ for
some small constant $c$.
  Our results include:
  -- an algorithm for computing the determinant and the rank of an $n\times n$
matrix using $O(k^3\cdot n)$ time and arithmetic operations;
  -- an algorithm for solving a system of linear equations using $O(k^3\cdot
n)$ time and arithmetic operations;
  -- an $O(k^3\cdot n\log n)$-time randomized algorithm for finding the
cardinality of a maximum matching in a graph;
  -- an $O(k^4\cdot n\log^2 n)$-time randomized algorithm for constructing a
maximum matching in a graph;
  -- an $O(k^2\cdot n\log n)$-time algorithm for finding a maximum vertex flow
in a directed graph.
  Moreover, we give an approximation algorithm for treewidth with time
complexity suited to the running times as above. Namely, the algorithm, when
given a graph $G$ and integer $k$, runs in time $O(k^7\cdot n\log n)$ and
either correctly reports that the treewidth of $G$ is larger than $k$, or
constructs a tree decomposition of $G$ of width $O(k^2)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01380</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01380</id><created>2015-11-04</created><authors><author><keyname>Tran</keyname><forenames>Loc V.</forenames></author><author><keyname>Phung-Van</keyname><forenames>Phuc</forenames></author><author><keyname>Lee</keyname><forenames>Jaehong</forenames></author><author><keyname>Nguyen-Xuan</keyname><forenames>H.</forenames></author><author><keyname>Wahab</keyname><forenames>M. Abdel</forenames></author></authors><title>Isogeometric approach for nonlinear bending and post-buckling analysis
  of functionally graded plates under thermal environment</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, equilibrium and stability equations of functionally graded
material (FGM) plate under thermal environment are formulated based on
isogeometric analysis (IGA) in combination with higher-order shear deformation
theory (HSDT). The FGM plate is made by a mixture of two distinct components,
for which material properties not only vary continuously through thickness
according to a power-law distribution but also are assumed to be a function of
temperature. Temperature field is assumed to be constant in any plane and
uniform, linear and nonlinear through plate thickness, respectively. The
governing equation is in nonlinear form based on von Karman assumption and
thermal effect. A NURBS-based isogeometric finite element formulation is
utilized to naturally fulfil the rigorous C1-continuity required by the present
plate model. Influences of gradient indices, boundary conditions, temperature
distributions, material properties, length-to-thickness ratios on the behaviour
of FGM plate are discussed in details. Numerical results demonstrate excellent
performance of the present approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01399</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01399</id><created>2015-11-04</created><updated>2015-11-20</updated><authors><author><keyname>Garcia</keyname><forenames>Ronald</forenames></author><author><keyname>Tanter</keyname><forenames>&#xc9;ric</forenames></author></authors><title>Deriving a Simple Gradual Security Language</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstracting Gradual Typing (AGT) is an approach to systematically deriving
gradual counterparts to static type disciplines. The approach consists of
defining the semantics of gradual types by interpreting them as sets of static
types, and then defining an optimal abstraction back to gradual types. These
operations are used to lift the static discipline to the gradual setting. The
runtime semantics of the gradual language then arises as reductions on gradual
typing derivations.
  To demonstrate the flexibility of AGT, we gradualize $\lambda_\text{SEC}$,
the prototypical security-typed language, with respect to only security labels
rather than entire types, yielding a type system that ranges gradually from
simply-typed to securely-typed. We establish noninterference for the gradual
language, called $\lambda_{\widetilde{\text{SEC}}}$, using Zdancewic's logical
relation proof method. Whereas prior work presents gradual security cast
languages, which require explicit security casts, this work yields the first
gradual security source language, which requires no explicit casts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01409</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01409</id><created>2015-11-04</created><updated>2015-11-16</updated><authors><author><keyname>Mehta</keyname><forenames>Ruta</forenames></author><author><keyname>Panageas</keyname><forenames>Ioannis</forenames></author><author><keyname>Piliouras</keyname><forenames>Georgios</forenames></author><author><keyname>Tetali</keyname><forenames>Prasad</forenames></author><author><keyname>Vazirani</keyname><forenames>Vijay V.</forenames></author></authors><title>The game of survival: Sexual evolution in dynamic environments</title><categories>q-bio.PE cs.DM math.DS math.PR</categories><comments>22 pages, 3 figures. Section for open problems added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolution is a complex algorithmic solution to life's most pressing
challenge, that of survival. It is a mixture of numerous textbook optimization
techniques. Natural selection, the preferential replication ofthe fittest,
encodes the multiplicative weights update algorithm, which in static
environments is tantamount to exponential growth for the best solution. Sex can
be interpreted as a game between different agents/genes with identical
interests, maximizing the fitness of the individual. Mutation forces the
exploration of consistently suboptimal solutions. Are all of these mechanisms
necessary to ensure for survival? Also, how is it that despite their
contradictory character (e.g., selection versus mutation) they do not cancel
each other out? We address these questions by extending classic evolutionary
models to allow for a dynamically changing environment. Sexual selection is
well suited for static environments where we show that it converges
polynomially fast to monomorphic populations. Mutations make the difference in
dynamic environments. Without them species become extinct as they do not have
the flexibility to recover fast given environmental change. On the other hand,
we show that with mutation, as long as the rate of change of the environment is
not too fast, long term survival is possible. Finally, mutation does not cancel
the role of selection in static environments. Convergence remains guaranteed
and only the level of polymorphism of the equilibria is affected. Our
techniques quantify exploration-exploitation tradeoffs in time evolving
non-convex optimization problems which could be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01411</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01411</id><created>2015-11-04</created><updated>2015-12-19</updated><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Learning in Auctions: Regret is Hard, Envy is Easy</title><categories>cs.GT cs.AI cs.CC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there are no polynomial-time no-regret learning algorithms for
simultaneous second price auctions (SiSPAs), unless $RP\supseteq NP$, even when
the bidders are unit-demand. We prove this by establishing a specific result
about SiSPAs and a generic statement about online learning.
  We complement this result by proposing a novel solution concept of learning
in auctions, termed &quot;no-envy learning&quot;. This notion is founded on Walrasian
equilibrium, and we show that it is both efficiently computable and it results
in approximate efficiency in SiSPAs, even for bidders from the broad class of
XOS valuations (assuming demand oracle access to the valuations) or coverage
valuations (even without demand oracles). Our result can be viewed as the first
constant approximation for welfare maximization in combinatorial auctions with
XOS valuations, where both the designer and the agents are computationally
bounded. Our positive result for XOS valuations is based on a new class of
Follow-The-Perturbed-Leader algorithms and an analysis framework for general
online learning problems, which generalizes the existing framework of (Kalai
and Vempala 2005) beyond linear utilities. Our results provide a positive
counterpart to recent negative results on adversarial online learning via
best-response oracles (Hazan and Korren 2015). We show that these results are
of interest even outside auction settings, such as in security games of (Balcan
et al. 2015). Our efficient learning result for coverage valuations is based on
a novel use of convex rounding (Dughmi et al. 2011) and a reduction to online
convex optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01413</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01413</id><created>2015-11-04</created><authors><author><keyname>Liqat</keyname><forenames>Umer</forenames></author><author><keyname>Georgiou</keyname><forenames>Kyriakos</forenames></author><author><keyname>Kerrison</keyname><forenames>Steve</forenames></author><author><keyname>Lopez-Garcia</keyname><forenames>Pedro</forenames></author><author><keyname>Gallagher</keyname><forenames>John P.</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel V.</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>Inferring Parametric Energy Consumption Functions at Different Software
  Levels: ISA vs. LLVM IR</title><categories>cs.PL</categories><comments>22 pages, 4 figures, 2 tables</comments><acm-class>F.3.2; D.3.4; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The static estimation of the energy consumed by program executions is an
important challenge, which has applications in program optimization and
verification, and is instrumental in energy-aware software development. Our
objective is to estimate such energy consumption in the form of functions on
the input data sizes of programs. We have developed a tool for experimentation
with static analysis which infers such energy functions at two levels, the
instruction set architecture (ISA) and the intermediate code (LLVM IR) levels,
and reflects it upwards to the higher source code level. This required the
development of a translation from LLVM IR to an intermediate representation and
its integration with existing components, a translation from ISA to the same
representation, a resource analyzer, an ISA-level energy model, and a mapping
from this model to LLVM IR. The approach has been applied to programs written
in the XC language running on XCore architectures, but is general enough to be
applied to other languages. Experimental results show that our LLVM IR level
analysis is reasonably accurate (less than 6.4% average error vs. hardware
measurements) and more powerful than analysis at the ISA level. This paper
provides insights into the trade-off of precision versus analyzability at these
levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01419</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01419</id><created>2015-11-04</created><updated>2015-11-06</updated><authors><author><keyname>Meshi</keyname><forenames>Ofer</forenames></author><author><keyname>Mahdavi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Sontag</keyname><forenames>David</forenames></author></authors><title>On the Tightness of LP Relaxations for Structured Prediction</title><categories>stat.ML cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured prediction applications often involve complex inference problems
that require the use of approximate methods. Approximations based on linear
programming (LP) relaxations have proved particularly successful in this
setting, with both theoretical and empirical support. Despite the general
intractability of inference, it has been observed that in many real-world
applications the LP relaxation is often tight. In this work we propose a
theoretical explanation to this striking observation. In particular, we show
that learning with LP relaxed inference encourages tightness of training
instances. We complement this result with a generalization bound showing that
tightness generalizes from train to test data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01427</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01427</id><created>2015-11-04</created><authors><author><keyname>Carmantini</keyname><forenames>Giovanni S</forenames></author><author><keyname>Graben</keyname><forenames>Peter beim</forenames></author><author><keyname>Desroches</keyname><forenames>Mathieu</forenames></author><author><keyname>Rodrigues</keyname><forenames>Serafim</forenames></author></authors><title>Turing Computation with Recurrent Artificial Neural Networks</title><categories>cs.NE</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve the results by Siegelmann &amp; Sontag (1995) by providing a novel and
parsimonious constructive mapping between Turing Machines and Recurrent
Artificial Neural Networks, based on recent developments of Nonlinear Dynamical
Automata. The architecture of the resulting R-ANNs is simple and elegant,
stemming from its transparent relation with the underlying NDAs. These
characteristics yield promise for developments in machine learning methods and
symbolic computation with continuous time dynamical systems. A framework is
provided to directly program the R-ANNs from Turing Machine descriptions, in
absence of network training. At the same time, the network can potentially be
trained to perform algorithmic tasks, with exciting possibilities in the
integration of approaches akin to Google DeepMind's Neural Turing Machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01432</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01432</id><created>2015-11-04</created><authors><author><keyname>Dai</keyname><forenames>Andrew M.</forenames></author><author><keyname>Le</keyname><forenames>Quoc V.</forenames></author></authors><title>Semi-supervised Sequence Learning</title><categories>cs.LG cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two approaches that use unlabeled data to improve sequence
learning with recurrent networks. The first approach is to predict what comes
next in a sequence, which is a conventional language model in natural language
processing. The second approach is to use a sequence autoencoder, which reads
the input sequence into a vector and predicts the input sequence again. These
two algorithms can be used as a &quot;pretraining&quot; step for a later supervised
sequence learning algorithm. In other words, the parameters obtained from the
unsupervised step can be used as a starting point for other supervised training
models. In our experiments, we find that long short term memory recurrent
networks after being pretrained with the two approaches are more stable and
generalize better. With pretraining, we are able to train long short term
memory recurrent networks up to a few hundred timesteps, thereby achieving
strong performance in many text classification tasks, such as IMDB, DBpedia and
20 Newsgroups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01436</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01436</id><created>2015-11-04</created><authors><author><keyname>Fay</keyname><forenames>Damien</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Seto</keyname><forenames>Michael C.</forenames></author><author><keyname>Wang</keyname><forenames>Han</forenames></author><author><keyname>Kling</keyname><forenames>Christoph Carl</forenames></author></authors><title>An exploration of fetish social networks and communities</title><categories>cs.SI</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Social Networks (OSNs) provide a venue for virtual interactions and
relationships between individuals. In some communities, OSNs also facilitate
arranging online meetings and relationships. FetLife, the worlds largest
anonymous social network for the BDSM, fetish and kink communities, provides a
unique example of an OSN that serves as an interaction space, community
organizing tool, and sexual market. In this paper, we present a ?rst look at
the characteristics of European members of Fetlife, comprising 504,416
individual nodes with 1,912,196 connections. We looked at user characteristics
in terms of gender, sexual orientation, and preferred role. We further examined
the topological and structural properties of groups, as well as the type of
interactions and relations between their members. Our results suggest there are
important differences between the FetLife community and conventional OSNs. The
network can be characterised by complex gender based interactions both from a
sexual market and platonic viewpoint which point to a truly fascinating social
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01438</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01438</id><created>2015-11-04</created><authors><author><keyname>Martinsen</keyname><forenames>Thor</forenames></author><author><keyname>Meidl</keyname><forenames>Wilfried</forenames></author><author><keyname>Stanica</keyname><forenames>Pantelimon</forenames></author></authors><title>Generalized bent functions and their Gray images</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove that generalized bent (gbent) functions defined on
$\mathbb{Z}_2^n$ with values in $\mathbb{Z}_{2^k}$ are regular, and find
connections between the (generalized) Walsh spectrum of these functions and
their components. We comprehensively characterize generalized bent and semibent
functions with values in $\mathbb{Z}_{16}$, which extends earlier results on
gbent functions with values in $\mathbb{Z}_4$ and $\mathbb{Z}_8$. We also show
that the Gray images of gbent functions with values in $\mathbb{Z}_{2^k}$ are
semibent/plateaued when $k=3,4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01440</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01440</id><created>2015-11-04</created><authors><author><keyname>Yang</keyname><forenames>Jianxiao</forenames><affiliation>ENSTA ParisTech U2IS/IS</affiliation></author><author><keyname>Kai</keyname><forenames>Wan</forenames><affiliation>L2S</affiliation></author><author><keyname>Geller</keyname><forenames>Benoit</forenames><affiliation>ENSTA ParisTech U2IS/IS</affiliation></author><author><keyname>Nour</keyname><forenames>Charbel Abdel</forenames><affiliation>ELEC</affiliation></author><author><keyname>Rioul</keyname><forenames>Olivier</forenames><affiliation>ELEC</affiliation></author><author><keyname>Douillard</keyname><forenames>Catherine</forenames><affiliation>ELEC</affiliation></author></authors><title>A low-complexity 2D signal space diversity solution for future
  broadcasting systems</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><doi>10.1109/ICC.2015.7248744</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  -DVB-T2 was the first industrial standard deploying rotated and cyclic Q
delayed (RCQD)modulation to improve performance over fading channels. This
enablesimportantgains compared toconventional quadrature amplitude
modulations(QAM) under severe channel conditions.However, the corresponding
demodulation complexitystill prevents its use forwider applications. This paper
proposes several rotation angles for different QAM constellations anda
corresponding low-complexity detection method. Results show that the proposed
solution simplifies both the transmitter and the receiver with often
betterperformancethan the proposed angles in DVB-T2. Compared with the lowest
complexity demappers currently used in DVB-T2, the proposed solution achieves
an additional reduction bymore than 60%. Index Terms- DVB-T2, Rotated and
Cyclic Q Delayed (RCQD) Modulations, Signal Space Diversity (SSD), Fading
Channel, Quadrature Amplitude Modulations (QAM), Max-Log,
ComputationalComplexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01442</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01442</id><created>2015-11-04</created><updated>2015-12-24</updated><authors><author><keyname>Rabusseau</keyname><forenames>Guillaume</forenames></author><author><keyname>Balle</keyname><forenames>Borja</forenames></author><author><keyname>Cohen</keyname><forenames>Shay B.</forenames></author></authors><title>Low-Rank Approximation of Weighted Tree Automata</title><categories>cs.LG cs.FL</categories><comments>To appear in AISTATS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a technique to minimize weighted tree automata (WTA), a powerful
formalisms that subsumes probabilistic context-free grammars (PCFGs) and
latent-variable PCFGs. Our method relies on a singular value decomposition of
the underlying Hankel matrix defined by the WTA. Our main theoretical result is
an efficient algorithm for computing the SVD of an infinite Hankel matrix
implicitly represented as a WTA. We provide an analysis of the approximation
error induced by the minimization, and we evaluate our method on real-world
data originating in newswire treebank. We show that the model achieves lower
perplexity than previous methods for PCFG minimization, and also is much more
stable due to the absence of local optima.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01443</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01443</id><created>2015-11-04</created><updated>2015-11-10</updated><authors><author><keyname>Huang</keyname><forenames>Cheng</forenames></author><author><keyname>Huo</keyname><forenames>Xiaoming</forenames></author></authors><title>A Distributed One-Step Estimator</title><categories>stat.ME cs.DC stat.ML</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed statistical inference has recently attracted enormous attention.
Many existing work focuses on the averaging estimator. We propose a one-step
approach to enhance a simple-averaging based distributed estimator. We derive
the corresponding asymptotic properties of the newly proposed estimator. We
find that the proposed one-step estimator enjoys the same asymptotic properties
as the centralized estimator. The proposed one-step approach merely requires
one additional round of communication in relative to the averaging estimator;
so the extra communication burden is insignificant. In finite sample cases,
numerical examples show that the proposed estimator outperforms the simple
averaging estimator with a large margin in terms of the mean squared errors. A
potential application of the one-step approach is that one can use multiple
machines to speed up large scale statistical inference with little compromise
in the quality of estimators. The proposed method becomes more valuable when
data can only be available at distributed machines with limited communication
bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01446</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01446</id><created>2015-11-04</created><updated>2015-11-05</updated><authors><author><keyname>Soualhia</keyname><forenames>Mbarka</forenames></author><author><keyname>Khomh</keyname><forenames>Foutse</forenames></author><author><keyname>Tahar</keyname><forenames>Sofiene</forenames></author></authors><title>ATLAS: An Adaptive Failure-aware Scheduler for Hadoop</title><categories>cs.DC</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hadoop has become the de facto standard for processing large data in today's
cloud environment. The performance of Hadoop in the cloud has a direct impact
on many important applications ranging from web analytic, web indexing, image
and document processing to high-performance scientific?c computing. However,
because of the scale, complexity and dynamic nature of the cloud, failures are
common and these failures often impact the performance of jobs running in
Hadoop. Although Hadoop possesses built-in failure detection and recovery
mechanisms, several scheduled jobs still fail because of unforeseen events in
the cloud environment. A single task failure can cause the failure of the whole
job and unpredictable job running times. In this report, we propose ATLAS
(AdapTive faiLure-Aware Scheduler), a new scheduler for Hadoop that can adapt
its scheduling decisions to events occurring in the cloud environment. Using
statistical models, ATLAS predicts task failures and adjusts its scheduling
decisions on the fly to reduce task failure occurrences. We implement ATLAS in
the Hadoop framework of Amazon Elastic MapReduce (EMR) and perform a case study
to compare its performance with those of the FIFO, Fair and Capacity
schedulers. Results show that ATLAS can reduce the percentage of failed jobs by
up to 28% and the percentage of failed tasks by up to 39%, and the total
execution time of jobs by 10 minutes on average. ATLAS also reduces CPU and
memory usages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01449</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01449</id><created>2015-11-03</created><authors><author><keyname>Anedda</keyname><forenames>Matteo</forenames></author><author><keyname>Meloni</keyname><forenames>Alessio</forenames></author><author><keyname>Murroni</keyname><forenames>Maurizio</forenames></author></authors><title>64-APSK Constellation and Mapping Optimization for Satellite
  Broadcasting Using Genetic Algorithms</title><categories>cs.IT math.IT</categories><doi>10.1109/TBC.2015.2470134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DVB-S2 and DVB-SH satellite broadcasting standards currently deploy 16- and
32-amplitude phase shift keying (APSK) modulation using the consultative
committee for space data systems (CCSDS) mapping. Such standards also include
hierarchical modulation as a mean to provide unequal error protection in highly
variable channels over satellite. Foreseeing the increasing need for higher
data rates, this paper tackles the optimization of 64-APSK constellations to
minimize the mean square error between the original and received symbol.
Optimization is performed according to the sensitivity of the data to the
channel errors, by means of genetic algorithms, a well-known technique
currently used in a variety of application domains, when close form solutions
are impractical. Test results show that through non-uniform constellation and
asymmetric symbol mapping, it is possible to significantly reduce the
distortion while preserving bandwidth efficiency. Tests performed on real
signals based on perceptual quality measurements allow validating the proposed
scheme against conventional 64-APSK constellations and CCSDS mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01473</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01473</id><created>2015-11-04</created><authors><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author><author><keyname>Perry</keyname><forenames>William</forenames></author><author><keyname>Wein</keyname><forenames>Alexander S.</forenames></author></authors><title>How Robust are Reconstruction Thresholds for Community Detection?</title><categories>cs.DS cs.IT cs.LG math.IT math.PR stat.ML</categories><comments>35 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic block model is one of the oldest and most ubiquitous models
for studying clustering and community detection. In an exciting sequence of
developments, motivated by deep but non-rigorous ideas from statistical
physics, Decelle et al. conjectured a sharp threshold for when community
detection is possible in the sparse regime. Mossel, Neeman and Sly and
Massouli\'e proved the conjecture and gave matching algorithms and lower
bounds.
  Here we revisit the stochastic block model from the perspective of semirandom
models where we allow an adversary to make `helpful' changes that strengthen
ties within each community and break ties between them. We show a surprising
result that these `helpful' changes can shift the information-theoretic
threshold, making the community detection problem strictly harder. We
complement this by showing that algorithms based on semidefinite programming
(which were known to get close to the threshold) continue to work in the
semirandom model (even for partial recovery). Thus algorithms based on
semidefinite programming are robust in ways that any algorithm meeting the
information-theoretic threshold for community detection cannot be.
  These results point to an interesting new direction: Can we find robust,
semirandom analogues to some of the classical, average-case thresholds in
statistics? We also explore this question in the broadcast tree model, and as a
bonus we show that the viewpoint of semirandom models can help explain why some
algorithms are preferred to others in practice, in spite of the gaps in their
statistical performance on random models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01480</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01480</id><created>2015-11-04</created><authors><author><keyname>Naldi</keyname><forenames>Maurizio</forenames></author></authors><title>Approximation of the truncated Zeta distribution and Zipf's law</title><categories>stat.AP cs.CL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zipf's law appears in many application areas but does not have a closed form
expression, which may make its use cumbersome. Since it coincides with the
truncated version of the Zeta distribution, in this paper we propose three
approximate closed form expressions for the truncated Zeta distribution, which
may be employed for Zipf's law as well. The three approximations are based on
the replacement of the sum occurring in Zipf's law with an integral, and are
named respectively the integral approximation, the average integral
approximation, and the trapezoidal approximation. While the first one is shown
to be of little use, the trapezoidal approximation exhibits an error which is
typically lower than 1\%, but is as low as 0.1\% for the range of values of the
Zipf parameter below 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01508</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01508</id><created>2015-11-04</created><authors><author><keyname>Poling</keyname><forenames>Bryan</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Enhancing Feature Tracking With Gyro Regularization</title><categories>cs.CV</categories><comments>Preprint submitted to Image and Vision Computing</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deeply integrated method of exploiting low-cost gyroscopes to
improve general purpose feature tracking. Most previous methods use gyroscopes
to initialize and bound the search for features. In contrast, we use them to
regularize the tracking energy function so that they can directly assist in the
tracking of ambiguous and poor-quality features. We demonstrate that our simple
technique offers significant improvements in performance over conventional
template-based tracking methods, and is in fact competitive with more complex
and computationally expensive state-of-the-art trackers, but at a fraction of
the computational cost. Additionally, we show that the practice of initializing
template-based feature trackers like KLT (Kanade-Lucas-Tomasi) using
gyro-predicted optical flow offers no advantage over using a careful
optical-only initialization method, suggesting that some deeper level of
integration, like the method we propose, is needed in order to realize a
genuine improvement in tracking performance from these inertial sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01512</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01512</id><created>2015-11-04</created><authors><author><keyname>Bacry</keyname><forenames>Emmanuel</forenames></author><author><keyname>Ga&#xef;ffas</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Mastromatteo</keyname><forenames>Iacopo</forenames></author><author><keyname>Muzy</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Mean-field inference of Hawkes point processes</title><categories>cs.LG cond-mat.stat-mech</categories><comments>29 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a fast and efficient estimation method that is able to accurately
recover the parameters of a d-dimensional Hawkes point-process from a set of
observations. We exploit a mean-field approximation that is valid when the
fluctuations of the stochastic intensity are small. We show that this is
notably the case in situations when interactions are sufficiently weak, when
the dimension of the system is high or when the fluctuations are self-averaging
due to the large number of past events they involve. In such a regime the
estimation of a Hawkes process can be mapped on a least-squares problem for
which we provide an analytic solution. Though this estimator is biased, we show
that its precision can be comparable to the one of the Maximum Likelihood
Estimator while its computation speed is shown to be improved considerably. We
give a theoretical control on the accuracy of our new approach and illustrate
its efficiency using synthetic datasets, in order to assess the statistical
estimation error of the parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01513</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01513</id><created>2015-11-04</created><authors><author><keyname>Kliesch</keyname><forenames>Martin</forenames></author><author><keyname>Kueng</keyname><forenames>Richard</forenames></author><author><keyname>Eisert</keyname><forenames>Jens</forenames></author><author><keyname>Gross</keyname><forenames>David</forenames></author></authors><title>Improving compressed sensing with the diamond norm</title><categories>cs.IT math.IT quant-ph</categories><comments>24 pages + Appendix, 7 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In low-rank matrix recovery, one aims to reconstruct a low-rank matrix from a
minimal number of linear measurements. Within the paradigm of compressed
sensing, this is made computationally efficient by minimizing the nuclear norm
as a convex surrogate for rank. In this work, we identify an improved
regularizer based on the so-called diamond norm, a concept imported from
quantum information theory. We show that - for a class of matrices saturating a
certain norm inequality - the descent cone of the diamond norm is contained in
that of the nuclear norm. This suggests superior reconstruction properties for
these matrices and we explicitly characterize this set. We demonstrate
numerically that the diamond norm indeed outperforms the nuclear norm in a
number of relevant applications: These include signal analysis tasks such as
blind matrix deconvolution or the retrieval of certain unitary basis changes,
as well as the quantum information problem of process tomography with random
measurements. The diamond norm is defined for matrices that can be interpreted
as order-4 tensors and it turns out that the above condition depends crucially
on that tensorial structure. In this sense, this work touches on an aspect of
the notoriously difficult tensor completion problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01514</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01514</id><created>2015-11-04</created><authors><author><keyname>Chuat</keyname><forenames>Laurent</forenames></author><author><keyname>Szalachowski</keyname><forenames>Pawel</forenames></author><author><keyname>Perrig</keyname><forenames>Adrian</forenames></author><author><keyname>Laurie</keyname><forenames>Ben</forenames></author><author><keyname>Messeri</keyname><forenames>Eran</forenames></author></authors><title>Efficient Gossip Protocols for Verifying the Consistency of Certificate
  Logs</title><categories>cs.CR</categories><comments>9 pages, 5 figures</comments><doi>10.1109/CNS.2015.7346853</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The level of trust accorded to certification authorities has been decreasing
over the last few years as several cases of misbehavior and compromise have
been observed. Log-based approaches, such as Certificate Transparency, ensure
that fraudulent TLS certificates become publicly visible. However, a key
element that log-based approaches still lack is a way for clients to verify
that the log behaves in a consistent and honest manner. This task is
challenging due to privacy, efficiency, and deployability reasons. In this
paper, we propose the first (to the best of our knowledge) gossip protocols
that enable the detection of log inconsistencies. We analyze these protocols
and present the results of a simulation based on real Internet traffic traces.
We also give a deployment plan, discuss technical issues, and present an
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01523</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01523</id><created>2015-11-04</created><authors><author><keyname>Manshaei</keyname><forenames>Kasra</forenames></author><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author></authors><title>SGPD Volume Maximization for Community Detection</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we briefly study the feasibility of community detection in
complex networks using peripheral vertices. Our method suggests a novel
direction in axiomizing the problem of clustering in graphs and complex
networks by looking at the topological role each vertex plays in the community
structure, regardless of the attributes. The promising strength of
pseudo-peripheral vertices as a lever for analysis of complex networks is also
demonstrated on real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01534</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01534</id><created>2015-11-04</created><authors><author><keyname>Valluri</keyname><forenames>Abhijit Kiran</forenames></author></authors><title>Equilibrium Properties of Rate Control Protocols</title><categories>cs.NI cs.SY</categories><comments>8 pages, 4 figures, submitted to ICITST 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the stability of the Rate Control Protocol (RCP) using two
different models that have been proposed in literature. Our objective is to
better understand the impact of the protocol parameters and the effect
different forms of feedback have on the stability of the network. We also
highlight that different time scales, depending on the propagation delay
relative to the queuing delay, have an impact on the nonlinear and the
stochastic properties of the protocol fluid models. To better understand some
of the nonlinear properties, we resort to local bifurcation analysis where we
exhibit the existence of a Hopf type bifurcation that then leads to stable
limit cycles. Our work serves as a step towards a more comprehensive
understanding of the nonlinear fluid models that have been used as
representative models for RCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01535</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01535</id><created>2015-11-04</created><authors><author><keyname>Jose</keyname><forenames>Jubin</forenames></author><author><keyname>Li</keyname><forenames>Chong</forenames></author><author><keyname>Wu</keyname><forenames>Xinzhou</forenames></author><author><keyname>Ying</keyname><forenames>Lei</forenames></author><author><keyname>Zhu</keyname><forenames>Kai</forenames></author></authors><title>Distributed Rate and Power Control in Vehicular Networks</title><categories>cs.SY cs.NI</categories><comments>Submitted to IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is on the rate and power control algorithms in
Dedicated Short Range Communication (DSRC) for vehicular networks. We first
propose a utility maximization framework by leveraging the well-developed
network congestion control, and formulate two subproblems, one on rate control
with fixed transmit powers and the other on power control with fixed rates.
Distributed rate control and power control algorithms are developed to solve
these two subproblems, respectively, and are proved to be asymptotically
optimal. Joint rate and power control can be done by using the two algorithms
in an alternating fashion. The performance enhancement of our algorithms
compared with a recent rate control algorithm, called EMBARC, is evaluated by
using the network simulator ns2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01538</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01538</id><created>2015-11-04</created><authors><author><keyname>Stamatescu</keyname><forenames>Grigore</forenames></author></authors><title>Achieving Sensor Fusion for Collaborative Multi-level Monitoring of
  Pipeline Infrastructures</title><categories>cs.SY</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large scale monitoring systems enable efficient field level data collection
at high temporal and spatial resolutions. One example is the deployment of such
systems in pipeline infrastructure applications which have to be monitored for
leaks and protected from unauthorized access, with the potential of causing
significant environmental and economic damage. The paper discusses a
multi-level system architecture for data collection and processing based on the
collaborative integration of wireless sensor networks and unmanned aerial
vehicles. Three sensor fusion methods: Kalman filtering, Fuzzy Sensor
Validation and Fusion (FUSVAF) and Consensus-based processing, are considered
for intelligent data reduction and situational awareness while alleviating
communication bottlenecks across the multi-level network. Simulation and
experimental results are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01540</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01540</id><created>2015-11-04</created><updated>2016-01-17</updated><authors><author><keyname>Kheirkhahzadeh</keyname><forenames>Masoumeh</forenames></author><author><keyname>Lancichinetti</keyname><forenames>Andrea</forenames></author><author><keyname>Rosvall</keyname><forenames>Martin</forenames></author></authors><title>Efficient community detection of network flows for varying Markov times
  and bipartite networks</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages, 4 figures, and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection of network flows conventionally assumes one-step dynamics
on the links. For sparse networks and interest in large-scale structures,
longer timescales may be more appropriate. Oppositely, for large networks and
interest in small-scale structures, shorter timescales may be better. However,
current methods for analyzing networks at different timescales require
expensive and often infeasible network reconstructions. To overcome this
problem, we introduce a method that takes advantage of the inner-workings of
the map equation and evades the reconstruction step. This makes it possible to
efficiently analyze large networks at different Markov times with no extra
overhead cost. The method also evades the costly unipartite projection for
identifying flow modules in bipartite networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01543</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01543</id><created>2015-11-04</created><authors><author><keyname>Chiuso</keyname><forenames>A.</forenames></author></authors><title>Regularization and Bayesian Learning in Dynamical Systems: Past, Present
  and Future</title><categories>cs.SY stat.ML</categories><comments>Plenary Presentation at the IFAC SYSID 2015. Submitted to Annual
  Reviews in Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regularization and Bayesian methods for system identification have been
repopularized in the recent years, and proved to be competitive w.r.t.
classical parametric approaches. In this paper we shall make an attempt to
illustrate how the use of regularization in system identification has evolved
over the years, starting from the early contributions both in the Automatic
Control as well as Econometrics and Statistics literature. In particular we
shall discuss some fundamental issues such as compound estimation problems and
exchangeability which play and important role in regularization and Bayesian
approaches, as also illustrated in early publications in Statistics. The
historical and foundational issues will be given more emphasis (and space), at
the expense of the more recent developments which are only briefly discussed.
The main reason for such a choice is that, while the recent literature is
readily available, and surveys have already been published on the subject, in
the author's opinion a clear link with past work had not been completely
clarified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01545</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01545</id><created>2015-11-04</created><authors><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>Ranking scientists</title><categories>cs.DL physics.soc-ph</categories><comments>3 pages, 2 figures, the original version</comments><journal-ref>Nature Physics 11, 882 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently the ranking of scientists is based on the $h$-index, which is
widely perceived as an imprecise and simplistic though still useful metric. We
find that the $h$-index actually favours modestly performing researchers and
propose a simple criterion for proper ranking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01549</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01549</id><created>2015-11-04</created><updated>2016-01-04</updated><authors><author><keyname>Horlemann-Trautmann</keyname><forenames>Anna-Lena</forenames></author><author><keyname>Marshall</keyname><forenames>Kyle</forenames></author><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author></authors><title>Extension of Overbeck's Attack for Gabidulin Based Cryptosystems</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new attack against cryptosystems based on the rank metric. Our
attack allows us to cryptanalyze two variants of the GPT cryptosystem which
were designed to resist the attack of Overbeck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01556</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01556</id><created>2015-11-04</created><authors><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author><author><keyname>Huang</keyname><forenames>Chih-Kai</forenames></author><author><keyname>Wang</keyname><forenames>Hongsu</forenames></author><author><keyname>Bol</keyname><forenames>Peter K.</forenames></author></authors><title>Mining Local Gazetteers of Literary Chinese with CRF and Pattern based
  Methods for Biographical Information in Chinese History</title><categories>cs.CL cs.DL cs.IR cs.LG</categories><comments>11 pages, 5 figures, 5 tables, the Third Workshop on Big Humanities
  Data (2015 IEEE BigData), the 29th Pacific Asia Conference on Language,
  Information and Computation (PACLIC 29)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Person names and location names are essential building blocks for identifying
events and social networks in historical documents that were written in
literary Chinese. We take the lead to explore the research on algorithmically
recognizing named entities in literary Chinese for historical studies with
language-model based and conditional-random-field based methods, and extend our
work to mining the document structures in historical documents. Practical
evaluations were conducted with texts that were extracted from more than 220
volumes of local gazetteers (Difangzhi). Difangzhi is a huge and the single
most important collection that contains information about officers who served
in local government in Chinese history. Our methods performed very well on
these realistic tests. Thousands of names and addresses were identified from
the texts. A good portion of the extracted names match the biographical
information currently recorded in the China Biographical Database (CBDB) of
Harvard University, and many others can be verified by historians and will
become as new additions to CBDB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01558</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01558</id><created>2015-11-04</created><authors><author><keyname>Kovchegov</keyname><forenames>Yevgeniy</forenames></author><author><keyname>Zaliapin</keyname><forenames>Ilya</forenames></author></authors><title>Horton Law in Self-Similar Trees</title><categories>cs.DM math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-similarity of random trees is related to the operation of pruning.
Pruning $R$ cuts the leaves and their parental edges and removes the resulting
chains of degree-two nodes from a finite tree. A Horton-Strahler order of a
vertex $v$ and its parental edge is defined as the minimal number of prunings
necessary to eliminate the subtree rooted at $v$. A branch is a group of
neighboring vertices and edges of the same order. The Horton numbers $N_k[K]$
and $N_{ij}[K]$ are defined as the expected number of branches of order $k$,
and the expected number of order-$i$ branches that merged order-$j$ branches,
$j&gt;i$, respectively, in a finite tree of order $K$. The Tokunaga coefficients
are defined as $T_{ij}[K]=N_{ij}[K]/N_j[K]$. The pruning decreases the orders
of tree vertices by unity. A rooted full binary tree is said to be
mean-self-similar if its Tokunaga coefficients are invariant with respect to
pruning: $T_k:=T_{i,i+k}[K]$. We show that for self-similar trees, the
condition $\limsup(T_k)^{1/k}&lt;\infty$ is necessary and sufficient for the
existence of the strong Horton law: $N_k[K]/N_1[K] \rightarrow R^{1-k}$, as $K
\rightarrow \infty$ for some $R&gt;0$ and every $k\geq 1$. This work is a step
toward providing rigorous foundations for the Horton law that, being
omnipresent in natural branching systems, has escaped so far a formal
explanation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01559</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01559</id><created>2015-11-04</created><authors><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author><author><keyname>Wang</keyname><forenames>Hongsu</forenames></author><author><keyname>Cheng</keyname><forenames>Wen-Huei</forenames></author><author><keyname>Hsu</keyname><forenames>Chu-Ting</forenames></author><author><keyname>Chiu</keyname><forenames>Wei-Yun</forenames></author></authors><title>Color Aesthetics and Social Networks in Complete Tang Poems:
  Explorations and Discoveries</title><categories>cs.CL cs.DL cs.IR</categories><comments>10 pages, 1 figure, 8 tables, The 29th Pacific Asia Conference on
  Language, Information and Computation (PACLIC 29), The 27th Conference on
  Computational Linguistics and Speech Analysis (ROCLING XXVII, Chinese
  version)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Complete Tang Poems (CTP) is the most important source to study Tang
poems. We look into CTP with computational tools from specific linguistic
perspectives, including distributional semantics and collocational analysis.
From such quantitative viewpoints, we compare the usage of &quot;wind&quot; and &quot;moon&quot; in
the poems of Li Bai and Du Fu. Colors in poems function like sounds in movies,
and play a crucial role in the imageries of poems. Thus, words for colors are
studied, and &quot;white&quot; is the main focus because it is the most frequent color in
CTP. We also explore some cases of using colored words in antithesis pairs that
were central for fostering the imageries of the poems. CTP also contains useful
historical information, and we extract person names in CTP to study the social
networks of the Tang poets. Such information can then be integrated with the
China Biographical Database of Harvard University.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01561</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01561</id><created>2015-11-04</created><authors><author><keyname>M&#xfc;ller</keyname><forenames>Andreas</forenames></author><author><keyname>Kopera</keyname><forenames>Michal A.</forenames></author><author><keyname>Marras</keyname><forenames>Simone</forenames></author><author><keyname>Wilcox</keyname><forenames>Lucas C.</forenames></author><author><keyname>Isaac</keyname><forenames>Tobin</forenames></author><author><keyname>Giraldo</keyname><forenames>Francis X.</forenames></author></authors><title>Strong Scaling for Numerical Weather Prediction at Petascale with the
  Atmospheric Model NUMA</title><categories>cs.DC cs.SE physics.ao-ph physics.flu-dyn physics.geo-ph</categories><comments>10 pages, 8 figures, submitted to 30th IEEE International Parallel &amp;
  Distributed Processing Symposium 2016</comments><acm-class>D.2.8; G.1.8; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical weather prediction (NWP) has proven to be computationally
challenging due to its inherent multiscale nature. Currently, the highest
resolution NWP models use a horizontal resolution of approximately 15km. At
this resolution many important processes in the atmosphere are not resolved.
Needless to say this introduces errors. In order to increase the resolution of
NWP models highly scalable atmospheric models are needed.
  The Non-hydrostatic Unified Model of the Atmosphere (NUMA), developed by the
authors at the Naval Postgraduate School, was designed to achieve this purpose.
NUMA is used by the Naval Research Laboratory, Monterey as the engine inside
its next generation weather prediction system NEPTUNE. NUMA solves the fully
compressible Navier-Stokes equations by means of high-order Galerkin methods
(both spectral element as well as discontinuous Galerkin methods can be used).
Mesh generation is done using the p4est library. NUMA is capable of running
middle and upper atmosphere simulations since it does not make use of the
shallow-atmosphere approximation.
  This paper presents the performance analysis and optimization of the spectral
element version of NUMA. The performance at different optimization stages is
analyzed. By using vector intrinsics the main computations reach 1.2 PFlops on
the entire machine Mira. The paper also presents scalability studies for two
idealized test cases that are relevant for NWP applications. The atmospheric
model NUMA delivers an excellent strong scaling efficiency of 99% on the entire
supercomputer Mira using a mesh with 1.8 billion grid points. This allows us to
run a global forecast of a baroclinic wave test case at 3km uniform horizontal
resolution and double precision within the time frame required for operational
weather prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01566</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01566</id><created>2015-11-04</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Horsman</keyname><forenames>Dominic</forenames><affiliation>University of Oxford</affiliation></author></authors><title>DEMONIC programming: a computational language for single-particle
  equilibrium thermodynamics, and its formal semantics</title><categories>cs.LO cond-mat.stat-mech cs.PL quant-ph</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181. Dominic Horsman published
  previously as Clare Horsman</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 1-16</journal-ref><doi>10.4204/EPTCS.195.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maxwell's Demon, 'a being whose faculties are so sharpened that he can follow
every molecule in its course', has been the centre of much debate about its
abilities to violate the second law of thermodynamics. Landauer's hypothesis,
that the Demon must erase its memory and incur a thermodynamic cost, has become
the standard response to Maxwell's dilemma, and its implications for the
thermodynamics of computation reach into many areas of quantum and classical
computing. It remains, however, still a hypothesis. Debate has often centred
around simple toy models of a single particle in a box. Despite their
simplicity, the ability of these systems to accurately represent thermodynamics
(specifically to satisfy the second law) and whether or not they display
Landauer Erasure, has been a matter of ongoing argument. The recent
Norton-Ladyman controversy is one such example.
  In this paper we introduce a programming language to describe these simple
thermodynamic processes, and give a formal operational semantics and program
logic as a basis for formal reasoning about thermodynamic systems. We formalise
the basic single-particle operations as statements in the language, and then
show that the second law must be satisfied by any composition of these basic
operations. This is done by finding a computational invariant of the system. We
show, furthermore, that this invariant requires an erasure cost to exist within
the system, equal to kTln2 for a bit of information: Landauer Erasure becomes a
theorem of the formal system. The Norton-Ladyman controversy can therefore be
resolved in a rigorous fashion, and moreover the formalism we introduce gives a
set of reasoning tools for further analysis of Landauer erasure, which are
provably consistent with the second law of thermodynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01567</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01567</id><created>2015-11-04</created><authors><author><keyname>B&#x103;descu</keyname><forenames>Costin</forenames></author><author><keyname>Panangaden</keyname><forenames>Prakash</forenames></author></authors><title>Quantum Alternation: Prospects and Problems</title><categories>cs.PL quant-ph</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 33-42</journal-ref><doi>10.4204/EPTCS.195.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a notion of quantum control in a quantum programming language
which permits the superposition of finitely many quantum operations without
performing a measurement. This notion takes the form of a conditional construct
similar to the IF statement in classical programming languages. We show that
adding such a quantum IF statement to the QPL programming language simplifies
the presentation of several quantum algorithms. This motivates the possibility
of extending the denotational semantics of QPL to include this form of quantum
alternation. We give a denotational semantics for this extension of QPL based
on Kraus decompositions rather than on superoperators. Finally, we clarify the
relation between quantum alternation and recursion, and discuss the possibility
of lifting the semantics defined by Kraus operators to the superoperator
semantics defined by Selinger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01568</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01568</id><created>2015-11-04</created><authors><author><keyname>Boender</keyname><forenames>Jaap</forenames><affiliation>Middlesex University</affiliation></author><author><keyname>Kamm&#xfc;ller</keyname><forenames>Florian</forenames><affiliation>Middlesex University</affiliation></author><author><keyname>Nagarajan</keyname><forenames>Rajagopal</forenames><affiliation>Middlesex University</affiliation></author></authors><title>Formalization of Quantum Protocols using Coq</title><categories>cs.LO cs.CR</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 71-83</journal-ref><doi>10.4204/EPTCS.195.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum Information Processing, which is an exciting area of research at the
intersection of physics and computer science, has great potential for
influencing the future development of information processing systems. The
building of practical, general purpose Quantum Computers may be some years into
the future. However, Quantum Communication and Quantum Cryptography are well
developed. Commercial Quantum Key Distribution systems are easily available and
several QKD networks have been built in various parts of the world. The
security of the protocols used in these implementations rely on
information-theoretic proofs, which may or may not reflect actual system
behaviour. Moreover, testing of implementations cannot guarantee the absence of
bugs and errors. This paper presents a novel framework for modelling and
verifying quantum protocols and their implementations using the proof assistant
Coq. We provide a Coq library for quantum bits (qubits), quantum gates, and
quantum measurement. As a step towards verifying practical quantum
communication and security protocols such as Quantum Key Distribution, we
support multiple qubits, communication and entanglement. We illustrate these
concepts by modelling the Quantum Teleportation Protocol, which communicates
the state of an unknown quantum bit using only a classical channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01569</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01569</id><created>2015-11-04</created><authors><author><keyname>Cho</keyname><forenames>Kenta</forenames><affiliation>Radboud University, Nijmegen</affiliation></author></authors><title>Total and Partial Computation in Categorical Quantum Foundations</title><categories>cs.LO math.CT quant-ph</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 116-135</journal-ref><doi>10.4204/EPTCS.195.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uncovers the fundamental relationship between total and partial
computation in the form of an equivalence of certain categories. This
equivalence involves on the one hand effectuses, which are categories for total
computation, introduced by Jacobs for the study of quantum/effect logic. On the
other hand, it involves what we call FinPACs with effects; they are finitely
partially additive categories equipped with effect algebra structures, serving
as categories for partial computation. It turns out that the Kleisli category
of the lift monad (-)+1 on an effectus is always a FinPAC with effects, and
this construction gives rise to the equivalence. Additionally, state-and-effect
triangles over FinPACs with effects are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01570</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01570</id><created>2015-11-04</created><authors><author><keyname>Cho</keyname><forenames>Kenta</forenames></author><author><keyname>Jacobs</keyname><forenames>Bart</forenames></author><author><keyname>Westerbaan</keyname><forenames>Bas</forenames></author><author><keyname>Westerbaan</keyname><forenames>Bram</forenames></author></authors><title>Quotient-Comprehension Chains</title><categories>cs.LO</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 136-147</journal-ref><doi>10.4204/EPTCS.195.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quotients and comprehension are fundamental mathematical constructions that
can be described via adjunctions in categorical logic. This paper reveals that
quotients and comprehension are related to measurement, not only in quantum
logic, but also in probabilistic and classical logic. This relation is
presented by a long series of examples, some of them easy, and some also highly
non-trivial (esp. for von Neumann algebras). We have not yet identified a
unifying theory. Nevertheless, the paper contributes towards such a theory by
introducing the new quotient-and-comprehension perspective on measurement
instruments, and by describing the examples on which such a theory should be
built.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01572</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01572</id><created>2015-11-04</created><authors><author><keyname>Honda</keyname><forenames>Kentaro</forenames></author></authors><title>Analysis of Quantum Entanglement in Quantum Programs using Stabilizer
  Formalism</title><categories>quant-ph cs.PL</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 262-272</journal-ref><doi>10.4204/EPTCS.195.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum entanglement plays an important role in quantum computation and
communication. It is necessary for many protocols and computations, but causes
unexpected disturbance of computational states. Hence, static analysis of
quantum entanglement in quantum programs is necessary. Several papers studied
the problem. They decided qubits were entangled if multiple qubits unitary
gates are applied to them, and some refined this reasoning using information
about the state of each separated qubit. However, they do not care about the
fact that unitary gate undoes entanglement and that measurement may separate
multiple qubits. In this paper, we extend prior work using stabilizer
formalism. It refines reasoning about separability of quantum variables in
quantum programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01573</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01573</id><created>2015-11-04</created><authors><author><keyname>Quick</keyname><forenames>David</forenames></author></authors><title>Encoding !-tensors as !-graphs with neighbourhood orders</title><categories>cs.LO</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 307-320</journal-ref><doi>10.4204/EPTCS.195.23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diagrammatic reasoning using string diagrams provides an intuitive language
for reasoning about morphisms in a symmetric monoidal category. To allow
working with infinite families of string diagrams, !-graphs were introduced as
a method to mark repeated structure inside a diagram. This led to !-graphs
being implemented in the diagrammatic proof assistant Quantomatic. Having a
partially automated program for rewriting diagrams has proven very useful, but
being based on !-graphs, only commutative theories are allowed. An enriched
abstract tensor notation, called !-tensors, has been used to formalise the
notion of !-boxes in non-commutative structures. This work-in-progress paper
presents a method to encode !-tensors as !-graphs with some additional
structure. This will allow us to leverage the existing code from Quantomatic
and quickly provide various tools for non-commutative diagrammatic reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01574</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01574</id><created>2015-11-04</created><updated>2016-02-22</updated><authors><author><keyname>Chelba</keyname><forenames>Ciprian</forenames></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames></author></authors><title>Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix
  Language Model</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe Sparse Non-negative Matrix (SNM) language model estimation using
multinomial loss on held-out data.
  Being able to train on held-out data is important in practical situations
where the training data is usually mismatched from the held-out/test data. It
is also less constrained than the previous training algorithm using
leave-one-out on training data: it allows the use of richer meta-features in
the adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing
which would be difficult to deal with correctly in leave-one-out training.
  In experiments on the one billion words language modeling benchmark, we are
able to slightly improve on our previous results which use a different loss
function, and employ leave-one-out training on a subset of the main training
set. Surprisingly, an adjustment model with meta-features that discard all
lexical information can perform as well as lexicalized meta-features. We find
that fairly small amounts of held-out data (on the order of 30-70 thousand
words) are sufficient for training the adjustment model.
  In a real-life scenario where the training data is a mix of data sources that
are imbalanced in size, and of different degrees of relevance to the held-out
and test data, taking into account the data source for a given skip-/n-gram
feature and combining them for best performance on held-out/test data improves
over skip-/n-gram SNM models trained on pooled data by about 8% in the SMT
setup, or as much as 15% in the ASR/IME setup.
  The ability to mix various data sources based on how relevant they are to a
mismatched held-out set is probably the most attractive feature of the new
estimation method for SNM LM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01593</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01593</id><created>2015-11-04</created><authors><author><keyname>Rao</keyname><forenames>Vishwas</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author><author><keyname>Ng</keyname><forenames>Michael</forenames></author><author><keyname>Nino-Ruiz</keyname><forenames>Elias</forenames></author></authors><title>Robust data assimilation using $L_1$ and Huber norms</title><categories>math.NA cs.NA</categories><comments>25 pages, Submitted to SISC</comments><report-no>CSL-TR-15-21</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data assimilation is the process to fuse information from priors,
observations of nature, and numerical models, in order to obtain best estimates
of the parameters or state of a physical system of interest. Presence of large
errors in some observational data, e.g., data collected from a faulty
instrument, negatively affect the quality of the overall assimilation results.
  This work develops a systematic framework for robust data assimilation. The
new algorithms continue to produce good analyses in the presence of observation
outliers. The approach is based on replacing the traditional $\L_2$ norm
formulation of data assimilation problems with formulations based on $\L_1$ and
Huber norms. Numerical experiments using the Lorenz-96 and the shallow water on
the sphere models illustrate how the new algorithms outperform traditional data
assimilation approaches in the presence of data outliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01598</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01598</id><created>2015-11-04</created><updated>2015-12-09</updated><authors><author><keyname>Ye</keyname><forenames>Haishan</forenames></author><author><keyname>Li</keyname><forenames>Yujun</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>A Simple Approach to Optimal CUR Decomposition</title><categories>cs.NA</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Prior optimal CUR decomposition and near optimal column reconstruction
methods have been established by combining BSS sampling and adaptive sampling.
In this paper, we propose a new approach to the optimal CUR decomposition and
near optimal column reconstruction by just using leverage score sampling. In
our approach, both the BSS sampling and adaptive sampling are not needed.
Moreover, our approach is the first $O(\mathrm{nnz}(\A))$ optimal CUR algorithm
where $\A$ is a data matrix in question. We also extend our approach to the
Nystr{\&quot;o}m method, obtaining a fast algorithm which runs $\tilde{O}(n^{2})$ or
$O(\mathrm{\nnz}(\A))$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01612</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01612</id><created>2015-11-05</created><authors><author><keyname>Abam</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>de Berg</keyname><forenames>Mark</forenames></author><author><keyname>Seraji</keyname><forenames>Mohammad Javad Rezaei</forenames></author></authors><title>Geodesic Spanners for Points on a Polyhedral Terrain</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there exists a geodesic spanner with almost linear number of
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01616</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01616</id><created>2015-11-05</created><authors><author><keyname>Ding</keyname><forenames>Baokun</forenames></author><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>New constructions of quantum MDS convolutional codes derived from
  generalized Reed-Solomon codes</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1408.5782 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum convolutional codes can be used to protect a sequence of qubits of
arbitrary length against decoherence. In this paper, we give two new
constructions of quantum MDS convolutional codes derived from generalized
Reed-Solomon codes and obtain eighteen new classes of quantum MDS convolutional
codes. Most of them are new in the sense that the parameters of the codes are
different from all the previously known ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01619</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01619</id><created>2015-11-05</created><authors><author><keyname>Narayana</keyname><forenames>Manjunath</forenames></author><author><keyname>Hanson</keyname><forenames>Allen</forenames></author><author><keyname>Learned-Miller</keyname><forenames>Erik</forenames></author></authors><title>Coherent Motion Segmentation in Moving Camera Videos using Optical Flow
  Orientations</title><categories>cs.CV</categories><comments>8 pages, 5 figures, in 2013 IEEE International Conference on Computer
  Vision (ICCV)</comments><doi>10.1109/ICCV.2013.199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In moving camera videos, motion segmentation is commonly performed using the
image plane motion of pixels, or optical flow. However, objects that are at
different depths from the camera can exhibit different optical flows even if
they share the same real-world motion. This can cause a depth-dependent
segmentation of the scene. Our goal is to develop a segmentation algorithm that
clusters pixels that have similar real-world motion irrespective of their depth
in the scene. Our solution uses optical flow orientations instead of the
complete vectors and exploits the well-known property that under camera
translation, optical flow orientations are independent of object depth. We
introduce a probabilistic model that automatically estimates the number of
observed independent motions and results in a labeling that is consistent with
real-world motion in the scene. The result of our system is that static objects
are correctly identified as one segment, even if they are at different depths.
Color features and information from previous frames in the video sequence are
used to correct occasional errors due to the orientation-based segmentation. We
present results on more than thirty videos from different benchmarks. The
system is particularly robust on complex background scenes containing objects
at significantly different depths
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01622</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01622</id><created>2015-11-05</created><authors><author><keyname>Arachchilage</keyname><forenames>Nalin Asanka Gamagedara</forenames></author><author><keyname>Love</keyname><forenames>Steve</forenames></author><author><keyname>Maple</keyname><forenames>Carsten</forenames></author></authors><title>Can a Mobile Game Teach Computer Users to Thwart Phishing Attacks?</title><categories>cs.CY cs.CR cs.HC</categories><comments>11 pages</comments><journal-ref>International Journal for Infonomics (IJI), Volume 6, Issues 3/4,
  ISSN: 1742 4712, pp. 720-730 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phishing is an online fraudulent technique, which aims to steal sensitive
information such as usernames, passwords and online banking details from its
victims. To prevent this, anti-phishing education needs to be considered. This
research focuses on examining the effectiveness of mobile game based learning
compared to traditional online learning to thwart phishing threats. Therefore,
a mobile game prototype was developed based on the design introduced by
Arachchilage and Cole [3]. The game design aimed to enhance avoidance behaviour
through motivation to thwart phishing threats. A website developed by
Anti-Phishing Work Group (APWG) for the public Anti-phishing education
initiative was used as a traditional web based learning source. A think-aloud
experiment along with a pre- and post-test was conducted through a user study.
The study findings revealed that the participants who played the mobile game
were better able to identify fraudulent web sites compared to the participants
who read the website without any training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01627</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01627</id><created>2015-11-05</created><authors><author><keyname>Narayana</keyname><forenames>Manjunath</forenames></author><author><keyname>Hanson</keyname><forenames>Allen</forenames></author><author><keyname>Learned-Miller</keyname><forenames>Erik</forenames></author></authors><title>Background subtraction - separating the modeling and the inference</title><categories>cs.CV</categories><comments>19 pages, 6 figures, Machine Vision and Applications journal</comments><journal-ref>Machine Vision and Applications July 2014, Volume 25, Issue 5, pp
  1163-1174</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In its early implementations, background modeling was a process of building a
model for the background of a video with a stationary camera, and identifying
pixels that did not conform well to this model. The pixels that were not
well-described by the background model were assumed to be moving objects. Many
systems today maintain models for the foreground as well as the background, and
these models compete to explain the pixels in a video. In this paper, we argue
that the logical endpoint of this evolution is to simply use Bayes' rule to
classify pixels. In particular, it is essential to have a background
likelihood, a foreground likelihood, and a prior at each pixel. A simple
application of Bayes' rule then gives a posterior probability over the label.
The only remaining question is the quality of the component models: the
background likelihood, the foreground likelihood, and the prior. We describe a
model for the likelihoods that is built by using not only the past observations
at a given pixel location, but by also including observations in a spatial
neighborhood around the location. This enables us to model the influence
between neighboring pixels and is an improvement over earlier pixelwise models
that do not allow for such influence. Although similar in spirit to the joint
domain-range model, we show that our model overcomes certain deficiencies in
that model. We use a spatially dependent prior for the background and
foreground. The background and foreground labels from the previous frame, after
spatial smoothing to account for movement of objects,are used to build the
prior for the current frame.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01630</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01630</id><created>2015-11-05</created><updated>2016-03-03</updated><authors><author><keyname>Berdinsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Khoussainov</keyname><forenames>Bakhadyr</forenames></author></authors><title>Cayley automatic representations of wreath products</title><categories>math.GR cs.FL</categories><comments>Final version, to appear in the International Journal of Foundations
  of Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct the representations of Cayley graphs of wreath products using
finite automata, pushdown automata and nested stack automata. These
representations are in accordance with the notion of Cayley automatic groups
introduced by Kharlampovich, Khoussainov and Miasnikov and its extensions
introduced by Elder and Taback. We obtain the upper and lower bounds for a
length of an element of a wreath product in terms of the representations
constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01631</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01631</id><created>2015-11-05</created><authors><author><keyname>Narayana</keyname><forenames>Manjunath</forenames></author><author><keyname>Hanson</keyname><forenames>Allen</forenames></author><author><keyname>Learned-Miller</keyname><forenames>Erik</forenames></author></authors><title>Background Modeling Using Adaptive Pixelwise Kernel Variances in a
  Hybrid Feature Space</title><categories>cs.CV</categories><comments>8 pages, 4 figures, CVPR 2012 conference paper in CVPR '12
  Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on background subtraction has shown developments on two major
fronts. In one, there has been increasing sophistication of probabilistic
models, from mixtures of Gaussians at each pixel [7], to kernel density
estimates at each pixel [1], and more recently to joint domainrange density
estimates that incorporate spatial information [6]. Another line of work has
shown the benefits of increasingly complex feature representations, including
the use of texture information, local binary patterns, and recently
scale-invariant local ternary patterns [4]. In this work, we use joint
domain-range based estimates for background and foreground scores and show that
dynamically choosing kernel variances in our kernel estimates at each
individual pixel can significantly improve results. We give a heuristic method
for selectively applying the adaptive kernel calculations which is nearly as
accurate as the full procedure but runs much faster. We combine these modeling
improvements with recently developed complex features [4] and show significant
improvements on a standard backgrounding benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01633</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01633</id><created>2015-11-05</created><authors><author><keyname>Lin</keyname><forenames>Anthony W.</forenames></author><author><keyname>Barcelo</keyname><forenames>Pablo</forenames></author></authors><title>String Solving with Word Equations and Transducers: Towards a Logic for
  Analysing Mutation XSS (Full Version)</title><categories>cs.LO</categories><comments>Full version of POPL'16 paper</comments><acm-class>F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental issue of decidability of satisfiability over string
logics with concatenations and finite-state transducers as atomic operations.
Although restricting to one type of operations yields decidability, little is
known about the decidability of their combined theory, which is especially
relevant when analysing security vulnerabilities of dynamic web pages in a more
realistic browser model. On the one hand, word equations (string logic with
concatenations) cannot precisely capture sanitisation functions (e.g.
htmlescape) and implicit browser transductions (e.g. innerHTML mutations). On
the other hand, transducers suffer from the reverse problem of being able to
model sanitisation functions and browser transductions, but not string
concatenations. Naively combining word equations and transducers easily leads
to an undecidable logic. Our main contribution is to show that the
&quot;straight-line fragment&quot; of the logic is decidable (complexity ranges from
PSPACE to EXPSPACE). The fragment can express the program logics of
straight-line string-manipulating programs with concatenations and
transductions as atomic operations, which arise when performing bounded model
checking or dynamic symbolic executions. We demonstrate that the logic can
naturally express constraints required for analysing mutation XSS in web
applications. Finally, the logic remains decidable in the presence of length,
letter-counting, regular, indexOf, and disequality constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01634</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01634</id><created>2015-11-05</created><authors><author><keyname>Haghighatshoar</keyname><forenames>Saeid</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>An Active-Sensing Approach to Channel Vector Subspace Estimation in
  mm-Wave Massive MIMO Systems</title><categories>cs.IT math.IT stat.AP</categories><comments>7 pages, 2 figures, Submitted to IEEE International Conference on
  Communications (ICC 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter-wave (mm-Wave) cellular systems are a promising option for a very
high data rate communication because of the large bandwidth available at
mm-Wave frequencies. Due to the large path-loss exponent in the mm-Wave range
of the spectrum, directional beamforming with a large antenna gain is necessary
at the transmitter, the receiver or both for capturing sufficient signal power.
This in turn implies that fast and robust channel estimation plays a central
role in systems performance since without a reliable estimate of the channel
state the received signal-to-noise ratio (SNR) would be much lower than the
minimum necessary for a reliable communication.
  In this paper, we mainly focus on single-antenna users and a multi-antenna
base-station. We propose an adaptive sampling scheme to speed up the user's
signal subspace estimation. In our scheme, the beamforming vector for taking
every new sample is adaptively selected based on all the previous beamforming
vectors and the resulting output observations. We apply the theory of optimal
design of experiments in statistics to design an adaptive algorithm for
estimating the signal subspace of each user. The resulting subspace estimates
for different users can be exploited to efficiently communicate to the users
and to manage the interference. We cast our proposed algorithm as
low-complexity optimization problems, and illustrate its efficiency via
numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01640</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01640</id><created>2015-11-05</created><authors><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>Computing sets of graded attribute implications with witnessed
  non-redundancy</title><categories>cs.AI</categories><msc-class>68P20, 68T30, 03B52</msc-class><acm-class>H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend our previous results on sets of graded attribute
implications with witnessed non-redundancy. We assume finite residuated
lattices as structures of truth degrees and use arbitrary idempotent
truth-stressing linguistic hedges as parameters which influence the semantics
of graded attribute implications. In this setting, we introduce algorithm which
transforms any set of graded attribute implications into an equivalent
non-redundant set of graded attribute implications with saturated consequents
whose non-redundancy is witnessed by antecedents of the formulas. As a
consequence, we solve the open problem regarding the existence of general
systems of pseudo-intents which appear in formal concept analysis of
object-attribute data with graded attributes and linguistic hedges.
Furthermore, we show a polynomial-time procedure for determining bases given by
general systems of pseudo-intents from sets of graded attribute implications
which are complete in data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01643</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01643</id><created>2015-11-03</created><authors><author><keyname>Panwar</keyname><forenames>Nisha</forenames></author><author><keyname>Sharma</keyname><forenames>Shantanu</forenames></author><author><keyname>Singh</keyname><forenames>Awadhesh Kumar</forenames></author></authors><title>A Survey on 5G: The Next Generation of Mobile Communication</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>Accepted in Elsevier Physical Communication, 24 pages, 5 figures, 2
  tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapidly increasing number of mobile devices, voluminous data, and higher
data rate are pushing to rethink the current generation of the cellular mobile
communication. The next or fifth generation (5G) cellular networks are expected
to meet high-end requirements. The 5G networks are broadly characterized by
three unique features: ubiquitous connectivity, extremely low latency, and very
high-speed data transfer. The 5G networks would provide novel architectures and
technologies beyond state-of-the-art architectures and technologies. In this
paper, our intent is to find an answer to the question: &quot;what will be done by
5G and how?&quot; We investigate and discuss serious limitations of the fourth
generation (4G) cellular networks and corresponding new features of 5G
networks. We identify challenges in 5G networks, new technologies for 5G
networks, and present a comparative study of the proposed architectures that
can be categorized on the basis of energy-efficiency, network hierarchy, and
network types. Interestingly, the implementation issues, e.g., interference,
QoS, handoff, security-privacy, channel access, and load balancing, hugely
effect the realization of 5G networks. Furthermore, our illustrations highlight
the feasibility of these models through an evaluation of existing
real-experiments and testbeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01644</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01644</id><created>2015-11-05</created><authors><author><keyname>Letham</keyname><forenames>Benjamin</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author><author><keyname>McCormick</keyname><forenames>Tyler H.</forenames></author><author><keyname>Madigan</keyname><forenames>David</forenames></author></authors><title>Interpretable classifiers using rules and Bayesian analysis: Building a
  better stroke prediction model</title><categories>stat.AP cs.LG stat.ML</categories><comments>Published at http://dx.doi.org/10.1214/15-AOAS848 in the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS848</report-no><journal-ref>Annals of Applied Statistics 2015, Vol. 9, No. 3, 1350-1371</journal-ref><doi>10.1214/15-AOAS848</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We aim to produce predictive models that are not only accurate, but are also
interpretable to human experts. Our models are decision lists, which consist of
a series of if...then... statements (e.g., if high blood pressure, then stroke)
that discretize a high-dimensional, multivariate feature space into a series of
simple, readily interpretable decision statements. We introduce a generative
model called Bayesian Rule Lists that yields a posterior distribution over
possible decision lists. It employs a novel prior structure to encourage
sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy
on par with the current top algorithms for prediction in machine learning. Our
method is motivated by recent developments in personalized medicine, and can be
used to produce highly accurate and interpretable medical scoring systems. We
demonstrate this by producing an alternative to the CHADS$_2$ score, actively
used in clinical practice for estimating the risk of stroke in patients that
have atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more
accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01646</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01646</id><created>2015-11-05</created><authors><author><keyname>Trifonov</keyname><forenames>Peter</forenames></author><author><keyname>Miloslavskaya</keyname><forenames>Vera</forenames></author></authors><title>Polar Subcodes</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE JSAC special issue on Recent Advances In Capacity
  Approaching Codes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extension of polar codes is proposed, which allows some of the frozen
symbols, called dynamic frozen symbols, to be data-dependent. A construction of
polar codes with dynamic frozen symbols, being subcodes of extended BCH codes,
is proposed. The proposed codes have higher minimum distance than classical
polar codes, but still can be efficiently decoded using the successive
cancellation algorithm and its extensions. The codes with Arikan, extended BCH
and Reed-Solomon kernel are considered. The proposed codes are shown to
outperform LDPC and turbo codes, as well as polar codes with CRC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01650</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01650</id><created>2015-11-05</created><authors><author><keyname>Barbier</keyname><forenames>Jean</forenames></author></authors><title>Statistical physics and approximate message-passing algorithms for
  sparse linear estimation problems in signal processing and coding theory</title><categories>cs.IT math.IT</categories><comments>PhD thesis defended the september 18th 2015 at the Ecole Normale
  Sup\'erieure of Paris, in front of the jury composed of Prof. Laurent DAUDET,
  examinateur, Prof. Silvio FRANZ, examinateur, Prof. Florent KRZAKALA,
  directeur, Prof. Marc LELARGE, examinateur, Prof. Nicolas MACRIS, rapporteur,
  Prof. Marc M\'EZARD, examinateur, Prof. Federico RICCI-TERSENGHI,
  examinateur, Prof. David SAAD, rapporteur</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis is interested in the application of statistical physics methods
and inference to sparse linear estimation problems. The main tools are the
graphical models and approximate message-passing algorithm together with the
cavity method. We will also use the replica method of statistical physics of
disordered systems which allows to associate to the studied problems a cost
function referred as the potential of free entropy in physics. It allows to
predict the different phases of typical complexity of the problem as a function
of external parameters such as the noise level or the number of measurements
one has about the signal: the inference can be typically easy, hard or
impossible. We will see that the hard phase corresponds to a regime of
coexistence of the actual solution together with another unwanted solution of
the message passing equations. In this phase, it represents a metastable state
which is not the true equilibrium solution. This phenomenon can be linked to
supercooled water blocked in the liquid state below its freezing critical
temperature. We will use a method that allows to overcome the metastability
mimicing the strategy adopted by nature itself for supercooled water: the
nucleation and spatial coupling. In supercooled water, a weak localized
perturbation is enough to create a crystal nucleus that will propagate in all
the medium thanks to the physical couplings between closeby atoms. The same
process will help the algorithm to find the signal, thanks to the introduction
of a nucleus containing local information about the signal. It will then spread
as a &quot;reconstruction wave&quot; similar to the crystal in the water. After an
introduction to statistical inference and sparse linear estimation, we will
introduce the necessary tools. Then we will move to applications of these
notions to signal processing and coding theory problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01661</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01661</id><created>2015-11-05</created><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Yeo</keyname><forenames>Anders</forenames></author></authors><title>Note on Perfect Forests in Digraphs</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A spanning subgraph $F$ of a graph $G$ is called {\em perfect} if $F$ is a
forest, the degree $d_F(x)$ of each vertex $x$ in $F$ is odd, and each tree of
$F$ is an induced subgraph of $G$. Alex Scott (Graphs \&amp; Combin., 2001) proved
that every connected graph $G$ contains a perfect forest if and only if $G$ has
an even number of vertices. We consider four generalizations to directed graphs
of the concept of a perfect forest. While the problem of existence of the most
straightforward one is NP-hard, for the three others this problem is
polynomial-time solvable. Moreover, every digraph with only one strong
component contains a directed forest of each of these three generalization
types. One of our results extends Scott's theorem to digraphs in a non-trivial
way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01664</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01664</id><created>2015-11-05</created><updated>2015-12-05</updated><authors><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Stochastic Proximal Gradient Descent for Nuclear Norm Regularization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we utilize stochastic optimization to reduce the space
complexity of convex composite optimization with a nuclear norm regularizer,
where the variable is a matrix of size $m \times n$. By constructing a low-rank
estimate of the gradient, we propose an iterative algorithm based on stochastic
proximal gradient descent (SPGD), and take the last iterate of SPGD as the
final solution. The main advantage of the proposed algorithm is that its space
complexity is $O(m+n)$, in contrast, most of previous algorithms have a $O(mn)$
space complexity. Theoretical analysis shows that it achieves $O(\log
T/\sqrt{T})$ and $O(\log T/T)$ convergence rates for general convex functions
and strongly convex functions, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01665</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01665</id><created>2015-11-05</created><authors><author><keyname>Lin</keyname><forenames>Yiou</forenames></author><author><keyname>Lei</keyname><forenames>Hang</forenames></author><author><keyname>Wu</keyname><forenames>Jia</forenames></author><author><keyname>Li</keyname><forenames>Xiaoyu</forenames></author></authors><title>An Empirical Study on Sentiment Classification of Chinese Review using
  Word Embedding</title><categories>cs.CL</categories><comments>The 29th Pacific Asia Conference on Language, Information and
  Computing</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this article, how word embeddings can be used as features in Chinese
sentiment classification is presented. Firstly, a Chinese opinion corpus is
built with a million comments from hotel review websites. Then the word
embeddings which represent each comment are used as input in different machine
learning methods for sentiment classification, including SVM, Logistic
Regression, Convolutional Neural Network (CNN) and ensemble methods. These
methods get better performance compared with N-gram models using Naive Bayes
(NB) and Maximum Entropy (ME). Finally, a combination of machine learning
methods is proposed which presents an outstanding performance in precision,
recall and F1 score. After selecting the most useful methods to construct the
combinational model and testing over the corpus, the final F1 score is 0.920.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01666</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01666</id><created>2015-11-05</created><authors><author><keyname>Tushar</keyname><forenames>Abhinav</forenames></author><author><keyname>Dahiya</keyname><forenames>Abhinav</forenames></author></authors><title>Comparing Writing Styles using Word Embedding and Dynamic Time Warping</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The development of plot or story in novels is reflected in the content and
the words used. The flow of sentiments, which is one aspect of writing style,
can be quantified by analyzing the flow of words. This study explores literary
works as signals in word embedding space and tries to compare writing styles of
popular classic novels using dynamic time warping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01668</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01668</id><created>2015-11-05</created><updated>2015-11-26</updated><authors><author><keyname>Illuri</keyname><forenames>Madhu</forenames></author><author><keyname>Renjith</keyname><forenames>P.</forenames></author><author><keyname>Sadagopan</keyname><forenames>N.</forenames></author></authors><title>Complexity of Steiner Tree in Split Graphs - Dichotomy Results</title><categories>cs.DM</categories><comments>12 pages, 2 figures, CALDAM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a connected graph $G$ and a terminal set $R \subseteq V(G)$, {\em
Steiner tree} asks for a tree that includes all of $R$ with at most $r$ edges
for some integer $r \geq 0$. It is known from [ND12,Garey et. al] that Steiner
tree is NP-complete in general graphs. {\em Split graph} is a graph which can
be partitioned into a clique and an independent set. K. White et. al has
established that Steiner tree in split graphs is NP-complete. In this paper, we
present an interesting dichotomy: we show that Steiner tree on $K_{1,4}$-free
split graphs is polynomial-time solvable, whereas, Steiner tree on
$K_{1,5}$-free split graphs is NP-complete. We investigate $K_{1,4}$-free and
$K_{1,3}$-free (also known as claw-free) split graphs from a structural
perspective. Further, using our structural study, we present polynomial-time
algorithms for Steiner tree in $K_{1,4}$-free and $K_{1,3}$-free split graphs.
Although, polynomial-time solvability of $K_{1,3}$-free split graphs is implied
from $K_{1,4}$-free split graphs, we wish to highlight our structural
observations on $K_{1,3}$-free split graphs which may be used in other
combinatorial problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01669</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01669</id><created>2015-11-05</created><updated>2015-11-13</updated><authors><author><keyname>Qiu</keyname><forenames>Tianyu</forenames></author><author><keyname>Babu</keyname><forenames>Prabhu</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author></authors><title>PRIME: Phase Retrieval via Majorization-Minimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the phase retrieval problem in which measurements
consist of only the magnitude of several linear measurements of the unknown,
e.g., spectral components of a time sequence. We develop low-complexity
algorithms with superior performance based on the majorization-minimization
(MM) framework. The proposed algorithms are referred to as PRIME: Phase
Retrieval vIa the Majorization-minimization techniquE. They are preferred to
existing benchmark methods since at each iteration a simple surrogate problem
is solved with a closed-form solution that monotonically decreases the original
objective function. In total, four algorithms are proposed using different
majorization-minimization techniques. Experimental results validate that our
algorithms outperform existing methods in terms of successful recovery and mean
square error under various settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01696</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01696</id><created>2015-11-05</created><authors><author><keyname>Reddy</keyname><forenames>K. Krishna Mohan</forenames></author><author><keyname>Renjith</keyname><forenames>P.</forenames></author><author><keyname>Sadagopan</keyname><forenames>N.</forenames></author></authors><title>Listing All Spanning Trees in Halin Graphs - Sequential and Parallel
  view</title><categories>cs.DM</categories><comments>12 pages, 3 figures, IJFCS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a connected labelled graph $G$, a {\em spanning tree} $T$ is a connected
and an acyclic subgraph that spans all the vertices of $G$. In this paper, we
consider a classical combinatorial problem which is to list all spanning trees
of $G$. A Halin graph is a graph obtained from a tree with no degree two
vertices and by joining all leaves with a cycle. We present a sequential and
parallel algorithm to enumerate all spanning trees in Halin graphs. Our
approach enumerates without repetitions and we make use of $O((2pd)^{p})$
processors for parallel algorithmics, where $d$ and $p$ are the depth, the
number of leaves, respectively, of the Halin graph. We also prove that the
number of spanning trees in Halin graphs is $O((2pd)^{p})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01697</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01697</id><created>2015-11-05</created><authors><author><keyname>Wang</keyname><forenames>Fu-Hong</forenames></author><author><keyname>Guo</keyname><forenames>Jin-Li</forenames></author><author><keyname>Shen</keyname><forenames>Ai-Zhong</forenames></author><author><keyname>Suo</keyname><forenames>Qi</forenames></author></authors><title>Evolving hypernetwork model based on WeChat user relations</title><categories>cs.SI</categories><comments>14 pages, in Chinese, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Based on the theory of hypernetwork and WeChat online social relations, the
paper proposes an evolving hypernetwork model with the competitiveness and the
age of nodes. In the model, nodes arrive at the system in accordance with
Poisson process and are gradual aging. We analyze the model by using a Poisson
process theory and a continuous technique, and give a characteristic equation
of hyperdegrees. We obtain the stationary average hyperdegree distribution of
the hypernetwork by the characteristic equation. The numerical simulations of
the models agree with the analytical results well. It is expected that our work
may give help to the study of WeChat information transmission dynamics and
mobile e-commerce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01699</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01699</id><created>2015-11-05</created><authors><author><keyname>Dan</keyname><forenames>Chen</forenames></author><author><keyname>Hansen</keyname><forenames>Kristoffer Arnsfelt</forenames></author><author><keyname>Jiang</keyname><forenames>He</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author><author><keyname>Zhou</keyname><forenames>Yuchen</forenames></author></authors><title>On Low Rank Approximation of Binary Matrices</title><categories>cs.CC cs.DS</categories><comments>29 pages. Submitted to STOC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of low rank approximation of binary matrices. Here we
are given a $d \times n$ binary matrix $A$ and a small integer $k &lt; d$. The
goal is to find two binary matrices $U$ and $V$ of sizes $d \times k$ and $k
\times n$ respectively, so that the Frobenius norm of $A-U V$ is minimized.
There are two models of this problem, depending on the definition of the
product of binary matrices: The $\mathrm{GF}(2)$ model and the Boolean semiring
model. Previously, the only known results are $2$-approximation algorithms for
the special case $k=1$ \cite{KDD:ShenJY09, Jiang14} (where the two models are
equivalent).
  In this paper, we give the first results for the general case $k&gt;1$ for both
$\mathrm{GF}(2)$ and Boolean model. For the $\mathrm{GF}(2)$ model, we show
that a simple column-selection algorithm achieves $O(k)$-approximation. For the
Boolean model, we develop a new algorithm and show that it is
$O(2^k)$-approximation. For constant $k$, both algorithms run in polynomial
time in the size of the matrix. We also show that the low rank binary matrix
approximation problem is NP-hard even for $k=1$, solving a conjecture in
\cite{Koyuturk03}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01705</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01705</id><created>2015-11-05</created><authors><author><keyname>Martinsen</keyname><forenames>Thor</forenames></author><author><keyname>Meidl</keyname><forenames>Wilfried</forenames></author><author><keyname>Stanica</keyname><forenames>Pantelimon</forenames></author></authors><title>Partial Spread and Vectorial Generalized Bent Functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we generalize the partial spread class and completely describe
it for generalized Boolean functions from $\F_2^n$ to $\mathbb{Z}_{2^t}$.
Explicitly, we describe gbent functions from $\F_2^n$ to $\mathbb{Z}_{2^t}$,
which can be seen as a gbent version of Dillon's $PS_{ap}$ class. For the first
time, we also introduce the concept of a vectorial gbent function from $\F_2^n$
to $\Z_q^m$, and determine the maximal value which $m$ can attain for the case
$q=2^t$. Finally we point to a relation between vectorial gbent functions and
relative difference sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01706</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01706</id><created>2015-11-05</created><authors><author><keyname>Gao</keyname><forenames>Huilin</forenames></author><author><keyname>Chen</keyname><forenames>Wenjie</forenames></author><author><keyname>Dou</keyname><forenames>Lihua</forenames></author></authors><title>Image classification based on support vector machine and the fusion of
  complementary features</title><categories>cs.CV</categories><comments>22 pages,4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image Classification based on BOW (Bag-of-words) has broad application
prospect in pattern recognition field but the shortcomings are existed because
of single feature and low classification accuracy. To this end we combine three
ingredients: (i) Three features with functions of mutual complementation are
adopted to describe the images, including PHOW (Pyramid Histogram of Words),
PHOC (Pyramid Histogram of Color) and PHOG (Pyramid Histogram of Orientated
Gradients). (ii) The improvement of traditional BOW model is presented by using
dense sample and an improved K-means clustering method for constructing the
visual dictionary. (iii) An adaptive feature-weight adjusted image
categorization algorithm based on the SVM and the fusion of multiple features
is adopted. Experiments carried out on Caltech 101 database confirm the
validity of the proposed approach. From the experimental results can be seen
that the classification accuracy rate of the proposed method is improved by
7%-17% higher than that of the traditional BOW methods. This algorithm makes
full use of global, local and spatial information and has significant
improvements to the classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01710</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01710</id><created>2015-11-05</created><authors><author><keyname>Grau-Moya</keyname><forenames>Jordi</forenames></author><author><keyname>Braun</keyname><forenames>Daniel A.</forenames></author></authors><title>Adaptive information-theoretic bounded rational decision-making with
  parametric priors</title><categories>cs.AI</categories><comments>4 pages, 1 figure, Workshop on Bounded Optimality and Rational
  Metareasoning at Neural Information Processing Systems conference, Montreal,
  Canada, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deviations from rational decision-making due to limited computational
resources have been studied in the field of bounded rationality, originally
proposed by Herbert Simon. There have been a number of different approaches to
model bounded rationality ranging from optimality principles to heuristics.
Here we take an information-theoretic approach to bounded rationality, where
information-processing costs are measured by the relative entropy between a
posterior decision strategy and a given fixed prior strategy. In the case of
multiple environments, it can be shown that there is an optimal prior rendering
the bounded rationality problem equivalent to the rate distortion problem for
lossy compression in information theory. Accordingly, the optimal prior and
posterior strategies can be computed by the well-known Blahut-Arimoto algorithm
which requires the computation of partition sums over all possible outcomes and
cannot be applied straightforwardly to continuous problems. Here we derive a
sampling-based alternative update rule for the adaptation of prior behaviors of
decision-makers and we show convergence to the optimal prior predicted by rate
distortion theory. Importantly, the update rule avoids typical infeasible
operations such as the computation of partition sums. We show in simulations a
proof of concept for discrete action and environment domains. This approach is
not only interesting as a generic computational method, but might also provide
a more realistic model of human decision-making processes occurring on a fast
and a slow time scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01726</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01726</id><created>2015-11-05</created><authors><author><keyname>Ata-ur-Rehman</keyname></author><author><keyname>Naqvi</keyname><forenames>Syed Mohsen</forenames></author><author><keyname>Mihaylova</keyname><forenames>Lyudmila</forenames></author><author><keyname>Chambers</keyname><forenames>Jonathon</forenames></author></authors><title>Multi-Target Tracking and Occlusion Handling with Learned Variational
  Bayesian Clusters and a Social Force Model</title><categories>cs.CV</categories><comments>19 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of multiple human target tracking in a
sequence of video data. A solution is proposed which is able to deal with the
challenges of a varying number of targets, interactions and when every target
gives rise to multiple measurements. The developed novel algorithm comprises
variational Bayesian clustering combined with a social force model, integrated
within a particle filter with an enhanced prediction step. It performs
measurement-to-target association by automatically detecting the measurement
relevance. The performance of the developed algorithm is evaluated over several
sequences from publicly available data sets: AV16.3, CAVIAR and PETS2006, which
demonstrates that the proposed algorithm successfully initializes and tracks a
variable number of targets in the presence of complex occlusions. A comparison
with state-of-the-art techniques due to Khan et al., Laet et al. and Czyz et
al. shows improved tracking performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01754</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01754</id><created>2015-11-05</created><updated>2015-11-07</updated><authors><author><keyname>Badrinarayanan</keyname><forenames>Vijay</forenames></author><author><keyname>Mishra</keyname><forenames>Bamdev</forenames></author><author><keyname>Cipolla</keyname><forenames>Roberto</forenames></author></authors><title>Symmetry-invariant optimization in deep networks</title><categories>cs.LG cs.AI cs.CV</categories><comments>Submitted to ICLR 2016. arXiv admin note: text overlap with
  arXiv:1511.01029</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works have highlighted scale invariance or symmetry that is present in
the weight space of a typical deep network and the adverse effect that it has
on the Euclidean gradient based stochastic gradient descent optimization. In
this work, we show that these and other commonly used deep networks, such as
those which use a max-pooling and sub-sampling layer, possess more complex
forms of symmetry arising from scaling based reparameterization of the network
weights. We then propose two symmetry-invariant gradient based weight updates
for stochastic gradient descent based learning. Our empirical evidence based on
the MNIST dataset shows that these updates improve the test performance without
sacrificing the computational efficiency of the weight updates. We also show
the results of training with one of the proposed weight updates on an image
segmentation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01756</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01756</id><created>2015-11-05</created><authors><author><keyname>Mpouli</keyname><forenames>Suzanne</forenames><affiliation>ACASA</affiliation></author><author><keyname>Ganascia</keyname><forenames>Jean-Gabriel</forenames><affiliation>ACASA</affiliation></author></authors><title>&quot;Pale as death&quot; or &quot;p\^ale comme la mort&quot;: Frozen similes used as
  literary clich\'es</title><categories>cs.CL</categories><comments>Europhras 2015: Computerised and corpus-based approaches to
  phraseology: monolingual and multilingual perspectivs, Jun 2015, Malaga,
  Spain. \&amp;lt;The European Society of Phraseology\&amp;gt</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present study is focused on the automatic identification and description
of frozen similes in British and French novels written between the 19 th
century and the beginning of the 20 th century. Two main patterns of frozen
similes were considered: adjectival ground + simile marker + nominal vehicle
(e.g. happy as a lark) and eventuality + simile marker + nominal vehicle (e.g.
sleep like a top). All potential similes and their components were first
extracted using a rule-based algorithm. Then, frozen similes were identified
based on reference lists of existing similes and semantic distance between the
tenor and the vehicle. The results obtained tend to confirm the fact that
frozen similes are not used haphazardly in literary texts. In addition,
contrary to how they are often presented, frozen similes often go beyond the
ground or the eventuality and the vehicle to also include the tenor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01764</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01764</id><created>2015-11-05</created><authors><author><keyname>Razaviyayn</keyname><forenames>Meisam</forenames></author><author><keyname>Farnia</keyname><forenames>Farzan</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Discrete R\'enyi Classifiers</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the binary classification problem of predicting a target variable
$Y$ from a discrete feature vector $X = (X_1,...,X_d)$. When the probability
distribution $\mathbb{P}(X,Y)$ is known, the optimal classifier, leading to the
minimum misclassification rate, is given by the Maximum A-posteriori
Probability decision rule. However, estimating the complete joint distribution
$\mathbb{P}(X,Y)$ is computationally and statistically impossible for large
values of $d$. An alternative approach is to first estimate some low order
marginals of $\mathbb{P}(X,Y)$ and then design the classifier based on the
estimated low order marginals. This approach is also helpful when the complete
training data instances are not available due to privacy concerns. In this
work, we consider the problem of finding the optimum classifier based on some
estimated low order marginals of $(X,Y)$. We prove that for a given set of
marginals, the minimum Hirschfeld-Gebelein-Renyi (HGR) correlation principle
introduced in [1] leads to a randomized classification rule which is shown to
have a misclassification rate no larger than twice the misclassification rate
of the optimal classifier. Then, under a separability condition, we show that
the proposed algorithm is equivalent to a randomized linear regression
approach. In addition, this method naturally results in a robust feature
selection method selecting a subset of features having the maximum worst case
HGR correlation with the target variable. Our theoretical upper-bound is
similar to the recent Discrete Chebyshev Classifier (DCC) approach [2], while
the proposed algorithm has significant computational advantages since it only
requires solving a least square optimization problem. Finally, we numerically
compare our proposed algorithm with the DCC classifier and show that the
proposed algorithm results in better misclassification rate over various
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01768</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01768</id><created>2015-11-05</created><authors><author><keyname>Trummer</keyname><forenames>Immanuel</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>Parallelizing Query Optimization on Shared-Nothing Architectures</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data processing systems offer an ever increasing degree of parallelism on the
levels of cores, CPUs, and processing nodes. Query optimization must exploit
high degrees of parallelism in order not to gradually become the bottleneck of
query evaluation. We show how to parallelize query optimization at a massive
scale.
  We present algorithms for parallel query optimization in left-deep and bushy
plan spaces. At optimization start, we divide the plan space for a given query
into partitions of equal size that are explored in parallel by worker nodes. At
the end of optimization, each worker returns the optimal plan in its partition
to the master which determines the globally optimal plan from the
partition-optimal plans. No synchronization or data exchange is required during
the actual optimization phase. The amount of data sent over the network, at the
start and at the end of optimization, as well as the complexity of serial steps
within our algorithms increase only linearly in the number of workers and in
the query size. The time and space complexity of optimization within one
partition decreases uniformly in the number of workers. We parallelize single-
and multi-objective query optimization over a cluster with 100 nodes in our
experiments, using more than 250 concurrent worker threads (Spark executors).
Despite high network latency and task assignment overheads, parallelization
yields speedups of up to one order of magnitude for large queries whose
optimization takes minutes on a single node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01770</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01770</id><created>2015-11-05</created><authors><author><keyname>Neou</keyname><forenames>Both Emerite</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author><author><keyname>Vialette</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Pattern matching in $(213,231)$-avoiding permutations</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given permutations $\sigma \in S_k$ and $\pi \in S_n$ with $k&lt;n$, the
\emph{pattern matching} problem is to decide whether $\pi$ matches $\sigma$ as
an order-isomorphic subsequence. We give a linear-time algorithm in case both
$\pi$ and $\sigma$ avoid the two size-$3$ permutations $213$ and $231$. For the
special case where only $\sigma$ avoids $213$ and $231$, we present a
$O(max(kn^2,n^2\log(\log(n)))$ time algorithm. We extend our research to
bivincular patterns that avoid $213$ and $231$ and present a $O(kn^4)$ time
algorithm. Finally we look at the related problem of the longest subsequence
which avoids $213$ and $231$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01776</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01776</id><created>2015-11-05</created><authors><author><keyname>Razaviyayn</keyname><forenames>Meisam</forenames></author><author><keyname>Tseng</keyname><forenames>Hung-Wei</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>Computational Intractability of Dictionary Learning for Sparse
  Representation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the dictionary learning problem for sparse
representation. We first show that this problem is NP-hard by polynomial time
reduction of the densest cut problem. Then, using successive convex
approximation strategies, we propose efficient dictionary learning schemes to
solve several practical formulations of this problem to stationary points.
Unlike many existing algorithms in the literature, such as K-SVD, our proposed
dictionary learning scheme is theoretically guaranteed to converge to the set
of stationary points under certain mild assumptions. For the image denoising
application, the performance and the efficiency of the proposed dictionary
learning scheme are comparable to that of K-SVD algorithm in simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01779</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01779</id><created>2015-11-05</created><authors><author><keyname>Ravi</keyname><forenames>Srivatsan</forenames></author></authors><title>On the Cost of Concurrency in Transactional Memory</title><categories>cs.DC</categories><comments>Ph.D. Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional techniques for synchronization are based on \emph{locking} that
provides threads with exclusive access to shared data. \emph{Coarse-grained}
locking typically forces threads to access large amounts of data sequentially
and, thus, does not fully exploit hardware concurrency. Program-specific
\emph{fine-grained} locking or \emph{non-blocking} (\emph{i.e.}, not using
locks) synchronization, on the other hand, is a dark art to most programmers
and trusted to the wisdom of a few computing experts. Thus, it is appealing to
seek a middle ground between these two extremes: a synchronization mechanism
that relieves the programmer of the overhead of reasoning about data conflicts
that may arise from concurrent operations without severely limiting the
program's performance. The \emph{Transactional Memory (TM)} abstraction is
proposed as such a mechanism: it intends to combine an easy-to-use programming
interface with an efficient utilization of the concurrent-computing abilities
provided by multicore architectures. TM allows the programmer to
\emph{speculatively} execute sequences of shared-memory operations as
\emph{atomic transactions} with \emph{all-or-nothing} semantics: the
transaction can either \emph{commit}, in which case it appears as executed
sequentially, or \emph{abort}, in which case its update operations do not take
effect. Thus, the programmer can design software having only sequential
semantics in mind and let TM take care, at run-time, of resolving the conflicts
in concurrent executions.
  Intuitively, we want TMs to allow for as much \emph{concurrency} as possible:
in the absence of severe data conflicts, transactions should be able to
progress in parallel. But what are the inherent costs associated with providing
high degrees of concurrency in TMs? This is the central question of the thesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01782</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01782</id><created>2015-11-05</created><authors><author><keyname>Trummer</keyname><forenames>Immanuel</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>Probably Approximately Optimal Query Optimization</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating query predicates on data samples is the only way to estimate their
selectivity in certain scenarios. Finding a guaranteed optimal query plan is
not a reasonable optimization goal in those cases as it might require an
infinite number of samples. We therefore introduce probably approximately
optimal query optimization (PAO) where the goal is to find a query plan whose
cost is near-optimal with a certain probability. We will justify why PAO is a
suitable formalism to model scenarios in which predicate sampling and
optimization need to be interleaved.
  We present the first algorithm for PAO. Our algorithm is non-intrusive and
uses standard query optimizers and sampling components as sub-functions. It is
generic and can be applied to a wide range of scenarios. Our algorithm is
iterative and calculates in each iteration a query plan together with a region
in the selectivity space where the plan has near-optimal cost. It determines
the confidence that the true selectivity values fall within the aforementioned
region and chooses the next samples to take based on the current state if the
confidence does not reach the threshold specified as problem input. We devise
different algorithm variants and analyze their complexity. We experimentally
compare them in terms of the number of optimizer invocations, samples, and
iterations over many different query classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01791</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01791</id><created>2015-11-05</created><authors><author><keyname>Liu</keyname><forenames>Ta-Yuan</forenames></author><author><keyname>Lin</keyname><forenames>Shih-Chun</forenames></author><author><keyname>Hong</keyname><forenames>Y. -W. Peter</forenames></author></authors><title>On the Role of Artificial Noise in Training and Data Transmission for
  Secret Communications</title><categories>cs.IT math.IT</categories><comments>38 pages, 5 figures, submitted to IEEE Transactions on Information
  Theory (submitted November 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the joint design of training and data transmission in
physical-layer secret communication systems, and examines the role of
artificial noise (AN) in both of these phases. In particular, AN in the
training phase is used to prevent the eavesdropper from obtaining accurate
channel state information (CSI) whereas AN in the data transmission phase can
be used to mask the transmission of the confidential message. By considering
AN-assisted training and secrecy beamforming schemes, we first derive bounds on
the achievable secrecy rate and obtain a closed-form approximation that is
asymptotically tight at high SNR. Then, by maximizing the approximate
achievable secrecy rate, the optimal power allocation between signal and AN in
both training and data transmission phases is obtained for both conventional
and AN-assisted training based schemes. We show that the use of AN is necessary
to achieve a high secrecy rate at high SNR, and its use in the training phase
can be more efficient than that in the data transmission phase when the
coherence time is large. However, at low SNR, the use of AN provides no
advantage since CSI is difficult to obtain in this case. Numerical results are
presented to verify our theoretical claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01794</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01794</id><created>2015-11-05</created><authors><author><keyname>Li</keyname><forenames>Mingfu</forenames></author></authors><title>Queueing Analysis of Unicast IPTV With User Mobility and Adaptive
  Modulation and Coding in Wireless Cellular Networks</title><categories>cs.NI cs.IT cs.MM cs.PF math.IT math.PR</categories><comments>12 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unicast IPTV services that can support live TV, video-on-demand (VoD), video
conferencing, and online gaming applications over broadband wireless cellular
networks have been becoming popular in recent years. However, video streaming
services significantly impact the performance of wireless cellular networks
because they are bandwidth hogs. To maintain the system performance, effective
admission control and resource allocation mechanisms based on an accurate
mathematical analysis are required. On the other hand, the quality of a
wireless link usually changes with time due to the user mobility or
time-varying channel characteristics. To counteract such time-varying channels
and improve the spectral efficiency, adaptive modulation and coding (AMC)
scheme can be adopted in offering unicast IPTV services for mobile users. In
this paper, closed-form solutions for the bandwidth usage, blocking rate, and
dropping rate of unicast IPTV services over wireless cellular networks were
derived based on the novel queueing model that considers both user mobility and
AMC. Simulations were also conducted to validate the accuracy of analytical
results. Numerical results demonstrate that the presented analytical results
are accurate. Based on the accurate closed-form solutions, network providers
can implement precise admission control and resource allocation for their
networks to enhance the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01795</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01795</id><created>2015-11-05</created><authors><author><keyname>Hsu</keyname><forenames>Yu-Pin</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author></authors><title>To Motivate Social Grouping in Wireless Networks</title><categories>cs.SI cs.NI math.OC</categories><comments>14 pages, submitted to IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ever-increasing data traffic challenges the limited capacity of base-stations
(BSs) in wireless networks. Traditionally, a BS simply unicasts packets to an
individual user without analyzing the packets' content. Practically, many users
might be interested in the same content at the same time, which we refer to as
common interests (e.g., popular live streaming). We first analyze if the BSs
can gain substantial benefit by identifying the common interests, which can be
synchronously delivered to the users by leveraging the wireless broadcast
medium. Interestingly, we show that the identification is no more beneficial if
the channels are terrible.
  To better utilize the identified common interests, we propose local content
sharing (enabled by D2D communications) by motivating physically neighboring
users to form a social group. As users are selfish in practice, an incentive
mechanism is needed to motivate the social grouping. We propose a novel concept
of equal-reciprocal incentive over broadcast communications, which fairly
ensures that each pair of the users in the social group share the same amount
of content with each other. As the equal-reciprocal incentive may restrict the
amount of content shared among the users, we analyze the optimal
equal-reciprocal scheme that maximizes local sharing content. While ensuring
fairness among users, we show that this optimized scheme also maximizes each
user's utility in the social group. Finally, we look at dynamic content
arrivals and extend our scheme successfully by proposing novel dynamic
scheduling algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01804</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01804</id><created>2015-11-05</created><updated>2015-12-15</updated><authors><author><keyname>Hu</keyname><forenames>Shuaiqi</forenames></author><author><keyname>Li</keyname><forenames>Ke</forenames></author><author><keyname>Bao</keyname><forenames>Xudong</forenames></author></authors><title>Wood Species Recognition Based on SIFT Keypoint Histogram</title><categories>cs.CV</categories><comments>CISP 2015</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Traditionally, only experts who are equipped with professional knowledge and
rich experience are able to recognize different species of wood. Applying image
processing techniques for wood species recognition can not only reduce the
expense to train qualified identifiers, but also increase the recognition
accuracy. In this paper, a wood species recognition technique base on Scale
Invariant Feature Transformation (SIFT) keypoint histogram is proposed. We use
first the SIFT algorithm to extract keypoints from wood cross section images,
and then k-means and k-means++ algorithms are used for clustering. Using the
clustering results, an SIFT keypoints histogram is calculated for each wood
image. Furthermore, several classification models, including Artificial Neural
Networks (ANN), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) are
used to verify the performance of the method. Finally, through comparing with
other prevalent wood recognition methods such as GLCM and LBP, results show
that our scheme achieves higher accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01806</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01806</id><created>2015-11-05</created><authors><author><keyname>Hon</keyname><forenames>Wing-Kai</forenames></author><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Liu</keyname><forenames>Fu-Hong</forenames></author><author><keyname>Liu</keyname><forenames>Hsiang-Hsuan</forenames></author><author><keyname>Wang</keyname><forenames>Hung-Lung</forenames></author></authors><title>Flood-it on AT-Free Graphs</title><categories>cs.DM</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solitaire {\sc Flood-it}, or {\sc Honey-Bee}, is a game played on a colored
graph. The player resides in a source vertex. Originally his territory is the
maximal connected, monochromatic subgraph that contains the source. A move
consists of calling a color. This conquers all the nodes of the graph that can
be reached by a monochromatic path of that color from the current territory of
the player. It is the aim of the player to add all vertices to his territory in
a minimal number of moves. We show that the minimal number of moves can be
computed in polynomial time when the game is played on AT-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01807</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01807</id><created>2015-11-05</created><authors><author><keyname>Karandikar</keyname><forenames>Prateek</forenames></author><author><keyname>Schnoebelen</keyname><forenames>Philippe</forenames></author></authors><title>Hierarchy of piecewise-testable languages and complexity of the
  two-variable logic of subsequences</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that FO2(A*,subsequence), the two-variable fragment of the
first-order logic of sequences with the subsequence ordering, can only express
piecewise-testable properties and is decidable with elementary complexity. To
prove this we develop new techniques for bounding the piecewise-testability
level of regular languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01808</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01808</id><created>2015-11-05</created><updated>2015-12-15</updated><authors><author><keyname>Hu</keyname><forenames>Shuaiqi</forenames></author></authors><title>A Hierarchical Key Management Scheme for Wireless Sensor Networks Based
  on Identity-based Encryption</title><categories>cs.CR</categories><comments>ICCC 2015</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Limited resources (such as energy, computing power, storage, and so on) make
it impractical for wireless sensor networks (WSNs) to deploy traditional
security schemes. In this paper, a hierarchical key management scheme is
proposed on the basis of identity-based encryption (IBE).This proposed scheme
not only converts the distributed flat architecture of the WSNs to a
hierarchical architecture for better network management but also ensures the
independence and security of the sub-networks. This paper firstly reviews the
identity-based encryption, particularly, the Boneh-Franklin algorithm. Then a
novel hierarchical key management scheme based on the basic Boneh-Franklin and
Diffie-Hellman (DH) algorithms is proposed. At last, the security and
efficiency of our scheme is discussed by comparing with other identity-based
schemes for flat architecture of WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01818</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01818</id><created>2015-11-05</created><authors><author><keyname>Bodic</keyname><forenames>Pierre Le</forenames></author><author><keyname>Nemhauser</keyname><forenames>George L.</forenames></author></authors><title>An Abstract Model for Branching and its Application to Mixed Integer
  Programming</title><categories>math.OC cs.DS</categories><msc-class>90C11, 90C60, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The selection of branching variables is a key component of branch-and-bound
algorithms for solving Mixed-Integer Programming (MIP) problems since the
quality of the selection procedure is likely to have a significant effect on
the size of the enumeration tree. State-of-the-art procedures base the
selection of variables on their &quot;LP gains&quot;, which is the dual bound improvement
obtained after branching on a variable. There are various ways of selecting
variables depending on their LP gains. However, all methods are evaluated
empirically. In this paper we present a theoretical model for the selection of
branching variables. It is based upon an abstraction of MIPs to a simpler
setting in which it is possible to analytically evaluate the dual bound
improvement of choosing a given variable. We then discuss how the analytical
results can be used to choose branching variables for MIPs, and we give
experimental results that demonstrate the effectiveness of the method on MIPLIB
2010 &quot;tree&quot; instances where we achieve a 5% geometric average time and node
improvement, over the default rule of SCIP, a state-of-the-art MIP solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01821</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01821</id><created>2015-11-05</created><authors><author><keyname>Su</keyname><forenames>Lili</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin H.</forenames></author></authors><title>Fault-Tolerant Distributed Optimization (Part IV): Constrained
  Optimization with Arbitrary Directed Networks</title><categories>cs.DC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of constrained distributed optimization in multi-agent
networks when some of the computing agents may be faulty. In this problem, the
system goal is to have all the non-faulty agents collectively minimize a global
objective given by weighted average of local cost functions, each of which is
initially known to a non-faulty agent only. In particular, we are interested in
the scenario when the computing agents are connected by an arbitrary directed
communication network, some of the agents may suffer from crash faults or
Byzantine faults, and the estimate of each agent is restricted to lie in a
common constraint set. This problem finds its applications in social computing
and distributed large-scale machine learning.
  The fault-tolerant multi-agent optimization problem was first formulated by
Su and Vaidya, and is solved when the local functions are defined over the
whole real line, and the networks are fully-connected. In this report, we
consider arbitrary directed communication networks and focus on the scenario
where, local estimates at the non-faulty agents are constrained, and only local
communication and minimal memory carried across iterations are allowed. In
particular, we generalize our previous results on fully-connected networks and
unconstrained optimization to arbitrary directed networks and constrained
optimization. As a byproduct, we provide a matrix representation for iterative
approximate crash consensus. The matrix representation allows us to
characterize the convergence rate for crash iterative consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01828</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01828</id><created>2015-11-05</created><updated>2015-11-08</updated><authors><author><keyname>Kesal</keyname><forenames>Mustafa</forenames></author></authors><title>Proof of the Most Informative Boolean Function Conjecture</title><categories>cs.IT math.IT</categories><comments>This paper is withdrawn due to a serious flaw in the proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose $\XX{N}$ is a uniformly distributed $N$-dimensional binary vector and
$\YY{N}$ is obtained by passing $\XX{N}$ through a binary symmetric channel
with crossover probability $\alpha$. Recently, Courtade and Kumar postulates
that $I(f(\XX{N});\YY{N})\leq 1-\Be(\alpha)$ for any Boolean function $f$
\cite{courtade}. In this paper, we provide a proof of the correctness of this
conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01830</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01830</id><created>2015-11-05</created><authors><author><keyname>Kalyanam</keyname><forenames>Janani</forenames></author><author><keyname>Quezada</keyname><forenames>Mauricio</forenames></author><author><keyname>Poblete</keyname><forenames>Barbara</forenames></author><author><keyname>Lanckriet</keyname><forenames>Gert</forenames></author></authors><title>Early prediction and characterization of high-impact world events using
  social media</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On-line social networks publish information about an enormous volume of
real-world events almost instantly, becoming a primary source for breaking
news. Many of the events reported in social media can be of high-impact to
society, such as important political decisions, natural disasters and terrorist
actions, but might go unnoticed in their early stages due to the overload of
other information. We ask, is it possible to clearly and quickly identify which
of these news events are going to have substantial impact before they actually
become a trend in the network?
  We investigate real-world news discussed on Twitter for approximately 1 year,
consisting of 5,234 news events that are composed of 43 million messages. We
show that using just the first 5% of the events' lifetime evolution, we are
able to predict with high precision the top 8% that have the most impact. We
observe that events that have high impact present unique characteristics in
terms of how they are adopted by the network and that these qualities are
independent of the event's size and scope. As a consequence, high impact news
events are naturally filtered by the social network, engaging users early on,
much before they are brought to the mainstream audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01838</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01838</id><created>2015-11-05</created><authors><author><keyname>Gimenez</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Moser</keyname><forenames>Georg</forenames></author></authors><title>The Complexity of Interaction (Long Version)</title><categories>cs.PL</categories><acm-class>F.3.2</acm-class><doi>10.1145/2837614.2837646</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the complexity of functional programs written in
the interaction-net computation model, an asynchronous, parallel and confluent
model that generalizes linear-logic proof nets. Employing user-defined sized
and scheduled types, we certify concrete time, space and space-time complexity
bounds for both sequential and parallel reductions of interaction-net programs
by suitably assigning complexity potentials to typed nodes. The relevance of
this approach is illustrated on archetypal programming examples. The provided
analysis is precise, compositional and is, in theory, not restricted to
particular complexity classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01839</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01839</id><created>2015-11-04</created><authors><author><keyname>Sch&#xe4;be</keyname><forenames>Hendrik</forenames></author><author><keyname>Braband</keyname><forenames>Jens</forenames></author></authors><title>Basic requirements for proven-in-use arguments</title><categories>cs.SE</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proven-in-use arguments are needed when pre-developed products with an
in-service history are to be used in different environments than those they
were originally developed for. A product may include software modules or may be
stand-alone integrated hardware and software modules.The topic itself is not
new, but most recent approaches have been based on elementary probability such
as urn models which lead to very restrictive requirements for the system or
software to which it has been applied.
  The aim of this paper is to base the argumentation on a general probabilistic
model based on Grigelionis or Palm Khintchine theorems, so that the results can
be applied to a very general class of products without unnecessary limitations.
The advantage of such an approach is also that the same requirements hold for a
broad class of products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01844</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01844</id><created>2015-11-05</created><updated>2016-01-06</updated><authors><author><keyname>Theis</keyname><forenames>Lucas</forenames></author><author><keyname>Oord</keyname><forenames>A&#xe4;ron van den</forenames></author><author><keyname>Bethge</keyname><forenames>Matthias</forenames></author></authors><title>A note on the evaluation of generative models</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic generative models can be used for compression, denoising,
inpainting, texture synthesis, semi-supervised learning, unsupervised feature
learning, and other tasks. Given this wide range of applications, it is not
surprising that a lot of heterogeneity exists in the way these models are
formulated, trained, and evaluated. As a consequence, direct comparison between
models is often difficult. This article reviews mostly known but often
underappreciated properties relating to the evaluation and interpretation of
generative models with a focus on image models. In particular, we show that
three of the currently most commonly used criteria---average log-likelihood,
Parzen window estimates, and visual fidelity of samples---are largely
independent of each other when the data is high-dimensional. Good performance
with respect to one criterion therefore need not imply good performance with
respect to the other criteria. Our results show that extrapolation from one
criterion to another is not warranted and generative models need to be
evaluated directly with respect to the application(s) they were intended for.
In addition, we provide examples demonstrating that Parzen window estimates
should generally be avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01853</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01853</id><created>2015-11-05</created><authors><author><keyname>Li</keyname><forenames>Pan</forenames></author><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author><author><keyname>Weng</keyname><forenames>Yang</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author></authors><title>Autoregressive Model for Individual Consumption Data - LASSO Selection
  and Significance Test</title><categories>stat.ML cs.SY math.OC</categories><comments>21 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding user flexibility and behavior patterns is becoming increasingly
vital to the design of robust and efficient energy saving programs. Accurate
prediction of consumption is a key part to this understanding. Existing
prediction methods usually have high relative errors that can be larger than
30\%. In this paper, we explore sparsity in users' past data and relationship
between different users to increase prediction accuracy. We show that using
LASSO and significance test techniques, prediction accuracy can be
significantly compared to standard existing algorithms. We use mean absolute
percentage error (MAPE) as the criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01854</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01854</id><created>2015-11-05</created><updated>2016-01-21</updated><authors><author><keyname>Rana</keyname><forenames>Swapan</forenames></author><author><keyname>Parashar</keyname><forenames>Preeti</forenames></author><author><keyname>Lewenstein</keyname><forenames>Maciej</forenames></author></authors><title>Trace-distance measure of coherence</title><categories>quant-ph cs.IT math.IT math.OA</categories><comments>7 pages, 1 figure; published version</comments><journal-ref>Phys. Rev. A 93, 012110 (2016)</journal-ref><doi>10.1103/PhysRevA.93.012110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that trace distance measure of coherence is a strong monotone for all
qubit and, so called, $X$ states. An expression for the trace distance
coherence for all pure states and a semi definite program for arbitrary states
is provided. We also explore the relation between $l_1$-norm and relative
entropy based measures of coherence, and give a sharp inequality connecting the
two. In addition, it is shown that both $l_p$-norm- and Schatten-$p$-norm-based
measures violate the (strong) monotonicity for all $p\in(1,\infty)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01861</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01861</id><created>2015-11-05</created><authors><author><keyname>Thij</keyname><forenames>Marijn ten</forenames></author><author><keyname>Bhulai</keyname><forenames>Sandjai</forenames></author></authors><title>Modeling trend progression through an extension of the Polya Urn Process</title><categories>cs.SI math.PR physics.soc-ph</categories><comments>11 pages, 2 figures, NetSci-X Conference, Wroclaw, Poland, 11-13
  January 2016. arXiv admin note: text overlap with arXiv:1502.00166</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowing how and when trends are formed is a frequently visited research goal.
In our work, we focus on the progression of trends through (social) networks.
We use a random graph (RG) model to mimic the progression of a trend through
the network. The context of the trend is not included in our model. We show
that every state of the RG model maps to a state of the Polya process. We find
that the limit of the component size distribution of the RG model shows
power-law behaviour. These results are also supported by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01862</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01862</id><created>2015-11-05</created><updated>2015-11-10</updated><authors><author><keyname>Chen</keyname><forenames>Haoming</forenames></author><author><keyname>Saxena</keyname><forenames>Ankur</forenames></author><author><keyname>Fernandes</keyname><forenames>Felix</forenames></author></authors><title>On Intra Prediction for Screen Content Video Coding</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Screen content coding (SCC) is becoming increasingly important in various
applications, such as desktop sharing, video conferencing, and remote
education. When compared to natural camera- captured content, screen content
has different characteristics, in particular sharper edges. In this paper, we
propose a novel intra prediction scheme for screen content video. In the
proposed scheme, bilinear interpolation in angular intra prediction in HEVC is
selectively replaced by nearest-neighbor intra prediction to preserve the sharp
edges in screen content video. We present three different variants of the
proposed nearest neighbor prediction algorithm: two implicit methods where both
the encoder, and the decoder derive whether to perform nearest neighbor
prediction or not based on either (a) the sum of the absolute difference, or
(b) the difference between the boundary pixels from which prediction is
performed; and another variant where Rate-Distortion-Optimization (RDO) search
is performed at the encoder to decide whether or not to use the nearest
neighbor interpolation, and explicitly signaled to the decoder. We also discuss
the various underlying trade-offs in terms of the complexity of the three
variants. All the three proposed variants provide significant gains over HEVC,
and simulation results show that average gains of 3.3% BD-bitrate in
Intra-frame coding are achieved by the RDO variant for screen content video. To
the best of our knowledge, this is the first paper that 1) points out current
HEVC intra prediction scheme with bilinear interpolation does not work
efficiently for screen content video and 2) uses different filters adaptively
in the HEVC intra prediction interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01865</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01865</id><created>2015-11-05</created><updated>2016-01-08</updated><authors><author><keyname>Rad</keyname><forenames>Nastaran Mohammadian</forenames></author><author><keyname>Bizzego</keyname><forenames>Andrea</forenames></author><author><keyname>Kia</keyname><forenames>Seyed Mostafa</forenames></author><author><keyname>Jurman</keyname><forenames>Giuseppe</forenames></author><author><keyname>Venuti</keyname><forenames>Paola</forenames></author><author><keyname>Furlanello</keyname><forenames>Cesare</forenames></author></authors><title>Convolutional Neural Network for Stereotypical Motor Movement Detection
  in Autism</title><categories>cs.NE cs.CV cs.LG</categories><comments>5th NIPS Workshop on Machine Learning and Interpretation in
  Neuroimaging 2015 (https://sites.google.com/site/mliniworkshop2015/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autism Spectrum Disorders (ASDs) are often associated with specific atypical
postural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have
a specific visibility. While the identification and the quantification of SMM
patterns remain complex, its automation would provide support to accurate
tuning of the intervention in the therapy of autism. Therefore, it is essential
to develop automatic SMM detection systems in a real world setting, taking care
of strong inter-subject and intra-subject variability. Wireless accelerometer
sensing technology can provide a valid infrastructure for real-time SMM
detection, however such variability remains a problem also for machine learning
methods, in particular whenever handcrafted features extracted from
accelerometer signal are considered. Here, we propose to employ the deep
learning paradigm in order to learn discriminating features from multi-sensor
accelerometer signals. Our results provide preliminary evidence that feature
learning and transfer learning embedded in the deep architecture achieve higher
accurate SMM detectors in longitudinal scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01868</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01868</id><created>2015-11-05</created><authors><author><keyname>Li</keyname><forenames>Zhepeng</forenames></author><author><keyname>Fang</keyname><forenames>Xiao</forenames></author><author><keyname>Sheng</keyname><forenames>Olivia</forenames></author></authors><title>A Survey of Link Recommendation for Social Networks: Methods,
  Theoretical Foundations, and Future Research Directions</title><categories>cs.SI physics.soc-ph</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link recommendation has attracted significant attentions from both industry
practitioners and academic researchers. In industry, link recommendation has
become a standard and most important feature in online social networks,
prominent examples of which include &quot;People You May Know&quot; on LinkedIn and &quot;You
May Know&quot; on Google+. In academia, link recommendation has been and remains a
highly active research area. This paper surveys state-of-the-art link
recommendation methods, which can be broadly categorized into learning-based
methods and proximity-based methods. We further identify social and economic
theories, such as social interaction theory, that underlie these methods and
explain from a theoretical perspective why a link recommendation method works.
Finally, we propose to extend link recommendation research in several
directions that include utility-based link recommendation, diversity of link
recommendation, link recommendation from incomplete data, and experimental
study of link recommendation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01870</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01870</id><created>2015-11-05</created><authors><author><keyname>Wilson</keyname><forenames>Andrew Gordon</forenames></author><author><keyname>Dann</keyname><forenames>Christoph</forenames></author><author><keyname>Nickisch</keyname><forenames>Hannes</forenames></author></authors><title>Thoughts on Massively Scalable Gaussian Processes</title><categories>cs.LG cs.AI stat.ME stat.ML</categories><comments>25 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a framework and early results for massively scalable Gaussian
processes (MSGP), significantly extending the KISS-GP approach of Wilson and
Nickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs)
on billions of datapoints, without requiring distributed inference, or severe
assumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GP
learning and inference to $O(n)$, and the standard $O(n^2)$ complexity per test
point prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices as
Kronecker products of Toeplitz matrices approximated by circulant matrices.
This multi-level circulant approximation allows one to unify the orthogonal
computational benefits of fast Kronecker and Toeplitz approaches, and is
significantly faster than either approach in isolation; 2) local kernel
interpolation and inducing points to allow for arbitrarily located data inputs,
and $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-block
structure (BTTB), which enables fast inference and learning when
multidimensional Kronecker structure is not present; and 4) projections of the
input space to flexibly model correlated inputs and high dimensional data. The
ability to handle many ($m \approx n$) inducing points allows for near-exact
accuracy and large scale kernel learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01874</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01874</id><created>2015-11-05</created><updated>2015-11-10</updated><authors><author><keyname>Grigore</keyname><forenames>Radu</forenames></author><author><keyname>Yang</keyname><forenames>Hongseok</forenames></author></authors><title>Abstraction Refinement Guided by a Learnt Probabilistic Model</title><categories>cs.PL cs.SE</categories><comments>POPL2016</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The core challenge in designing an effective static program analysis is to
find a good program abstraction -- one that retains only details relevant to a
given query. In this paper, we present a new approach for automatically finding
such an abstraction. Our approach uses a pessimistic strategy, which can
optionally use guidance from a probabilistic model. Our approach applies to
parametric static analyses implemented in Datalog, and is based on
counterexample-guided abstraction refinement. For each untried abstraction, our
probabilistic model provides a probability of success, while the size of the
abstraction provides an estimate of its cost in terms of analysis time.
Combining these two metrics, probability and cost, our refinement algorithm
picks an optimal abstraction. Our probabilistic model is a variant of the
Erdos-Renyi random graph model, and it is tunable by what we call
hyperparameters. We present a method to learn good values for these
hyperparameters, by observing past runs of the analysis on an existing
codebase. We evaluate our approach on an object sensitive pointer analysis for
Java programs, with two client analyses (PolySite and Downcast).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01887</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01887</id><created>2015-11-05</created><updated>2015-12-15</updated><authors><author><keyname>Malyshkin</keyname><forenames>Vladislav Gennadievich</forenames></author></authors><title>Radon-Nikodym approximation in application to image analysis</title><categories>cs.CV</categories><comments>Images interpolated with d_x=d_y=100 are added to show the
  practicality of high order moments calculation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an image pixel information can be converted to the moments of some basis
$Q_k$, e.g. Fourier-Mellin, Zernike, monomials, etc. Given sufficient number of
moments pixel information can be completely recovered, for insufficient number
of moments only partial information can be recovered and the image
reconstruction is, at best, of interpolatory type. Standard approach is to
present interpolated value as a linear combination of basis functions, what is
equivalent to least squares expansion. However, recent progress in numerical
stability of moments estimation allows image information to be recovered from
moments in a completely different manner, applying Radon-Nikodym type of
expansion, what gives the result as a ratio of two quadratic forms of basis
functions. In contrast with least squares the Radon-Nikodym approach has
oscillation near the boundaries very much suppressed and does not diverge
outside of basis support. While least squares theory operate with vectors
$&lt;fQ_k&gt;$, Radon-Nikodym theory operates with matrices $&lt;fQ_jQ_k&gt;$, what make
the approach much more suitable to image transforms and statistical property
estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01891</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01891</id><created>2015-11-05</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Vortices determine the dynamics of biodiversity in cyclical interactions
  with protection spillovers</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>17 pages, 9 figures; accepted for publication in New Journal of
  Physics</comments><journal-ref>New J. Phys. 17 (2015) 113033</journal-ref><doi>10.1088/1367-2630/17/11/113033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If rock beats scissors and scissors beat paper, one might assume that rock
beats paper too. But this is not the case for intransitive relationships that
make up the famous rock-paper-scissors game. However, the sole presence of
paper might prevent rock from beating scissors, simply because paper beats
rock. This is the blueprint for the rock-paper-scissors game with protection
spillovers, which has recently been introduced as a new paradigm for
biodiversity in well-mixed microbial populations. Here we study the game in
structured populations, demonstrating that protection spillovers give rise to
spatial patterns that are impossible to observe in the classical
rock-paper-scissors game. We show that the spatiotemporal dynamics of the
system is determined by the density of stable vortices, which may ultimately
transform to frozen states, to propagating waves, or to target waves with
reversed propagation direction, depending further on the degree and type of
randomness in the interactions among the species. If vortices are rare, the
fixation to waves and complex oscillatory solutions is likelier. Moreover,
annealed randomness in interactions favors the emergence of target waves, while
quenched randomness favors collective synchronization. Our results demonstrate
that protection spillovers may fundamentally change the dynamics of cyclic
dominance in structured populations, and they outline the possibility of
programming pattern formation in microbial populations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01892</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01892</id><created>2015-11-04</created><authors><author><keyname>Tesoro</keyname><forenames>S.</forenames></author><author><keyname>Ahnert</keyname><forenames>S. E.</forenames></author></authors><title>Non-deterministic self-assembly of two tile types on a lattice</title><categories>cs.ET cond-mat.soft</categories><comments>10 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-assembly is ubiquitous in nature, particularly in biology, where it
underlies the formation of protein quaternary structure and protein
aggregation. Quaternary structure assembles deterministically and performs a
wide range of important functions in the cell, whereas protein aggregation is
the hallmark of a number of diseases and represents a non-deterministic
self-assembly process. Here we build on previous work on a lattice model of
deterministic self-assembly to investigate non-deterministic self-assembly of
single lattice tiles and mixtures of two tiles at varying relative
concentrations. Despite limiting the simplicity of the model to two interface
types, which results in 13 topologically distinct single tiles and 106
topologically distinct sets of two tiles, we observe a wide variety of
concentration-dependent behaviours. Several two-tile sets display critical
behaviours in form of a sharp transition from bound to unbound structures as
the relative concentration of one tile to another increases. Other sets exhibit
gradual monotonic changes in structural density, or non-monotonic changes,
while again others show no concentration dependence at all. We catalogue this
extensive range of behaviours and present a model that provides a reasonably
good estimate of the critical concentrations for a subset of the critical
transitions. In addition we show that the structures resulting from these tile
sets are fractal, with one of two different fractal dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01937</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01937</id><created>2015-11-05</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author><author><keyname>Ben-David</keyname><forenames>Shalev</forenames></author><author><keyname>Kothari</keyname><forenames>Robin</forenames></author></authors><title>Separations in query complexity using cheat sheets</title><categories>quant-ph cs.CC</categories><comments>31 pages; subsumes arXiv:1506.08106</comments><report-no>MIT-CTP #4730</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a power 2.5 separation between bounded-error randomized and quantum
query complexity for a total Boolean function, refuting the widely believed
conjecture that the best such separation could only be quadratic (from Grover's
algorithm). We also present a total function with a power 4 separation between
quantum query complexity and approximate polynomial degree, showing severe
limitations on the power of the polynomial method. Finally, we exhibit a total
function with a quadratic gap between quantum query complexity and certificate
complexity, which is optimal (up to log factors). These separations are shown
using a new, general technique that we call the cheat sheet technique. The
technique is based on a generic transformation that converts any (possibly
partial) function into a new total function with desirable properties for
showing separations. The framework also allows many known separations,
including some recent breakthrough results of Ambainis et al., to be shown in a
unified manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01942</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01942</id><created>2015-11-05</created><authors><author><keyname>Babanezhad</keyname><forenames>Reza</forenames></author><author><keyname>Ahmed</keyname><forenames>Mohamed Osama</forenames></author><author><keyname>Virani</keyname><forenames>Alim</forenames></author><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author><author><keyname>Kone&#x10d;n&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>Sallinen</keyname><forenames>Scott</forenames></author></authors><title>Stop Wasting My Gradients: Practical SVRG</title><categories>cs.LG math.OC stat.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and analyze several strategies for improving the performance of
stochastic variance-reduced gradient (SVRG) methods. We first show that the
convergence rate of these methods can be preserved under a decreasing sequence
of errors in the control variate, and use this to derive variants of SVRG that
use growing-batch strategies to reduce the number of gradient calculations
required in the early iterations. We further (i) show how to exploit support
vectors to reduce the number of gradient computations in the later iterations,
(ii) prove that the commonly-used regularized SVRG iteration is justified and
improves the convergence rate, (iii) consider alternate mini-batch selection
strategies, and (iv) consider the generalization error of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01946</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01946</id><created>2015-11-05</created><authors><author><keyname>Ragel</keyname><forenames>Roshan G.</forenames></author><author><keyname>Ambrose</keyname><forenames>Jude A.</forenames></author><author><keyname>Parameswaran</keyname><forenames>Sri</forenames></author></authors><title>SecureD: A Secure Dual Core Embedded Processor</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security of embedded computing systems is becoming of paramount concern as
these devices become more ubiquitous, contain personal information and are
increasingly used for financial transactions. Security attacks targeting
embedded systems illegally gain access to the information in these devices or
destroy information. The two most common types of attacks embedded systems
encounter are code-injection and power analysis attacks. In the past, a number
of countermeasures, both hardware- and software-based, were proposed
individually against these two types of attacks. However, no single system
exists to counter both of these two prominent attacks in a processor based
embedded system. Therefore, this paper, for the first time, proposes a
hardware/software based countermeasure against both code-injection attacks and
power analysis based side-channel attacks in a dual core embedded system. The
proposed processor, named SecureD, has an area overhead of just 3.80% and an
average runtime increase of 20.0% when compared to a standard dual processing
system. The overhead were measured using a set of industry standard application
benchmarks, with two encryption and five other programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01953</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01953</id><created>2015-11-05</created><authors><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Wu</keyname><forenames>Dapeng Oliver</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>Weighted Sum-Throughput Maximization for MIMO Broadcast Channel: Energy
  Harvesting Under System Imperfection</title><categories>cs.IT math.IT</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a MIMO broadcast channel under the energy harvesting (EH)
constraint and the peak power constraint is investigated. The transmitter is
equipped with a hybrid energy storage system consisting of a perfect super
capacitor (SC) and an inefficient battery, where both elements have limited
energy storage capacities. In addition, the effect of data processing circuit
power consumption is also addressed. To be specific, two extreme cases are
studied here, where the first assumes ideal/zero circuit power consumption and
the second considers a positive constant circuit power consumption where the
circuit is always operating at its highest power level. The performance of
these two extreme cases hence serve as the upper bound and the lower bound of
the system performance in practice, respectively. In this setting, the offline
scheduling with ideal and maximum circuit power consumptions are investigated.
The associated optimization problems are formulated and solved in terms of
weighted throughput optimization. Further, we extend to a general circuit power
consumption model. To complement this work, some intuitive online policies are
presented for all cases. Interestingly, for the case with maximum circuit power
consumption, a close-to-optimal online policy is presented and its performance
is shown to be comparable to its offline counterpart in the numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01954</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01954</id><created>2015-11-05</created><authors><author><keyname>M.</keyname><forenames>Jose Oramas</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>Recovering hard-to-find object instances by sampling context-based
  object proposals</title><categories>cs.CV</categories><comments>Preprint submitted to CVIU (12 Pages, 8 Figures, 1 Table)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on improving object detection performance in terms of
recall. We propose a post-detection stage during which we explore the image
with the objective of recovering missed detections. This exploration is
performed by sampling object proposals in the image. We analyse four different
strategies to perform this sampling, giving special attention to strategies
that exploit spatial relations between objects. In addition, we propose a novel
method to discover higher-order relations between groups of objects.
Experiments on the challenging KITTI dataset show that our proposed
relations-based proposal generation strategies can help improving recall at the
cost of a relatively low amount of object proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01955</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01955</id><created>2015-11-05</created><authors><author><keyname>Ndiaye</keyname><forenames>Ousmane</forenames></author></authors><title>One Cyclic Codes over $\mathbb{F}_{p^k} + v\mathbb{F}_{p^k} +
  v^2\mathbb{F}_{p^k} + ... + v^r\mathbb{F}_{p^k}$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate cyclic code over the ring $\mathbb{F}_{p^k} +
v\mathbb{F}_{p^k} + v^2\mathbb{F}_{p^k} + ... + v^r\mathbb{F}_{p^k}$, where
$v^{r+1}=v$, $p$ a prime number, $r&gt;1$ and $\gcd(r,p)=1$, we prove as
generalisation of P. Sol\'e et al. in 2015 that these codes are principally
generated, give generator polynomial and idempotent depending on idempotents
over this ring as response to an open problem related by J. QIAN et al. in
2005. we also give a gray map and proprieties of the related dual code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01957</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01957</id><created>2015-11-05</created><updated>2015-11-29</updated><authors><author><keyname>Su</keyname><forenames>Weijie</forenames></author><author><keyname>Bogdan</keyname><forenames>Malgorzata</forenames></author><author><keyname>Candes</keyname><forenames>Emmanuel</forenames></author></authors><title>False Discoveries Occur Early on the Lasso Path</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>Added references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In regression settings where explanatory variables have very low correlations
and where there are relatively few effects each of large magnitude, it is
commonly believed that the Lasso shall be able to find the important variables
with few errors---if any. In contrast, this paper shows that this is not the
case even when the design variables are stochastically independent. In a regime
of linear sparsity, we demonstrate that true features and null features are
always interspersed on the Lasso path, and that this phenomenon occurs no
matter how strong the effect sizes are. We derive a sharp asymptotic trade-off
between false and true positive rates or, equivalently, between measures of
type I and type II errors along the Lasso path. This trade-off states that if
we ever want to achieve a type II error (false negative rate) under a given
threshold, then anywhere on the Lasso path the type I error (false positive
rate) will need to exceed a given threshold so that we can never have both
errors at a low level at the same time. Our analysis uses tools from
approximate message passing (AMP) theory as well as novel elements to deal with
a possibly adaptive selection of the Lasso regularizing parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01960</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01960</id><created>2015-11-05</created><authors><author><keyname>Baral</keyname><forenames>Chitta</forenames></author><author><keyname>Gelfond</keyname><forenames>Gregory</forenames></author><author><keyname>Pontelli</keyname><forenames>Enrico</forenames></author><author><keyname>Son</keyname><forenames>Tran Cao</forenames></author></authors><title>An Action Language for Multi-Agent Domains: Foundations</title><categories>cs.AI</categories><comments>55 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-agent domains (MADs), an agent's action may not just change the
world and the agent's knowledge and beliefs about the world, but also may
change other agents' knowledge and beliefs about the world and their knowledge
and beliefs about other agents' knowledge and beliefs about the world. The
goals of an agent in a multi-agent world may involve manipulating the knowledge
and beliefs of other agents' and again, not just their knowledge/belief about
the world, but also their knowledge about other agents' knowledge about the
world. Our goal is to present an action language (mA+) that has the necessary
features to address the above aspects in representing and RAC in MADs. mA+
allows the representation of and reasoning about different types of actions
that an agent can perform in a domain where many other agents might be
present---such as world-altering actions, sensing actions, and
announcement/communication actions. It also allows the specification of agents'
dynamic awareness of action occurrences which has future implications on what
agents' know about the world and other agents' knowledge about the world. mA+
considers three different types of awareness: full,- partial- awareness, and
complete oblivion of an action occurrence and its effects. This keeps the
language simple, yet powerful enough to address a large variety of knowledge
manipulation scenarios in MADs. The semantics of mA+ relies on the notion of
state, which is described by a pointed Kripke model and is used to encode the
agent's knowledge and the real state of the world. It is defined by a
transition function that maps pairs of actions and states into sets of states.
We illustrate properties of the action theories, including properties that
guarantee finiteness of the set of initial states and their practical
implementability. Finally, we relate mA+ to other related formalisms that
contribute to RAC in MADs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01964</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01964</id><created>2015-11-05</created><authors><author><keyname>Apar&#xed;cio</keyname><forenames>David</forenames></author><author><keyname>Ribeiro</keyname><forenames>Pedro</forenames></author><author><keyname>Silva</keyname><forenames>Fernando</forenames></author></authors><title>Network comparison using directed graphlets</title><categories>cs.SI physics.soc-ph q-bio.MN</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With recent advances in high-throughput cell biology the amount of cellular
biological data has grown drastically. Such data is often modeled as graphs
(also called networks) and studying them can lead to new insights into
molecule-level organization. A possible way to understand their structure is by
analysing the smaller components that constitute them, namely network motifs
and graphlets. Graphlets are particularly well suited to compare networks and
to assess their level of similarity but are almost always used as small
undirected graphs of up to five nodes, thus limiting their applicability in
directed networks. However, a large set of interesting biological networks such
as metabolic, cell signaling or transcriptional regulatory networks are
intrinsically directional, and using metrics that ignore edge direction may
gravely hinder information extraction. The applicability of graphlets is
extended to directed networks by considering the edge direction of the
graphlets. We tested our approach on a set of directed biological networks and
verified that they were correctly grouped by type using directed graphlets.
However, enumerating all graphlets in a large network is a computationally
demanding task. Our implementation addresses this concern by using a
state-of-the-art data structure, the g-trie, which is able to greatly reduce
the necessary computation. We compared our tool, gtrieScanner, to other
state-of-the art methods and verified that it is the fastest general tool for
graphlet counting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01966</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01966</id><created>2015-11-05</created><updated>2015-11-09</updated><authors><author><keyname>Parekh</keyname><forenames>Ankit</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan W.</forenames></author></authors><title>Enhanced Low-Rank Matrix Approximation</title><categories>cs.CV cs.LG math.OC</categories><comments>9 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to estimate low-rank matrices by formulating a convex
optimization problem with non-convex regularization. We employ parameterized
non-convex penalty functions in order to estimate the non-zero singular values
more accurately than the nuclear norm. We further provide a closed form
solution for the global optimum of the proposed objective function (sum of data
fidelity and the non-convex regularizer). The closed form solution reduces to
the singular value thresholding method as a special case. The proposed &quot;convex
non-convex&quot; formulation for the low-rank matrix approximation problem is shown
to outperform several other methods. The proposed method is also applied to the
problem of image denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01969</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01969</id><created>2015-11-05</created><updated>2015-11-23</updated><authors><author><keyname>Liu</keyname><forenames>Qiang</forenames></author><author><keyname>Wu</keyname><forenames>Gang</forenames></author><author><keyname>Guo</keyname><forenames>Yingchu</forenames></author><author><keyname>Zhang</keyname><forenames>Yusong</forenames></author></authors><title>Energy Efficient Resource Allocation for Control Data Separation
  Architecture based H-CRAN with Heterogeneous Fronthaul</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures, submitted to IEEE ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Control data separation architecture (CDSA) is a more efficient architecture
to overcome the overhead issue than the conventional cellular networks,
especially for the huge bursty traffic like Internet of Things, and
over-the-top (OTT) content service. In this paper, we study the optimization
issue of network energy efficiency of the CDSA-based heterogeneous cloud radio
access networks (H-CRAN) networks, which has heterogeneous fronthaul between
control base station (CBS) and data base stations (DBSs). We first present a
modified power consumption model for the CDSA-based H-CRAN, and then formulate
the optimization problem with constraint of overall capacity of wireless
fronthaul. We work out the resource assignment and power allocation by the
convex relaxation approach Using fractional programming method and Lagrangian
dual decomposition method, we derive the close-form optimal solution and verify
it by comprehensive system-level simulation. The simulation results show that
our proposed algorithm has 8% EE gain compared to the static algorithm, and the
CDSA-based H-CRAN networks can achieve up to 16% EE gain compared to the
conventional network even under strict fronthaul capacity limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01974</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01974</id><created>2015-11-05</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Zhang</keyname><forenames>Han</forenames></author><author><keyname>Gelernter</keyname><forenames>Judith</forenames></author></authors><title>Multi-lingual Geoparsing based on Machine Translation</title><categories>cs.CL cs.IR</categories><comments>7 pages, 4 figures,</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our method for multi-lingual geoparsing uses monolingual tools and resources
along with machine translation and alignment to return location words in many
languages. Not only does our method save the time and cost of developing
geoparsers for each language separately, but also it allows the possibility of
a wide range of language capabilities within a single interface. We evaluated
our method in our LanguageBridge prototype on location named entities using
newswire, broadcast news and telephone conversations in English, Arabic and
Chinese data from the Linguistic Data Consortium (LDC). Our results for
geoparsing Chinese and Arabic text using our multi-lingual geoparsing method
are comparable to our results for geoparsing English text with our English
tools. Furthermore, experiments using our machine translation approach results
in accuracy comparable to results from the same data that was translated
manually.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01975</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01975</id><created>2015-11-05</created><authors><author><keyname>Jog</keyname><forenames>Varun</forenames></author><author><keyname>Loh</keyname><forenames>Po-Ling</forenames></author></authors><title>Persistence of centrality in random growing trees</title><categories>math.PR cs.DM cs.SI math.ST stat.TH</categories><comments>21 pages</comments><msc-class>60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate properties of node centrality in random growing tree models.
We focus on a measure of centrality that computes the maximum subtree size of
the tree rooted at each node, with the most central node being the tree
centroid. For random trees grown according to a preferential attachment model,
a uniform attachment model, or a diffusion processes over a regular tree, we
prove that a single node persists as the tree centroid after a finite number of
steps, with probability 1. Furthermore, this persistence property generalizes
to the top $K \ge 1$ nodes with respect to the same centrality measure. We also
establish necessary and sufficient conditions for the size of an initial seed
graph required to ensure persistence of a particular node with probability
$1-\epsilon$, as a function of $\epsilon$: In the case of preferential and
uniform attachment models, we derive bounds for the size of an initial hub
constructed around the special node. In the case of a diffusion process over a
regular tree, we derive bounds for the radius of an initial ball centered
around the special node. Our necessary and sufficient conditions match up to
constant factors for preferential attachment and diffusion tree models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01976</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01976</id><created>2015-11-05</created><authors><author><keyname>Maghsudi</keyname><forenames>Setareh</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Distributed User Association in Energy Harvesting Small Cell Networks: A
  Competitive Market Model with Uncertainty</title><categories>cs.GT cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider a distributed user association problem in the downlink of a small
cell network, where small cells obtain the required energy for providing
wireless services to users through ambient energy harvesting. Since energy
harvesting is opportunistic in nature, the amount of harvested energy is a
random variable, without a priori known characteristics. We model the network
as a competitive market with uncertainty, where self-interested small cells,
modeled as consumers, are willing to maximize their utility scores by selecting
users, represented by commodities. The utility scores of small cells depend on
the amount of harvested energy, formulated as natures' state. Under this model,
the problem is to assign users to small cells, so that the aggregate network
utility is maximized. The solution is the general equilibrium under
uncertainty, also called Arrow-Debreu equilibrium. We show that in our setting,
such equilibrium not only exists, but also is unique and is Pareto optimal in
the sense of expected aggregate network utility. We use the Walras' tatonnement
process with some modifications in order to implement the equilibrium
efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01984</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01984</id><created>2015-11-05</created><authors><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Joint Investment and Operation of Microgrid</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a theoretical framework for the joint optimization
of investment and operation of a microgrid, taking the impact of energy
storage, renewable energy integration, and demand response into consideration.
We first study the renewable energy generations in Hong kong, and identify the
potential benefit of mixed deployment of solar and wind energy generations.
Then we model the joint investment and operation as a two-period stochastic
programming program. In period-1, the microgrid operator makes the optimal
investment decisions on the capacities of solar power generation, wind power
generation, and energy storage. In period-2, the operator coordinates the power
supply and demand in the microgrid to minimize the operating cost. We design a
decentralized algorithm for computing the optimal pricing and power consumption
in period-2, based on which we solve the optimal investment problem in
period-1. We also study the impact of prediction error of renewable energy
generation on the portfolio investment using robust optimization framework.
Using realistic meteorological data obtained from the Hong Kong observatory, we
numerically characterize the optimal portfolio investment decisions, optimal
day-ahead pricing and power scheduling, and demonstrate the advantage of using
mixed renewable energy and demand response in terms of reducing investment
cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.01994</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.01994</id><created>2015-11-06</created><authors><author><keyname>Yarkony</keyname><forenames>Julian</forenames></author></authors><title>Next Generation Multicuts for Semi-Planar Graphs</title><categories>cs.CV cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of multicut segmentation. We introduce modified versions
of the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our work
to natural image segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02006</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02006</id><created>2015-11-06</created><authors><author><keyname>Cauwet</keyname><forenames>Marie-Liesse</forenames><affiliation>TAO, LRI</affiliation></author><author><keyname>Teytaud</keyname><forenames>Olivier</forenames><affiliation>TAO, LRI</affiliation></author><author><keyname>Liang</keyname><forenames>Hua-Min</forenames><affiliation>NCTU</affiliation></author><author><keyname>Yen</keyname><forenames>Shi-Jim</forenames><affiliation>NCTU</affiliation></author><author><keyname>Lin</keyname><forenames>Hung-Hsuan</forenames><affiliation>NCTU</affiliation></author><author><keyname>Wu</keyname><forenames>I-Chen</forenames><affiliation>NCTU</affiliation></author><author><keyname>Cazenave</keyname><forenames>Tristan</forenames><affiliation>LAMSADE</affiliation></author><author><keyname>Saffidine</keyname><forenames>Abdallah</forenames><affiliation>LAMSADE</affiliation></author></authors><title>Depth, balancing, and limits of the Elo model</title><categories>cs.GT</categories><proxy>ccsd</proxy><journal-ref>IEEE Conference on Computational Intelligence and Games 2015, Aug
  2015, Tainan, Taiwan. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  -Much work has been devoted to the computational complexity of games.
However, they are not necessarily relevant for estimating the complexity in
human terms. Therefore, human-centered measures have been proposed, e.g. the
depth. This paper discusses the depth of various games, extends it to a
continuous measure. We provide new depth results and present tool
(given-first-move, pie rule, size extension) for increasing it. We also use
these measures for analyzing games and opening moves in Y, NoGo, Killall Go,
and the effect of pie rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02012</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02012</id><created>2015-11-06</created><authors><author><keyname>Zhou</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Zhuo</keyname><forenames>Zhao</forenames></author><author><keyname>Liao</keyname><forenames>Hao</forenames></author><author><keyname>Fu</keyname><forenames>Zhong-Qian</forenames></author><author><keyname>Cai</keyname><forenames>Shi-Min</forenames></author></authors><title>Enhancing speed of pinning synchronizability: low-degree nodes with high
  feedback gains</title><categories>physics.soc-ph cs.SY nlin.AO</categories><comments>23 pages, 5 figures. To be accepted by Scientific Reports</comments><msc-class>93Bxx, 05Cxx</msc-class><acm-class>B.1.2; B.1.3; C.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlling complex networks is of paramount importance in science and
engineering. Despite recent efforts to improve controllability and synchronous
strength, little attention has been paid to the speed of pinning
synchronizability (rate of convergence in pinning control) and the
corresponding pinning node selection. To address this issue, we propose a
hypothesis to restrict the control cost, then build a linear matrix inequality
related to the speed of pinning controllability. By solving the inequality, we
obtain both the speed of pinning controllability and optimal control strength
(feedback gains in pinning control) for all nodes. Interestingly, some
low-degree nodes are able to achieve large feedback gains, which suggests that
they have high influence on controlling system. In addition, when choosing
nodes with high feedback gains as pinning nodes, the controlling speed of real
systems is remarkably enhanced compared to that of traditional large-degree and
large-betweenness selections. Thus, the proposed approach provides a novel way
to investigate the speed of pinning controllability and can evoke other
effective heuristic pinning node selections for large-scale systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02014</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02014</id><created>2015-11-06</created><updated>2016-02-22</updated><authors><author><keyname>Koplenig</keyname><forenames>Alexander</forenames></author><author><keyname>Mueller-Spitzer</keyname><forenames>Carolin</forenames></author></authors><title>Population size predicts lexical diversity, but so does the mean sea
  level - why it is important to correctly account for the structure of
  temporal data</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to demonstrate why it is important to correctly account for the
(serial dependent) structure of temporal data, we document an apparently
spectacular relationship between population size and lexical diversity: for
five out of seven investigated languages, there is a strong relationship
between population size and lexical diversity of the primary language in this
country. We show that this relationship is the result of a misspecified model
that does not consider the temporal aspect of the data by presenting a similar
but nonsensical relationship between the global annual mean sea level and
lexical diversity. Given the fact that in the recent past, several studies were
published that present surprising links between different economic, cultural,
political and (socio-)demographical variables on the one hand and cultural or
linguistic characteristics on the other hand, but seem to suffer from exactly
this problem, we explain the cause of the misspecification and show that it has
profound consequences. We demonstrate how simple transformation of the time
series can often solve problems of this type and argue that the evaluation of
the plausibility of a relationship is important in this context. We hope that
our paper will help both researchers and reviewers to understand why it is
important to use special models for the analysis of data with a natural
temporal ordering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02023</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02023</id><created>2015-11-06</created><authors><author><keyname>Abbasnejad</keyname><forenames>Mohammadamin</forenames></author><author><keyname>Masnadi-Shirazi</keyname><forenames>Mohammad Ali</forenames></author></authors><title>Facial Expression Recognition Using Sparse Gaussian Conditional Random
  Field</title><categories>cs.CV</categories><comments>http://waset.org/abstracts/computer-and-information-engineering/26245. arXiv
  admin note: text overlap with arXiv:1509.01343 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of expression and facial Action Units (AUs) detection are very
important tasks in fields of computer vision and Human Computer Interaction
(HCI) due to the wide range of applications in human life. Many works has been
done during the past few years which has their own advantages and
disadvantages. In this work we present a new model based on Gaussian
Conditional Random Field. We solve our objective problem using ADMM and we show
how well the proposed model works. We train and test our work on two facial
expression datasets, CK+ and RU-FACS. Experimental evaluation shows that our
proposed approach outperform state of the art expression recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02024</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02024</id><created>2015-11-06</created><authors><author><keyname>Keerthi</keyname><forenames>S. Sathiya</forenames></author><author><keyname>Schnabel</keyname><forenames>Tobias</forenames></author><author><keyname>Khanna</keyname><forenames>Rajiv</forenames></author></authors><title>Towards a Better Understanding of Predict and Count Models</title><categories>cs.LG cs.CL</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper, Levy and Goldberg pointed out an interesting connection
between prediction-based word embedding models and count models based on
pointwise mutual information. Under certain conditions, they showed that both
models end up optimizing equivalent objective functions. This paper explores
this connection in more detail and lays out the factors leading to differences
between these models. We find that the most relevant differences from an
optimization perspective are (i) predict models work in a low dimensional space
where embedding vectors can interact heavily; (ii) since predict models have
fewer parameters, they are less prone to overfitting.
  Motivated by the insight of our analysis, we show how count models can be
regularized in a principled manner and provide closed-form solutions for L1 and
L2 regularization. Finally, we propose a new embedding model with a convex
objective and the additional benefit of being intelligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02025</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02025</id><created>2015-11-06</created><updated>2016-01-21</updated><authors><author><keyname>Miller</keyname><forenames>Patrick J.</forenames></author><author><keyname>Lubke</keyname><forenames>Gitta H.</forenames></author><author><keyname>McArtor</keyname><forenames>Daniel B.</forenames></author><author><keyname>Bergeman</keyname><forenames>C. S.</forenames></author></authors><title>Finding structure in data using multivariate tree boosting</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technology and collaboration enable dramatic increases in the size of
psychological and psychiatric data collections, but finding structure in these
large data sets with many collected variables is challenging. Decision tree
ensembles like random forests (Strobl, Malley, and Tutz, 2009) are a useful
tool for finding structure, but are difficult to interpret with multiple
outcome variables which are often of interest in psychology. To find and
interpret structure in data sets with multiple outcomes and many predictors
(possibly exceeding the sample size), we introduce a multivariate extension to
a decision tree ensemble method called Gradient Boosted Regression Trees
(Friedman, 2001). Our method, multivariate tree boosting, can be used for
identifying important predictors, detecting predictors with non-linear effects
and interactions without specification of such effects, and for identifying
predictors that cause two or more outcome variables to covary without
parametric assumptions. We provide the R package 'mvtboost' to estimate, tune,
and interpret the resulting model, which extends the implementation of
univariate boosting in the R package 'gbm' (Ridgeway, 2013) to continuous,
multivariate outcomes. To illustrate the approach, we analyze predictors of
psychological well-being (Ryff and Keyes, 1995). Simulations verify that our
approach identifies predictors with non-linear effects and achieves high
prediction accuracy, exceeding or matching the performance of (penalized)
multivariate multiple regression and multivariate decision trees over a wide
range of conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02030</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02030</id><created>2015-11-06</created><authors><author><keyname>Berral</keyname><forenames>Josep Ll.</forenames></author><author><keyname>Poggi</keyname><forenames>Nicolas</forenames></author><author><keyname>Carrera</keyname><forenames>David</forenames></author><author><keyname>Call</keyname><forenames>Aaron</forenames></author><author><keyname>Reinauer</keyname><forenames>Rob</forenames></author><author><keyname>Green</keyname><forenames>Daron</forenames></author></authors><title>ALOJA-ML: A Framework for Automating Characterization and Knowledge
  Discovery in Hadoop Deployments</title><categories>cs.LG cs.DC</categories><comments>Submitted to KDD'2015. Part of the Aloja Project. Partially funded by
  European Research Council (ERC) under the European Union's Horizon 2020
  research and innovation programme (grant agreement No 639595) - HiEST Project</comments><acm-class>C.4; I.2.6</acm-class><journal-ref>Proceedings of the 21th ACM SIGKDD International Conference on
  Knowledge Discovery and Data Mining. Pages 1701-1710. ACM New York, NY, USA.
  2015. ISBN: 978-1-4503-3664-2</journal-ref><doi>10.1145/2783258.2788600</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents ALOJA-Machine Learning (ALOJA-ML) an extension to the
ALOJA project that uses machine learning techniques to interpret Hadoop
benchmark performance data and performance tuning; here we detail the approach,
efficacy of the model and initial results. Hadoop presents a complex execution
environment, where costs and performance depends on a large number of software
(SW) configurations and on multiple hardware (HW) deployment choices. These
results are accompanied by a test bed and tools to deploy and evaluate the
cost-effectiveness of the different hardware configurations, parameter tunings,
and Cloud services. Despite early success within ALOJA from expert-guided
benchmarking, it became clear that a genuinely comprehensive study requires
automation of modeling procedures to allow a systematic analysis of large and
resource-constrained search spaces. ALOJA-ML provides such an automated system
allowing knowledge discovery by modeling Hadoop executions from observed
benchmarks across a broad set of configuration parameters. The resulting
performance models can be used to forecast execution behavior of various
workloads; they allow 'a-priori' prediction of the execution times for new
configurations and HW choices and they offer a route to model-based anomaly
detection. In addition, these models can guide the benchmarking exploration
efficiently, by automatically prioritizing candidate future benchmark tests.
Insights from ALOJA-ML's models can be used to reduce the operational time on
clusters, speed-up the data acquisition and knowledge discovery process, and
importantly, reduce running costs. In addition to learning from the methodology
presented in this work, the community can benefit in general from ALOJA
data-sets, framework, and derived insights to improve the design and deployment
of Big Data applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02037</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02037</id><created>2015-11-06</created><authors><author><keyname>Berral</keyname><forenames>Josep Ll.</forenames></author><author><keyname>Poggi</keyname><forenames>Nicolas</forenames></author><author><keyname>Carrera</keyname><forenames>David</forenames></author><author><keyname>Call</keyname><forenames>Aaron</forenames></author><author><keyname>Reinauer</keyname><forenames>Rob</forenames></author><author><keyname>Green</keyname><forenames>Daron</forenames></author></authors><title>ALOJA: A Framework for Benchmarking and Predictive Analytics in Big Data
  Deployments</title><categories>cs.LG cs.DC</categories><comments>Submitted to IEEE Transactions on Emerging Topics in Computing
  (TETC). Part of the Aloja Project. Partially funded by European Research
  Council (ERC) under the European Union's Horizon 2020 research and innovation
  programme (grant agreement No 639595) - HiEST Project. arXiv admin note:
  substantial text overlap with arXiv:1511.02030</comments><acm-class>C.4; I.2.6</acm-class><doi>10.1109/TETC.2015.2496504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents the ALOJA project and its analytics tools, which
leverages machine learning to interpret Big Data benchmark performance data and
tuning. ALOJA is part of a long-term collaboration between BSC and Microsoft to
automate the characterization of cost-effectiveness on Big Data deployments,
currently focusing on Hadoop. Hadoop presents a complex run-time environment,
where costs and performance depend on a large number of configuration choices.
The ALOJA project has created an open, vendor-neutral repository, featuring
over 40,000 Hadoop job executions and their performance details. The repository
is accompanied by a test-bed and tools to deploy and evaluate the
cost-effectiveness of different hardware configurations, parameters and Cloud
services. Despite early success within ALOJA, a comprehensive study requires
automation of modeling procedures to allow an analysis of large and
resource-constrained search spaces. The predictive analytics extension,
ALOJA-ML, provides an automated system allowing knowledge discovery by modeling
environments from observed executions. The resulting models can forecast
execution behaviors, predicting execution times for new configurations and
hardware choices. That also enables model-based anomaly detection or efficient
benchmark guidance by prioritizing executions. In addition, the community can
benefit from ALOJA data-sets and framework to improve the design and deployment
of Big Data applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02038</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02038</id><created>2015-11-06</created><authors><author><keyname>Renjith</keyname><forenames>P.</forenames></author><author><keyname>Sadagopan</keyname><forenames>N.</forenames></author></authors><title>Hamiltonian Path in 2-Trees</title><categories>cs.DM math.CO</categories><comments>22 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a graph, a spanning path is a path containing all vertices and it is also
known as \emph{Hamiltonian path}. For general graphs, there is no known
necessary and sufficient condition for the existence of Hamiltonian path and
the complexity of finding a Hamiltonian path in general graphs is NP-Complete.
We present a necessary and sufficient condition for the existence of
Hamiltonian path in 2-trees. Using our characterization, we also present a
polynomial-time algorithm for the existence of Hamiltonian path in 2-trees. We
also highlight the fact that 2-trees are well-known subclass of chordal and
planar graphs. This paper makes the first attempt in identifying a non-trivial
subclass of planar graphs where Hamiltonian path is polynomial-time solvable
which is otherwise NP-Complete on planar as well as chordal graphs. Our
characterization is based on a deep understanding of the structure of 2-trees
and we believe that the combinatorics presented here can be used in other
combinatorial problems restricted to 2-trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02043</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02043</id><created>2015-11-06</created><authors><author><keyname>Amaral</keyname><forenames>Marcelo</forenames></author><author><keyname>Polo</keyname><forenames>Jord&#xe0;</forenames></author><author><keyname>Carrera</keyname><forenames>David</forenames></author><author><keyname>Mohomed</keyname><forenames>Iqbal</forenames></author><author><keyname>Unuvar</keyname><forenames>Merve</forenames></author><author><keyname>Steinder</keyname><forenames>Malgorzata</forenames></author></authors><title>Performance Evaluation of Microservices Architectures using Containers</title><categories>cs.DC</categories><comments>Submitted to the 14th IEEE International Symposium on Network
  Computing and Applications (IEEE NCA15). Partially funded by European
  Research Council (ERC) under the European Union's Horizon 2020 research and
  innovation programme (grant agreement No 639595) - HiEST Project</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microservices architecture has started a new trend for application
development for a number of reasons: (1) to reduce complexity by using tiny
services; (2) to scale, remove and deploy parts of the system easily; (3) to
improve flexibility to use different frameworks and tools; (4) to increase the
overall scalability; and (5) to improve the resilience of the system.
Containers have empowered the usage of microservices architectures by being
lightweight, providing fast start-up times, and having a low overhead.
Containers can be used to develop applications based on monolithic
architectures where the whole system runs inside a single container or inside a
microservices architecture where one or few processes run inside the
containers. Two models can be used to implement a microservices architecture
using containers: master-slave, or nested-container. The goal of this work is
to compare the performance of CPU and network running benchmarks in the two
aforementioned models of microservices architecture hence provide a benchmark
analysis guidance for system designers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02058</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02058</id><created>2015-11-06</created><authors><author><keyname>Chen</keyname><forenames>Hung-Hsuan</forenames></author><author><keyname>Ororbia</keyname><forenames>Alexander G.</forenames><suffix>II</suffix></author><author><keyname>Giles</keyname><forenames>C. Lee</forenames></author></authors><title>ExpertSeer: a Keyphrase Based Expert Recommender for Digital Libraries</title><categories>cs.DL cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We describe ExpertSeer, a generic framework for expert recommendation based
on the contents of a digital library. Given a query term q, ExpertSeer
recommends experts of q by retrieving authors who published relevant papers
determined by related keyphrases and the quality of papers. The system is based
on a simple yet effective keyphrase extractor and the Bayes' rule for expert
recommendation. ExpertSeer is domain independent and can be applied to
different disciplines and applications since the system is automated and not
tailored to a specific discipline. Digital library providers can employ the
system to enrich their services and organizations can discover experts of
interest within an organization. To demonstrate the power of ExpertSeer, we
apply the framework to build two expert recommender systems. The first, CSSeer,
utilizes the CiteSeerX digital library to recommend experts primarily in
computer science. The second, ChemSeer, uses publicly available documents from
the Royal Society of Chemistry (RSC) to recommend experts in chemistry. Using
one thousand computer science terms as benchmark queries, we compared the top-n
experts (n=3, 5, 10) returned by CSSeer to two other expert recommenders --
Microsoft Academic Search and ArnetMiner -- and a simulator that imitates the
ranking function of Google Scholar. Although CSSeer, Microsoft Academic Search,
and ArnetMiner mostly return prestigious researchers who published several
papers related to the query term, it was found that different expert
recommenders return moderately different recommendations. To further study
their performance, we obtained a widely used benchmark dataset as the ground
truth for comparison. The results show that our system outperforms Microsoft
Academic Search and ArnetMiner in terms of Precision-at-k (P@k) for k=3, 5, 10.
We also conducted several case studies to validate the usefulness of our
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02071</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02071</id><created>2015-11-06</created><authors><author><keyname>Trummer</keyname><forenames>Immanuel</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>Solving the Join Ordering Problem via Mixed Integer Linear Programming</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We transform join ordering into a mixed integer linear program (MILP). This
allows to address query optimization by mature MILP solver implementations that
have evolved over decades and steadily improved their performance. They offer
features such as anytime optimization and parallel search that are highly
relevant for query optimization.
  We present a MILP formulation for searching left-deep query plans. We use
sets of binary variables to represent join operands and intermediate results,
operator implementation choices or the presence of interesting orders. Linear
constraints restrict value assignments to the ones representing valid query
plans. We approximate the cost of scan and join operations via linear
functions, allowing to increase approximation precision up to arbitrary
degrees. Our experimental results are encouraging: we are able to find optimal
plans for joins between 60 tables; a query size that is beyond the capabilities
of prior exhaustive query optimization methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02074</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02074</id><created>2015-11-06</created><updated>2015-12-28</updated><authors><author><keyname>Avin</keyname><forenames>Chen</forenames></author><author><keyname>Loukas</keyname><forenames>Andreas</forenames></author><author><keyname>Pacut</keyname><forenames>Maciej</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author></authors><title>Online Balanced Repartitioning</title><categories>cs.DS</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper initiates the study of a fundamental online problem called online
balanced repartitioning. Unlike the classic graph partitioning problem, our
input is an arbitrary sequence of communication requests between nodes, with
patterns that may change over time. The objective is to dynamically repartition
the $n$ nodes into $\ell$ clusters, each of size $k$. Every communication
request needs to be served either locally (cost 0), if the communicating nodes
are collocated in the same cluster, or remotely (cost 1), using inter-cluster
communication, if they are located in different clusters. The algorithm can
also dynamically update the partitioning by migrating nodes between clusters at
cost $\alpha$ per node migration. Therefore, we are interested in online
algorithms which find a good trade-off between the communication cost and the
migration cost, maintaining partitions which minimize the number of
inter-cluster communications.
  We consider settings both with and without cluster-size augmentation. For the
former, we prove a lower bound which is strictly larger than $k$, which
highlights an interesting difference to online paging. Somewhat surprisingly,
and unlike online paging, we prove that any deterministic online algorithm has
a non-constant competitive ratio of at least $k$, even with augmentation. Our
main technical contribution is an $O(k \log{k})$-competitive algorithm for the
setting with (constant) augmentation.
  We believe that our model finds interesting applications, e.g., in the
context of datacenters, where virtual machines need to be dynamically embedded
on a set of (multi-core) servers, and where machines migrations are possible,
but costly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02093</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02093</id><created>2015-11-06</created><updated>2015-11-18</updated><authors><author><keyname>Heng</keyname><forenames>Ziling</forenames></author><author><keyname>Yue</keyname><forenames>Qin</forenames></author></authors><title>Evaluation of the weight distribution of a class of linear codes based
  on semi-primitive Gauss sums</title><categories>cs.IT math.IT</categories><comments>21 pages</comments><msc-class>11T71, 11T55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes with a few weights have been widely investigated in recent
years. However, most of these known linear codes were defined over a prime
field. In this paper, we mainly use Gauss sums to represent the Hamming weights
of a class of $q$-ary linear codes, where $q$ is a power of a prime. The lower
bound of the minimum Hamming distance of the codes is obtained. And in some
special cases, we evaluate the weight distributions of the linear codes based
on semi-primitive Gauss sums and obtain some one-weight, two-weight linear
codes. It is quite interesting that we find a few optimal codes achieving some
certain bounds on linear codes. The linear codes in this paper can be used in
secret sharing schemes, authentication codes and data storage systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02113</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02113</id><created>2015-11-06</created><authors><author><keyname>Pratt</keyname><forenames>Pete</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author></authors><title>How does mobility affect the connectivity of interference-limited ad-hoc
  networks?</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One limiting factor to the performance of mobile ad-hoc networks is the
amount of interference that is experienced by each node. In this paper we use
the Random Waypoint Mobility Model (RWPM) to represent such a network of mobile
devices, and show that the connectivity of a receiver at different parts of the
network domain varies significantly. This is a result of a large portion of the
nodes in the RWPM being located near the centre of the domain resulting in
increased levels of interference between neighbouring devices. A non-trivial
trade-off therefore exists between the spatial intensity of interfering signals
and non-interfering (useful) ones. Using tools from stochastic geometry, we
derive closed form expressions for the spatial distribution of nodes in a
rectangle and the connection probability for an interference limited network
indicating the impact an inhomogeneous distribution of nodes has on a network's
performance. Our novel results can be used to analyse this trade-off and
optimize network performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02117</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02117</id><created>2015-11-06</created><authors><author><keyname>Fultz</keyname><forenames>Kerry</forenames></author><author><keyname>Filip</keyname><forenames>Seth</forenames></author></authors><title>Introducing SKYSET - a Quintuple Approach for Improving Instructions</title><categories>cs.CL</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach called SKYSET (Synthetic Knowledge Yield Social Entities
Translation) is proposed to validate completeness and to reduce ambiguity from
written instructional documentation. SKYSET utilizes a quintuple set of
standardized categories, which differs from traditional approaches that
typically use triples. The SKYSET System defines the categories required to
form a standard template for representing information that is portable across
different domains. It provides a standardized framework that enables sentences
from written instructions to be translated into sets of category typed entities
on a table or database. The SKYSET entities contain conceptual units or phrases
that represent information from the original source documentation. SKYSET
enables information concatenation where multiple documents from different
domains can be translated and combined into a single common filterable and
searchable table of entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02119</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02119</id><created>2015-11-06</created><authors><author><keyname>Tamrakar</keyname><forenames>Sandeep</forenames></author><author><keyname>Nguyen</keyname><forenames>Long</forenames></author><author><keyname>Pendyala</keyname><forenames>Praveen Kumar</forenames></author><author><keyname>Paverd</keyname><forenames>Andrew</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Ahmad-Reza</forenames></author></authors><title>OmniShare: Securely Accessing Encrypted Cloud Storage from Multiple
  Authorized Devices</title><categories>cs.CR</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud storage services like Dropbox, Google Drive and OneDrive are becoming
increasingly popular. Two major reasons for the success of cloud storage
services are 1) their ability to synchronize stored data across multiple client
devices and 2) the possibility of sharing a subset of this data with other
people. But privacy of cloud data is a growing concern. Encrypting data on the
client-side before uploading it to cloud storage servers is an effective way to
ensure privacy of data. However, in order to allow users to access their data
from multiple devices, current solutions resort to deriving encryption keys
solely from user-chosen passwords which may have low entropy. We present
OmniShare, the first scheme to allow client-side encryption with high-entropy
keys whilst providing an intuitive key distribution mechanism enabling data
access from multiple client devices. It allows users to authorize their devices
to access encrypted storage and makes use of out-of-band channels for
distributing the relevant keys to authorized devices. OmniShare uses the cloud
storage itself as a communication channel between devices to ensure that user
actions needed during authorization are minimal and consistent. Furthermore,
OmniShare allows the possibility of sharing selected encrypted files with other
people. OmniShare is open source and currently available for Android and
Windows with other other platforms in development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02124</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02124</id><created>2015-11-06</created><updated>2015-11-25</updated><authors><author><keyname>Krishnan</keyname><forenames>Rahul G.</forenames></author><author><keyname>Lacoste-Julien</keyname><forenames>Simon</forenames></author><author><keyname>Sontag</keyname><forenames>David</forenames></author></authors><title>Barrier Frank-Wolfe for Marginal Inference</title><categories>stat.ML cs.LG math.OC</categories><comments>25 pages, 12 figures, To appear in Neural Information Processing
  Systems (NIPS) 2015, Corrected reference and cleaned up bibliography</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a globally-convergent algorithm for optimizing the
tree-reweighted (TRW) variational objective over the marginal polytope. The
algorithm is based on the conditional gradient method (Frank-Wolfe) and moves
pseudomarginals within the marginal polytope through repeated maximum a
posteriori (MAP) calls. This modular structure enables us to leverage black-box
MAP solvers (both exact and approximate) for variational inference, and obtains
more accurate results than tree-reweighted algorithms that optimize over the
local consistency relaxation. Theoretically, we bound the sub-optimality for
the proposed algorithm despite the TRW objective having unbounded gradients at
the boundary of the marginal polytope. Empirically, we demonstrate the
increased quality of results found by tightening the relaxation over the
marginal polytope as well as the spanning tree polytope on synthetic and
real-world instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02126</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02126</id><created>2015-11-06</created><authors><author><keyname>Zhao</keyname><forenames>Shichao</forenames></author><author><keyname>Liu</keyname><forenames>Yanbin</forenames></author><author><keyname>Han</keyname><forenames>Yahong</forenames></author><author><keyname>Hong</keyname><forenames>Richang</forenames></author></authors><title>Pooling the Convolutional Layers in Deep ConvNets for Action Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep ConvNets have shown its good performance in image classification tasks.
However it still remains as a problem in deep video representation for action
recognition. The problem comes from two aspects: on one hand, current video
ConvNets are relatively shallow compared with image ConvNets, which limits its
capability of capturing the complex video action information; on the other
hand, temporal information of videos is not properly utilized to pool and
encode the video sequences. Towards these issues, in this paper, we utilize two
state-of-the-art ConvNets, i.e., the very deep spatial net (VGGNet) and the
temporal net from Two-Stream ConvNets, for action representation. The
convolutional layers and the proposed new layer, called frame-diff layer, are
extracted and pooled with two temporal pooling strategy: Trajectory pooling and
line pooling. The pooled local descriptors are then encoded with VLAD to form
the video representations. In order to verify the effectiveness of the proposed
framework, we conduct experiments on UCF101 and HMDB51 datasets. It achieves
the accuracy of 93.78\% on UCF101 which is the state-of-the-art and the
accuracy of 65.62\% on HMDB51 which is comparable to the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02128</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02128</id><created>2015-11-06</created><updated>2016-01-18</updated><authors><author><keyname>Xiao</keyname><forenames>Zhenyu</forenames></author><author><keyname>He</keyname><forenames>Tong</forenames></author><author><keyname>Xia</keyname><forenames>Pengfei</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author></authors><title>Hierarchical Codebook Design for Beamforming Training in Millimeter-Wave
  Communication</title><categories>cs.IT math.IT</categories><comments>13 pages, 11 figures. To appear in IEEE Trans. Wireless Commn. This
  paper proposes the BMW-SS approach to design a fully-hierarchical codebook
  for mmWave communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In millimeter-wave communication, large antenna arrays are required to
achieve high power gain by steering towards each other with narrow beams, which
poses the problem to efficiently search the best beam direction in the angle
domain at both Tx and Rx sides. As the exhaustive search is time consuming,
hierarchical search has been widely accepted to reduce the complexity, and its
performance is highly dependent on the codebook design. In this paper, we
propose two basic criteria for the hierarchical codebook design, and devise an
efficient hierarchical codebook by jointly exploiting sub-array and
deactivation (turning-off) antenna processing techniques, where closed-form
expressions are provided to generate the codebook. Performance evaluations are
conducted under different system and channel models. Results show superiority
of the proposed codebook over the existing alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02134</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02134</id><created>2015-11-06</created><authors><author><keyname>Gmeiner</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Huber</keyname><forenames>Markus</forenames></author><author><keyname>John</keyname><forenames>Lorenz</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author><author><keyname>Wohlmuth</keyname><forenames>Barbara</forenames></author></authors><title>A quantitative performance analysis for Stokes solvers at the extreme
  scale</title><categories>cs.CE cs.MS cs.NA math.NA</categories><msc-class>65N55, 65Y05, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a systematic quantitative performance analysis for
large finite element computations on extreme scale computing systems. Three
parallel iterative solvers for the Stokes system, discretized by low order
tetrahedral elements, are compared with respect to their numerical efficiency
and their scalability running on up to $786\,432$ parallel threads. A genuine
multigrid method for the saddle point system using an Uzawa-type smoother
provides the best overall performance with respect to memory consumption and
time-to-solution. The largest system solved on a Blue Gene/Q system has more
than ten trillion ($1.1 \cdot 10 ^{13}$) unknowns and requires about 13 minutes
compute time. Despite the matrix free and highly optimized implementation, the
memory requirement for the solution vector and the auxiliary vectors is about
200 TByte. Brandt's notion of &quot;textbook multigrid efficiency&quot; is employed to
study the algorithmic performance of iterative solvers. A recent extension of
this paradigm to &quot;parallel textbook multigrid efficiency&quot; makes it possible to
assess also the efficiency of parallel iterative solvers for a given hardware
architecture in absolute terms. The efficiency of the method is demonstrated
for simulating incompressible fluid flow in a pipe filled with spherical
obstacles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02136</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02136</id><created>2015-11-06</created><updated>2016-01-19</updated><authors><author><keyname>Atwood</keyname><forenames>James</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Search-Convolutional Neural Networks</title><categories>cs.LG</categories><comments>Added a note about the time complexity of prediction for sparse
  graphs to section 2.4.1; added a link to a reference implementation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new deterministic relational model derived from convolutional
neural networks. Search-Convolutional Neural Networks (SCNNs) extend the notion
of convolution to graph search to construct a rich latent representation that
extracts local behavior from general graph-structured data. Unlike other neural
network models that take graph-structured data as input, SCNNs have a
parameterization that is independent of input size, a property that enables
transfer learning between datasets. SCNNs can be applied to a wide variety of
prediction tasks, including node classification, community detection, and link
prediction. Our results indicate that SCNNs can offer considerable lift over
off-the-shelf classifiers and simple multilayer perceptrons, and comparable
performance to state-of-the-art probabilistic graphical models at considerably
lower computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02141</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02141</id><created>2015-11-06</created><updated>2015-11-10</updated><authors><author><keyname>Lohrey</keyname><forenames>Markus</forenames></author><author><keyname>Maneth</keyname><forenames>Sebastian</forenames></author><author><keyname>Reh</keyname><forenames>Carl Philipp</forenames></author></authors><title>Traversing Grammar-Compressed Trees with Constant Delay</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A grammar-compressed ranked tree is represented with a linear space overhead
so that a single traversal step, i.e., the move to the parent or the i-th
child, can be carried out in constant time. Moreover, we extend our data
structure such that equality of subtrees can be checked in constant time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02147</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02147</id><created>2015-11-06</created><updated>2016-01-06</updated><authors><author><keyname>Chen</keyname><forenames>Liang-Ting</forenames></author><author><keyname>Adamek</keyname><forenames>Jiri</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author><author><keyname>Urbat</keyname><forenames>Henning</forenames></author></authors><title>Profinite Monads, Profinite Equations, and Reiterman's Theorem</title><categories>cs.FL math.CT</categories><comments>Accepted for presentation at FoSSaCS'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Profinite equations are an indispensable tool for the algebraic
classification of formal languages. Reiterman's theorem states that they
precisely specify pseudovarieties, i.e. classes of finite algebras closed under
finite products, subalgebras and quotients. In this paper Reiterman's theorem
is generalised to finite Eilenberg-Moore algebras for a monad T on a variety D
of (ordered) algebras: a class of finite T-algebras is a pseudovariety iff it
is presentable by profinite (in-)equations. As an application, quasivarieties
of finite algebras are shown to be presentable by profinite implications. Other
examples include finite ordered algebras, finite categories, finite
infinity-monoids, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02150</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02150</id><created>2015-11-06</created><authors><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Zhang</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Xianbin</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A Split-Reduced Successive Cancellation List Decoder for Polar Codes</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Journal on Selected Areas in
  Communications - Special Issue on Recent Advances In Capacity Approaching
  Codes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on low complexity successive cancellation list (SCL)
decoding of polar codes. In particular, using the fact that splitting may be
unnecessary when the reliability of decoding the unfrozen bit is sufficiently
high, a novel splitting rule is proposed. Based on this rule, it is conjectured
that, if the correct path survives at some stage, it tends to survive till
termination without splitting with high probability. On the other hand, the
incorrect paths are more likely to split at the following stages. Motivated by
these observations, a simple counter that counts the successive number of
stages without splitting is introduced for each decoding path to facilitate the
identification of correct and incorrect path. Specifically, any path with
counter value larger than a predefined threshold \omega is deemed to be the
correct path, which will survive at the decoding stage, while other paths with
counter value smaller than the threshold will be pruned, thereby reducing the
decoding complexity. Furthermore, it is proved that there exists a unique
unfrozen bit u_{N-K_1+1}, after which the successive cancellation decoder
achieves the same error performance as the maximum likelihood decoder if all
the prior unfrozen bits are correctly decoded, which enables further complexity
reduction. Simulation results demonstrate that the proposed low complexity SCL
decoder attains performance similar to that of the conventional SCL decoder,
while achieving substantial complexity reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02152</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02152</id><created>2015-11-06</created><authors><author><keyname>Xiao</keyname><forenames>Zhenyu</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Jin</keyname><forenames>Depeng</forenames></author><author><keyname>Ge</keyname><forenames>Ning</forenames></author></authors><title>Iterative Eigenvalue Decomposition and Multipath-Grouping Tx/Rx Joint
  Beamforming for Millimeter-Wave Communication</title><categories>cs.IT math.IT</categories><comments>13 pages, 12 figures</comments><journal-ref>IEEE Transactions on Wireless Communications, vol. 14, no. 3, pp.
  1595-1607, March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate Tx/Rx joint beamforming in millimeter-wave communications
(MMWC). As the multipath components (MPCs) have different steering angles and
independent fadings, beamforming aims at achieving array gain as well as
diversity gain in this scenario. A sub-optimal beamforming scheme is proposed
to find the antenna weight vectors (AWVs) at Tx/Rx via iterative eigenvalue
decomposition (EVD), provided that full channel state information (CSI) is
available at both the transmitter and receiver. To make this scheme practically
feasible in MMWC, a corresponding training approach is suggested to avoid the
channel estimation and iterative EVD computation. As in fast fading scenario
the training approach may be time-consuming due to frequent training, another
beamforming scheme, which exploits the quasi-static steering angles in MMWC, is
proposed to reduce the overhead and increase the system reliability by
multipath grouping (MPG). The scheme first groups the MPCs and then
concurrently beamforms towards multiple steering angles of the grouped MPCs, so
that both array gain and diversity gain are achieved. Performance comparisons
show that, compared with the corresponding state-of-the-art schemes, the
iterative EVD scheme with the training approach achieves the same performance
with a reduced overhead and complexity, while the MPG scheme achieves better
performance with an approximately equivalent complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02163</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02163</id><created>2015-11-06</created><authors><author><keyname>Gillenwater</keyname><forenames>Jennifer</forenames></author><author><keyname>Iyer</keyname><forenames>Rishabh</forenames></author><author><keyname>Lusch</keyname><forenames>Bethany</forenames></author><author><keyname>Kidambi</keyname><forenames>Rahul</forenames></author><author><keyname>Bilmes</keyname><forenames>Jeff</forenames></author></authors><title>Submodular Hamming Metrics</title><categories>cs.DS cs.AI cs.DM</categories><comments>15 pages, 1 figure, a short version of this will appear in the NIPS
  2015 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there is a largely unexplored class of functions (positive
polymatroids) that can define proper discrete metrics over pairs of binary
vectors and that are fairly tractable to optimize over. By exploiting
submodularity, we are able to give hardness results and approximation
algorithms for optimizing over such metrics. Additionally, we demonstrate
empirically the effectiveness of these metrics and associated algorithms on
both a metric minimization task (a form of clustering) and also a metric
maximization task (generating diverse k-best lists).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02166</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02166</id><created>2015-11-06</created><authors><author><keyname>Einkemmer</keyname><forenames>Lukas</forenames></author></authors><title>Evaluation of the Intel Xeon Phi and NVIDIA K80 as accelerators for
  two-dimensional panel codes</title><categories>cs.DC cs.MS physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To predict the properties of fluid flow over a solid geometry is an important
engineering problem. In many applications so-called panel methods (or boundary
element methods) have become the standard approach to solve the corresponding
partial differential equation. Since panel methods in two dimensions are
computationally cheap, they are well suited as the inner solver in an
optimization algorithm.
  In this paper we evaluate the performance of the Intel Xeon Phi 7120 and the
NVIDIA K80 to accelerate such an optimization algorithm. For that purpose, we
have implemented an optimized version of the algorithm on the CPU and Xeon Phi
(based on OpenMP, vectorization, and the Intel MKL library) and on the GPU
(based on CUDA and the MAGMA library). We present timing results for all codes
and discuss the similarities and differences between the three implementations.
Overall, we observe a speedup of approximately $2.5$ for adding a Intel Xeon
Phi 7120 to a dual socket workstation and a speedup between $3$ and $3.5$ for
adding a NVIDIA K80 to a dual socket workstation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02171</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02171</id><created>2015-11-06</created><authors><author><keyname>Catal&#xe1;n</keyname><forenames>Sandra</forenames></author><author><keyname>Herrero</keyname><forenames>Jos&#xe9; R.</forenames></author><author><keyname>Igual</keyname><forenames>Francisco D.</forenames></author><author><keyname>Rodr&#xed;guez-S&#xe1;nchez</keyname><forenames>Rafael</forenames></author><author><keyname>Quintana-Ort&#xed;</keyname><forenames>Enrique S.</forenames></author></authors><title>Multi-Threaded Dense Linear Algebra Libraries for Low-Power Asymmetric
  Multicore Processors</title><categories>cs.MS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dense linear algebra libraries, such as BLAS and LAPACK, provide a relevant
collection of numerical tools for many scientific and engineering applications.
While there exist high performance implementations of the BLAS (and LAPACK)
functionality for many current multi-threaded architectures,the adaption of
these libraries for asymmetric multicore processors (AMPs)is still pending. In
this paper we address this challenge by developing an asymmetry-aware
implementation of the BLAS, based on the BLIS framework, and tailored for AMPs
equipped with two types of cores: fast/power hungry versus slow/energy
efficient. For this purpose, we integrate coarse-grain and fine-grain
parallelization strategies into the library routines which, respectively,
dynamically distribute the workload between the two core types and statically
repartition this work among the cores of the same type.
  Our results on an ARM big.LITTLE processor embedded in the Exynos 5422 SoC,
using the asymmetry-aware version of the BLAS and a plain migration of the
legacy version of LAPACK, experimentally assess the benefits, limitations, and
potential of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02175</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02175</id><created>2015-11-06</created><authors><author><keyname>Arratia</keyname><forenames>Argimiro</forenames></author><author><keyname>Ortiz</keyname><forenames>Carlos E.</forenames></author></authors><title>Methods of Class Field Theory to Separate Logics over Finite Residue
  Classes and Circuit Complexity</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separations among the first order logic ${\cal R}ing(0,+,*)$ of finite
residue class rings, its extensions with generalized quantifiers, and in the
presence of a built-in order are shown, using algebraic methods from class
field theory. These methods include classification of spectra of sentences over
finite residue classes as systems of congruences, and the study of their
$h$-densities over the set of all prime numbers, for various functions $h$ on
the natural numbers. Over ordered structures the logic of finite residue class
rings and extensions are known to capture DLOGTIME-uniform circuit complexity
classes ranging from $AC^0$ to $TC^0$. Separating these circuit complexity
classes is directly related to classifying the $h$-density of spectra of
sentences in the corresponding logics of finite residue classes. We further
give general conditions under which a logic over the finite residue class rings
has a sentence whose spectrum has no $h$-density. One application of this
result is that in ${\cal R}ing(0,+,*,&lt;) + M$, the logic of finite residue class
rings with built-in order and extended with the majority quantifier $M$, there
are sentences whose spectrum have no exponential density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02176</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02176</id><created>2015-11-06</created><authors><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author><author><keyname>Pal</keyname><forenames>David</forenames></author></authors><title>Optimal Non-Asymptotic Lower Bound on the Minimax Regret of Learning
  with Expert Advice</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove non-asymptotic lower bounds on the expectation of the maximum of $d$
independent Gaussian variables and the expectation of the maximum of $d$
independent symmetric random walks. Both lower bounds recover the optimal
leading constant in the limit. A simple application of the lower bound for
random walks is an (asymptotically optimal) non-asymptotic lower bound on the
minimax regret of online learning with expert advice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02186</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02186</id><created>2015-11-06</created><updated>2015-12-10</updated><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Xu</keyname><forenames>Liangliang</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author></authors><title>Accelerating Adaptive IDW Interpolation Algorithm on a Single GPU</title><categories>cs.DC</categories><comments>Preprint version for submitting to a journal. 15 pages, 7 figures, 1
  table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the design and implementing of GPU-accelerated Adaptive
Inverse Distance Weighting (AIDW) interpolation algorithm. The AIDW is an
improved version of the standard IDW, which can adaptively determine the power
parameter according to the spatial points distribution pattern and achieve more
accurate predictions than those by IDW. In this paper, we first present two
versions of the GPU accelerated AIDW, the naive version without profiting from
shared memory and the tiled version taking advantage of shared memory. We also
implement the naive version and the tiled version using the data layouts,
Structure of Arrays (AoS) and Array of aligned Structures (AoaS), on single and
double precision. We then evaluate the performance of the GPU-accelerated AIDW
by comparing it with its original CPU version. Experimental results show that:
on single precision the naive version and the tiled version can achieve the
speedups of approximately 270 and 400, respectively. In addition, on single
precision the implementations using the layout SoA are always slightly faster
than those using layout AoaS. However, on double precision, the speedup is only
about 8; and we have also observed that: (1) there are no performance gains
obtained from the tiled version against the naive version; and (2) the use of
SoA and AoaS does not lead to significant differences in computational
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02196</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02196</id><created>2015-11-06</created><authors><author><keyname>Wang</keyname><forenames>Haohan</forenames></author><author><keyname>Ganapathiraju</keyname><forenames>Madhavi K.</forenames></author></authors><title>Evaluating Protein-protein Interaction Predictors with a Novel
  3-Dimensional Metric</title><categories>cs.LG</categories><comments>This article is an extended version of a poster presented in AMIA TBI
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order for the predicted interactions to be directly adopted by biologists,
the ma- chine learning predictions have to be of high precision, regardless of
recall. This aspect cannot be evaluated or numerically represented well by
traditional metrics like accuracy, ROC, or precision-recall curve. In this
work, we start from the alignment in sensitivity of ROC and recall of
precision-recall curve, and propose an evaluation metric focusing on the
ability of a model to be adopted by biologists. This metric evaluates the
ability of a machine learning algorithm to predict only new interactions,
meanwhile, it eliminates the influence of test dataset. In the experiment of
evaluating different classifiers with a same data set and evaluating the same
predictor with different datasets, our new metric fulfills the evaluation task
of our interest while two widely recognized metrics, ROC and precision-recall
curve fail the tasks for different reasons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02201</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02201</id><created>2015-11-06</created><authors><author><keyname>Contreras-Oca&#xf1;a</keyname><forenames>Jesus E.</forenames></author><author><keyname>Ortega-Vazquez</keyname><forenames>Miguel A.</forenames></author><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author></authors><title>Cooperation and Competition among Energy Storages</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study competition and cooperation among a group of storage units. We show
that as the number of storages increases, the profit of storages approaches
zero under Nash competition. We propose two ways in which storages can achieve
non-zero profit and show that they are optimal in the sense that storages
achieve the maximum possible profit. The first is a decentralized approach in
which storages are exposed to artificial cost functions that incentivize them
to behavior as a coalition. No private information needs to be exchanged
between the storages to calculate the artificial function. The second is a
centralized approach in which an aggregator coordinates and splits profits with
storages in order to achieve maximum profit. We use Nash's axiomatic bargaining
problem to model and predict the profit split between aggregator and storages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02210</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02210</id><created>2015-11-06</created><authors><author><keyname>Wang</keyname><forenames>Tong</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Learning Optimized Or's of And's</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Or's of And's (OA) models are comprised of a small number of disjunctions of
conjunctions, also called disjunctive normal form. An example of an OA model is
as follows: If ($x_1 = $ `blue' AND $x_2=$ `middle') OR ($x_1 = $ `yellow'),
then predict $Y=1$, else predict $Y=0$. Or's of And's models have the advantage
of being interpretable to human experts, since they are a set of conditions
that concisely capture the characteristics of a specific subset of data. We
present two optimization-based machine learning frameworks for constructing OA
models, Optimized OA (OOA) and its faster version, Optimized OA with
Approximations (OOAx). We prove theoretical bounds on the properties of
patterns in an OA model. We build OA models as a diagnostic screening tool for
obstructive sleep apnea, that achieves high accuracy with a substantial gain in
interpretability over other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02222</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02222</id><created>2015-11-06</created><authors><author><keyname>Wilson</keyname><forenames>Andrew Gordon</forenames></author><author><keyname>Hu</keyname><forenames>Zhiting</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Deep Kernel Learning</title><categories>cs.LG cs.AI stat.ME stat.ML</categories><comments>19 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce scalable deep kernels, which combine the structural properties
of deep learning architectures with the non-parametric flexibility of kernel
methods. Specifically, we transform the inputs of a spectral mixture base
kernel with a deep architecture, using local kernel interpolation, inducing
points, and structure exploiting (Kronecker and Toeplitz) algebra for a
scalable kernel representation. These closed-form kernels can be used as
drop-in replacements for standard kernels, with benefits in expressive power
and scalability. We jointly learn the properties of these kernels through the
marginal likelihood of a Gaussian process. Inference and learning cost $O(n)$
for $n$ training points, and predictions cost $O(1)$ per test point. On a large
and diverse collection of applications, including a dataset with 2 million
examples, we show improved performance over scalable Gaussian processes with
flexible kernel learning models, and stand-alone deep architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02228</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02228</id><created>2015-11-06</created><authors><author><keyname>Timofte</keyname><forenames>Radu</forenames></author><author><keyname>Rothe</keyname><forenames>Rasmus</forenames></author><author><keyname>Van Gool</keyname><forenames>Luc</forenames></author></authors><title>Seven ways to improve example-based single image super resolution</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present seven techniques that everybody should know to
improve example-based single image super resolution (SR): 1) augmentation of
data, 2) use of large dictionaries with efficient search structures, 3)
cascading, 4) image self-similarities, 5) back projection refinement, 6)
enhanced prediction by consistency check, and 7) context reasoning. We validate
our seven techniques on standard SR benchmarks (i.e. Set5, Set14, B100) and
methods (i.e. A+, SRCNN, ANR, Zeyde, Yang) and achieve substantial
improvements.The techniques are widely applicable and require no changes or
only minor adjustments of the SR methods. Moreover, our Improved A+ (IA) method
sets new state-of-the-art results outperforming A+ by up to 0.9dB on average
PSNR whilst maintaining a low time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02235</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02235</id><created>2015-11-06</created><authors><author><keyname>Jeffery</keyname><forenames>Stacey</forenames></author><author><keyname>Kimmel</keyname><forenames>Shelby</forenames></author></authors><title>NAND-Trees, Average Choice Complexity, and Effective Resistance</title><categories>quant-ph cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the quantum query complexity of evaluating NAND-tree instances
with average choice complexity at most $W$ is $O(W)$, where average choice
complexity is a measure of the difficulty of winning the associated two-player
game. This generalizes a superpolynomial speedup over classical query
complexity due to Zhan et al. [Zhan et al., ITCS 2012, 249-265]. We further
show that the player with a winning strategy for the two-player game associated
with the NAND-tree can win the game with an expected
$\widetilde{O}(N^{1/4}\sqrt{{\cal C}(x)})$ quantum queries against a random
opponent, where ${\cal C }(x)$ is the average choice complexity of the
instance. This gives an improvement over the query complexity of the naive
strategy, which costs $\widetilde{O}(\sqrt{N})$ queries.
  The results rely on a connection between NAND-tree evaluation and
$st$-connectivity problems on certain graphs, and span programs for
$st$-connectivity problems. Our results follow from relating average choice
complexity to the effective resistance of these graphs, which itself
corresponds to the span program witness size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02240</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02240</id><created>2015-11-06</created><authors><author><keyname>Natvig</keyname><forenames>Lasse</forenames></author><author><keyname>Follan</keyname><forenames>Torbj&#xf8;rn</forenames></author><author><keyname>St&#xf8;a</keyname><forenames>Simen</forenames></author><author><keyname>Magnussen</keyname><forenames>Sindre</forenames></author><author><keyname>Guirado</keyname><forenames>Antonio Garcia</forenames></author></authors><title>Climbing Mont Blanc - A Training Site for Energy Efficient Programming
  on Heterogeneous Multicore Processors</title><categories>cs.DC</categories><comments>4 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Climbing Mont Blanc (CMB) is an open online judge used for training in energy
efficient programming of state-of-the-art heterogeneous multicores. It uses an
Odroid-XU3 board from Hardkernel with an Exynos Octa processor and integrated
power sensors. This processor is three-way heterogeneous containing 14
different cores of three different types. The board currently accepts C and C++
programs, with support for OpenCL v1.1, OpenMP 4.0 and Pthreads. Programs
submitted using the graphical user interface are evaluated with respect to
time, energy used, and energy-efficiency (EDP). A small and varied set of
problems are available, and the system is currently in use in a medium sized
course on parallel computing at NTNU. Other online programming judges exist,
but we are not aware of any similar system that also reports energy-efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02251</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02251</id><created>2015-11-06</created><authors><author><keyname>Joulin</keyname><forenames>Armand</forenames></author><author><keyname>van der Maaten</keyname><forenames>Laurens</forenames></author><author><keyname>Jabri</keyname><forenames>Allan</forenames></author><author><keyname>Vasilache</keyname><forenames>Nicolas</forenames></author></authors><title>Learning Visual Features from Large Weakly Supervised Data</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional networks trained on large supervised dataset produce visual
features which form the basis for the state-of-the-art in many computer-vision
problems. Further improvements of these visual features will likely require
even larger manually labeled data sets, which severely limits the pace at which
progress can be made. In this paper, we explore the potential of leveraging
massive, weakly-labeled image collections for learning good visual features. We
train convolutional networks on a dataset of 100 million Flickr photos and
captions, and show that these networks produce features that perform well in a
range of vision problems. We also show that the networks appropriately capture
word similarity, and learn correspondences between different languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02254</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02254</id><created>2015-11-06</created><authors><author><keyname>Heim</keyname><forenames>Eric</forenames><affiliation>University of Pittsburgh</affiliation></author><author><keyname>Berger</keyname><forenames>Matthew</forenames><affiliation>Air Force Research Laboratory, Information Directorate</affiliation></author><author><keyname>Seversky</keyname><forenames>Lee</forenames><affiliation>Air Force Research Laboratory, Information Directorate</affiliation></author><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames><affiliation>University of Pittsburgh</affiliation></author></authors><title>Active Perceptual Similarity Modeling with Auxiliary Information</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning a model of perceptual similarity from a collection of objects is a
fundamental task in machine learning underlying numerous applications. A common
way to learn such a model is from relative comparisons in the form of triplets:
responses to queries of the form &quot;Is object a more similar to b than it is to
c?&quot;. If no consideration is made in the determination of which queries to ask,
existing similarity learning methods can require a prohibitively large number
of responses. In this work, we consider the problem of actively learning from
triplets -finding which queries are most useful for learning. Different from
previous active triplet learning approaches, we incorporate auxiliary
information into our similarity model and introduce an active learning scheme
to find queries that are informative for quickly learning both the relevant
aspects of auxiliary data and the directly-learned similarity components.
Compared to prior approaches, we show that we can learn just as effectively
with much fewer queries. For evaluation, we introduce a new dataset of
exhaustive triplet comparisons obtained from humans and demonstrate improved
performance for different types of auxiliary information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02256</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02256</id><created>2015-11-06</created><authors><author><keyname>Wan</keyname><forenames>Kai</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author></authors><title>On the Optimality of Uncoded Cache Placement</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, Submitted to ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching is an efficient way to reduce peak-hour network traffic congestion by
storing some contents at user's local cache without knowledge of later demands.
Maddah-Ali and Niesen initiated a fundamental study of caching systems; they
proposed a scheme (with uncoded cache placement and linear network coding
delivery) that is provably optimal to within a factor 12. In this paper, by
noticing that when the cache contents and the demands are fixed, the caching
problem can be seen as an index coding problem, we show the optimality of
Maddah-Ali and Niesen's scheme assuming that cache placement is restricted to
be uncoded and the number of users is not less than the number of files.
Furthermore, this result states that further improvement to the Maddah-Ali and
Niesen's scheme in this regimes can be obtained only by coded cache placement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02258</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02258</id><created>2015-11-06</created><updated>2016-03-06</updated><authors><author><keyname>Zhang</keyname><forenames>Z.</forenames></author><author><keyname>Duraisamy</keyname><forenames>K.</forenames></author><author><keyname>Gumerov</keyname><forenames>N. A.</forenames></author></authors><title>Efficient Multiscale Gaussian Process Regression using Hierarchical
  Clustering</title><categories>cs.LG stat.ML</categories><comments>22 pages, 9 figures. Preprint. Submitted to Machine Learning Mar.
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard Gaussian Process (GP) regression, a powerful machine learning tool,
is computationally expensive when it is applied to large datasets, and
potentially inaccurate when data points are sparsely distributed in a
high-dimensional feature space. To address these challenges, a new multiscale,
sparsified GP algorithm is formulated, with the goal of application to large
scientific computing datasets. In this approach, the data is partitioned into
clusters and the cluster centers are used to define a reduced training set,
resulting in an improvement over standard GPs in terms of training and
evaluation costs. Further, a hierarchical technique is used to adaptively map
the local covariance representation to the underlying sparsity of the feature
space, leading to improved prediction accuracy when the data distribution is
highly non-uniform. A theoretical investigation of the computational complexity
of the algorithm is presented. The efficacy of this method is then demonstrated
on smooth and discontinuous analytical functions and on data from a direct
numerical simulation of turbulent combustion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02259</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02259</id><created>2015-11-06</created><authors><author><keyname>Mehmetoglu</keyname><forenames>Mustafa Said</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>Deterministic Annealing Based Optimization for Zero-Delay Source-Channel
  Coding in Networks</title><categories>cs.IT math.IT</categories><comments>Accepted for publication at IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of global optimization of zero-delay
source-channel codes that map between the source space and the channel space,
under a given transmission power constraint and for the mean square error
distortion. Particularly, we focus on two well known network settings: the
Wyner-Ziv setting where only a decoder has access to side information and the
distributed setting where independent encoders transmit over independent
channels to a central decoder. Prior work derived the necessary conditions for
optimality of the encoder and decoder mappings, along with a greedy
optimization algorithm that imposes these conditions iteratively, in
conjunction with the heuristic noisy channel relaxation method to mitigate poor
local minima. While noisy channel relaxation is arguably effective in simple
settings, it fails to provide accurate global optimization in more complicated
settings considered in this paper. We propose a powerful non-convex
optimization method based on the concept of deterministic annealing -- which is
derived from information theoretic principles and was successfully employed in
several problems including vector quantization, classification and regression.
We present comparative numerical results that show strict superiority of the
proposed method over greedy optimization methods as well as prior approaches in
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02264</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02264</id><created>2015-11-06</created><authors><author><keyname>Xiang</keyname><forenames>Can</forenames></author><author><keyname>Feng</keyname><forenames>Keqin</forenames></author><author><keyname>Tang</keyname><forenames>Chunming</forenames></author></authors><title>A Construction of Linear Codes over $\f_{2^t}$ from Boolean Functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a construction of linear codes over $\f_{2^t}$ from
Boolean functions, which is a generalization of Ding's method \cite[Theorem
9]{Ding15}. Based on this construction, we give two classes of linear codes
$\tilde{\C}_{f}$ and $\C_f$ (see Theorem \ref{thm-maincode1} and Theorem
\ref{thm-maincodenew}) over $\f_{2^t}$ from a Boolean function
$f:\f_{q}\rightarrow \f_2$, where $q=2^n$ and $\f_{2^t}$ is some subfield of
$\f_{q}$. The complete weight enumerator of $\tilde{\C}_{f}$ can be easily
determined from the Walsh spectrum of $f$, while the weight distribution of the
code $\C_f$ can also be easily settled. Particularly, the number of nonzero
weights of $\tilde{\C}_{f}$ and $\C_f$ is the same as the number of distinct
Walsh values of $f$. As applications of this construction, we show several
series of linear codes over $\f_{2^t}$ with two or three weights by using bent,
semibent, monomial and quadratic Boolean function $f$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02273</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02273</id><created>2015-11-06</created><authors><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>Gravel</keyname><forenames>Claude</forenames></author></authors><title>The expected bit complexity of the von Neumann rejection algorithm</title><categories>cs.IT math.IT</categories><comments>22 pages, 4 figures</comments><msc-class>65C10, 68Q25, 68Q30, 68Q87, 68W20, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1952, von Neumann introduced the rejection method for random variate
generation. We revisit this algorithm when we have a source of perfect bits at
our disposal. In this random bit model, there are universal lower bounds for
generating a random variate with a given density to within an accuracy
$\epsilon$ derived by Knuth and Yao, and refined by the authors. In general,
von Neumann's method fails in this model. We propose a modification that
insures proper behavior for all Riemann integrable densities on compact sets,
and show that the expected number of random bits needed behaves optimally with
respect to its dependence on $\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02274</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02274</id><created>2015-11-06</created><updated>2016-01-26</updated><authors><author><keyname>Yang</keyname><forenames>Zichao</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author></authors><title>Stacked Attention Networks for Image Question Answering</title><categories>cs.LG cs.CL cs.CV cs.NE</categories><comments>test-dev/standard results added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents stacked attention networks (SANs) that learn to answer
natural language questions from images. SANs use semantic representation of a
question as query to search for the regions in an image that are related to the
answer. We argue that image question answering (QA) often requires multiple
steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an
image multiple times to infer the answer progressively. Experiments conducted
on four image QA data sets demonstrate that the proposed SANs significantly
outperform previous state-of-the-art approaches. The visualization of the
attention layers illustrates the progress that the SAN locates the relevant
visual clues that lead to the answer of the question layer-by-layer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02279</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02279</id><created>2015-11-06</created><updated>2016-02-29</updated><authors><author><keyname>Akkaya</keyname><forenames>Ilge</forenames></author><author><keyname>Fremont</keyname><forenames>Daniel J.</forenames></author><author><keyname>Valle</keyname><forenames>Rafael</forenames></author><author><keyname>Donz&#xe9;</keyname><forenames>Alexandre</forenames></author><author><keyname>Lee</keyname><forenames>Edward A.</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author></authors><title>Control Improvisation with Probabilistic Temporal Specifications</title><categories>cs.SY</categories><comments>to appear in Proceedings of the 1st IEEE Conference on
  Internet-of-Things Design and Implementation (IoTDI'16)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of generating randomized control sequences for
complex networked systems typically actuated by human agents. Our approach
leverages a concept known as control improvisation, which is based on a
combination of data-driven learning and controller synthesis from formal
specifications. We learn from existing data a generative model (for instance,
an explicit-duration hidden Markov model, or EDHMM) and then supervise this
model in order to guarantee that the generated sequences satisfy some desirable
specifications given in Probabilistic Computation Tree Logic (PCTL). We present
an implementation of our approach and apply it to the problem of mimicking the
use of lighting appliances in a residential unit, with potential applications
to home security and resource management. We present experimental results
showing that our approach produces realistic control sequences, similar to
recorded data based on human actuation, while satisfying suitable formal
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02281</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02281</id><created>2015-11-06</created><authors><author><keyname>Riek</keyname><forenames>Laurel D.</forenames></author></authors><title>Robotics Technology in Mental Health Care</title><categories>cs.RO cs.CY cs.HC</categories><acm-class>I.2.9; J.3</acm-class><journal-ref>Artificial Intelligence in Behavioral Health and Mental Health
  Care (2015) 185-203;</journal-ref><doi>10.1016/B978-0-12-420248-1.00008-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter discusses the existing and future use of robotics and
intelligent sensing technology in mental health care. While the use of this
technology is nascent in mental health care, it represents a potentially useful
tool in the practitioner's toolbox. The goal of this chapter is to provide a
brief overview of the field, discuss the recent use of robotics technology in
mental health care practice, explore some of the design issues and ethical
issues of using robots in this space, and finally to explore the potential of
emerging technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02282</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02282</id><created>2015-11-06</created><authors><author><keyname>Liu</keyname><forenames>Xiaorui</forenames></author><author><keyname>Huang</keyname><forenames>Yichao</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author></authors><title>Fingertip in the Eye: A cascaded CNN pipeline for the real-time
  fingertip detection in egocentric videos</title><categories>cs.CV</categories><comments>5 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new pipeline for hand localization and fingertip detection.
For RGB images captured from an egocentric vision mobile camera, hand and
fingertip detection remains a challenging problem due to factors like
background complexity and hand shape variety. To address these issues
accurately and robustly, we build a large scale dataset named Ego-Fingertip and
propose a bi-level cascaded pipeline of convolutional neural networks, namely,
Attention-based Hand Detector as well as Multi-point Fingertip Detector. The
proposed method significantly tackles challenges and achieves satisfactorily
accurate prediction and real-time performance compared to previous hand and
fingertip detection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02283</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02283</id><created>2015-11-06</created><updated>2015-11-29</updated><authors><author><keyname>Mao</keyname><forenames>Junhua</forenames></author><author><keyname>Huang</keyname><forenames>Jonathan</forenames></author><author><keyname>Toshev</keyname><forenames>Alexander</forenames></author><author><keyname>Camburu</keyname><forenames>Oana</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author></authors><title>Generation and Comprehension of Unambiguous Object Descriptions</title><categories>cs.CV cs.CL cs.LG cs.RO</categories><comments>We have released the Google Refexp dataset together with a toolbox
  for visualization and evaluation, see
  https://github.com/mjhucla/Google_Refexp_toolbox</comments><acm-class>I.2.6; I.2.7; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method that can generate an unambiguous description (known as a
referring expression) of a specific object or region in an image, and which can
also comprehend or interpret such an expression to infer which object is being
described. We show that our method outperforms previous methods that generate
descriptions of objects without taking into account other potentially ambiguous
objects in the scene. Our model is inspired by recent successes of deep
learning methods for image captioning, but while image captioning is difficult
to evaluate, our task allows for easy objective evaluation. We also present a
new large-scale dataset for referring expressions, based on MS-COCO. We have
released the dataset and a toolbox for visualization and evaluation, see
https://github.com/mjhucla/Google_Refexp_toolbox
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02285</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02285</id><created>2015-11-06</created><authors><author><keyname>Du</keyname><forenames>Xu</forenames></author><author><keyname>Tadrous</keyname><forenames>John</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Multiuser MIMO Sequential Beamforming with Full-duplex Training</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE transactions on wireless communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple transmitting antennas can considerably increase downlink spectral
efficiency by beamforming to multiple users at the same time. However,
multiuser beamforming requires channel state information (CSI) at the
transmitter, which essentially leads to training overhead. In this paper, we
propose and analyze a sequential beamforming strategy that utilizes full-duplex
base station to implement downlink data transmission concurrently with CSI
acquisition via in-band closed and open loop training. Our results demonstrate
that full-duplex capability at a base station can be used to increase spectral
efficiency significantly for multiuser downlink transmission. In moderate SNR
regimes, we analytically derive tight approximations for the optimal training
duration for closed and open loop training, and characterize the associated
respective spectral efficiency. We further characterize the enhanced
multiplexing gain performance in high SNR regime. In both regimes, the
performance of our proposed full-duplex strategy is compared to half-duplex
counterpart to quantify spectral efficiency improvement. With experimental
data, in a typical 1.4 MHz LTE band, where the block length is around 500
symbols, the proposed sequential beamforming strategy attains a spectral
efficiency improvement of 130% and 12% for 8 x 8 systems with closed and open
loop training, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02290</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02290</id><created>2015-11-06</created><authors><author><keyname>Sundermann</keyname><forenames>Camila V.</forenames></author><author><keyname>Domingues</keyname><forenames>Marcos A.</forenames></author><author><keyname>Marcacini</keyname><forenames>Ricardo M.</forenames></author><author><keyname>Rezende</keyname><forenames>Solange O.</forenames></author></authors><title>Combining Privileged Information to Improve Context-Aware Recommender
  Systems</title><categories>cs.IR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recommender system is an information filtering technology which can be used
to predict preference ratings of items (products, services, movies, etc) and/or
to output a ranking of items that are likely to be of interest to the user.
Context-aware recommender systems (CARS) learn and predict the tastes and
preferences of users by incorporating available contextual information in the
recommendation process. One of the major challenges in context-aware
recommender systems research is the lack of automatic methods to obtain
contextual information for these systems. Considering this scenario, in this
paper, we propose to use contextual information from topic hierarchies of the
items (web pages) to improve the performance of context-aware recommender
systems. The topic hierarchies are constructed by an extension of the
LUPI-based Incremental Hierarchical Clustering method that considers three
types of information: traditional bag-of-words (technical information), and the
combination of named entities (privileged information I) with domain terms
(privileged information II). We evaluated the contextual information in four
context-aware recommender systems. Different weights were assigned to each type
of information. The empirical results demonstrated that topic hierarchies with
the combination of the two kinds of privileged information can provide better
recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02296</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02296</id><created>2015-11-06</created><authors><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author><author><keyname>Psomas</keyname><forenames>Christos-Alexandros</forenames></author></authors><title>The Sample Complexity of Auctions with Side Information</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, the Bayesian optimal auction design problem has been
considered either when the bidder values are i.i.d, or when each bidder is
individually identifiable via her value distribution. The latter is a
reasonable approach when the bidders can be classified into a few categories,
but there are many instances where the classification of bidders is a
continuum. For example, the classification of the bidders may be based on their
annual income, their propensity to buy an item based on past behavior, or in
the case of ad auctions, the click through rate of their ads. We introduce an
alternate model that captures this aspect, where bidders are a priori
identical, but can be distinguished based (only) on some side information the
auctioneer obtains at the time of the auction. We extend the sample complexity
approach of Dhangwatnotai et al. and Cole and Roughgarden to this model and
obtain almost matching upper and lower bounds. As an aside, we obtain a revenue
monotonicity lemma which may be of independent interest. We also show how to
use Empirical Risk Minimization techniques to improve the sample complexity
bound of Cole and Roughgarden for the non-identical but independent value
distribution case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02299</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02299</id><created>2015-11-06</created><authors><author><keyname>Zhang</keyname><forenames>Bo</forenames></author><author><keyname>Wu</keyname><forenames>Yunlong</forenames></author><author><keyname>Yi</keyname><forenames>Xiaodong</forenames></author><author><keyname>Yang</keyname><forenames>Xuejun</forenames></author></authors><title>Joint Communication-Motion Planning in Wireless-Connected Robotic
  Networks: Overview and Design Guidelines</title><categories>cs.RO</categories><comments>This draft has been submitted for publication in International
  Conference on Communications ICC'2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed the prosperity of robots and in order to support
consensus and cooperation for multi-robot system, wireless communications and
networking among robots and the infrastructure have become indispensable. In
this technical note, we first provide an overview of the research contributions
on communication-aware motion planning (CAMP) in designing wireless-connected
robotic networks (WCRNs), where the degree-of-freedom (DoF) provided by motion
and communication capabilities embraced by the robots have not been fully
exploited. Therefore, we propose the framework of joint communication-motion
planning (JCMP) as well as the architecture for incorporating JCMP in WCRNs.
The proposed architecture is motivated by the observe-orient-decision-action
(OODA) model commonly adopted in robotic motion control and cognitive radio.
Then, we provide an overview of the orient module that quantify the
connectivity assessment. Afterwards, we highlight the JCMP module and compare
it with the conventional communication-planning, where the necessity of the
JCMP is validated via both theoretical analysis and simulation results of an
illustrative example. Finally, a series of open problems are discussed, which
picture the gap between the state-of-the-art and a practical WCRN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02300</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02300</id><created>2015-11-06</created><authors><author><keyname>Song</keyname><forenames>Shuran</forenames></author><author><keyname>Xiao</keyname><forenames>Jianxiong</forenames></author></authors><title>Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the task of amodal 3D object detection in RGB-D images, which
aims to produce a 3D bounding box of an object in metric form at its full
extent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a
3D volumetric scene from a RGB-D image as input and outputs 3D object bounding
boxes. In our approach, we propose the first 3D Region Proposal Network (RPN)
to learn objectness from geometric shapes and the first joint Object
Recognition Network (ORN) to extract geometric features in 3D and color
features in 2D. In particular, we handle objects of various sizes by training
an amodal RPN at two different scales and an ORN to regress 3D bounding boxes.
Experiments show that our algorithm outperforms the state-of-the-art by 13.8 in
mAP and is 200x faster than the original Sliding Shapes. All source code and
pre-trained models will be available at GitHub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02301</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02301</id><created>2015-11-06</created><updated>2016-01-05</updated><authors><author><keyname>Hill</keyname><forenames>Felix</forenames></author><author><keyname>Bordes</keyname><forenames>Antoine</forenames></author><author><keyname>Chopra</keyname><forenames>Sumit</forenames></author><author><keyname>Weston</keyname><forenames>Jason</forenames></author></authors><title>The Goldilocks Principle: Reading Children's Books with Explicit Memory
  Representations</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new test of how well language models capture meaning in
children's books. Unlike standard language modelling benchmarks, it
distinguishes the task of predicting syntactic function words from that of
predicting lower-frequency words, which carry greater semantic content. We
compare a range of state-of-the-art models, each with a different way of
encoding what has been previously read. We show that models which store
explicit representations of long-term contexts outperform state-of-the-art
neural language models at predicting semantic content words, although this
advantage is not observed for syntactic function words. Interestingly, we find
that the amount of text encoded in a single memory representation is highly
influential to the performance: there is a sweet-spot, not too big and not too
small, between single words and full sentences that allows the most meaningful
information in a text to be effectively retained and recalled. Further, the
attention over such window-based memories can be trained effectively through
self-supervision. We then assess the generality of this principle by applying
it to the CNN QA benchmark, which involves identifying named entities in
paraphrased summaries of news articles, and achieve state-of-the-art
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02307</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02307</id><created>2015-11-07</created><authors><author><keyname>Tahmasbi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>On the Capacity Achieving Probability Measures for Molecular Receivers</title><categories>cs.IT math.IT</categories><comments>6 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, diffusion-based molecular commu- nication with ligand receptor
receivers is studied. Information messages are assumed to be encoded via
variations of the con- centration of molecules. The randomness in the ligand
reception process induces uncertainty in the communication; limiting the rate
of information decoding. We model the ligand receptor receiver by a set of
finite-state Markov channels and study the general capacity of such a receiver.
Furthermore, the i.i.d. capacity of the receiver is characterized as a lower
bound for the general capacity. It is also proved that a finite support
probability measure can achieve the i.i.d. capacity of the receiver. Moreover,
a bound on the number of points in the support of the probability measure is
obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02308</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02308</id><created>2015-11-07</created><authors><author><keyname>Arvind</keyname><forenames>V.</forenames></author><author><keyname>Raja</keyname><forenames>S.</forenames></author></authors><title>Some Lower Bound Results for Set-Multilinear Arithmetic Computations</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the structure of set-multilinear arithmetic circuits
and set-multilinear branching programs with the aim of showing lower bound
results. We define some natural restrictions of these models for which we are
able to show lower bound results. Some of our results extend existing lower
bounds, while others are new and raise open questions. More specifically, our
main results are the following:
  (1) We observe that set-multilinear arithmetic circuits can be transformed
into shallow set-multilinear circuits efficiently, similar to depth reduction
results of [VSBR83,RY08] for more general commutative circuits. As a
consequence, we note that polynomial size set-multilinear circuits have
quasi-polynomial size set-multilinear branching programs. We show that
\emph{narrow} set-multilinear ABPs (with a restricted number of set types)
computing the Permanent polynomial $\mathrm{PER}_n$ require $2^{n^{\Omega(1)}}$
size. A similar result for general set-multilinear ABPs appears difficult as it
would imply that the Permanent requires superpolynomial size set-multilinear
circuits. It would also imply that the noncommutative Permanent requires
superpolynomial size noncommutative arithmetic circuits.
  (2) Indeed, we also show that set-multilinear branching programs are
exponentially more powerful than \emph{interval} multilinear circuits (where
the index sets for each gate is restricted to be an interval w.r.t.\ some
ordering), assuming the sum-of-squares conjecture. This further underlines the
power of set-multilinear branching programs.
  (3) Finally, we consider set-multilinear circuits with restrictions on the
number of proof trees of monomials computed by it, and prove exponential lower
bounds results. This raises some new lower bound questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02318</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02318</id><created>2015-11-07</created><authors><author><keyname>Srikanth</keyname><forenames>Kavirayani</forenames></author><author><keyname>Kumar</keyname><forenames>Gundavarapu Nagesh</forenames></author></authors><title>Stabilization at upright equilibrium position of a double inverted
  pendulum with unconstrained bat optimization</title><categories>cs.SY</categories><comments>15 Pages</comments><doi>10.5121/ijcsa.2015.5508</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A double inverted pendulum plant has been in the domain of control
researchers as an established model for studies on stability. The stability of
such as a system taking the linearized plant dynamics has yielded satisfactory
results by many researchers using classical control techniques. The established
model that is analyzed as part of this work was tested under the influence of
time delay, where the controller was fine tuned using a BAT algorithm taking
into considering the fitness function of square of error. This proposed method
gave results which were better when compared without time delay wherein the
calculated values indicated the issues when incorporating time delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02319</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02319</id><created>2015-11-07</created><authors><author><keyname>Saghafi</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Hussain</keyname><forenames>Aini</forenames></author><author><keyname>Zaman</keyname><forenames>Halimah Badioze</forenames></author><author><keyname>Saad</keyname><forenames>Mohamad Hanif Md</forenames></author></authors><title>Review of Person Re-identification Techniques</title><categories>cs.CV</categories><comments>Published 2014</comments><journal-ref>IET Computer Vision, 2014, 8, (6), p. 455-474</journal-ref><doi>10.1049/iet-cvi.2013.0180</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Person re-identification across different surveillance cameras with disjoint
fields of view has become one of the most interesting and challenging subjects
in the area of intelligent video surveillance. Although several methods have
been developed and proposed, certain limitations and unresolved issues remain.
In all of the existing re-identification approaches, feature vectors are
extracted from segmented still images or video frames. Different similarity or
dissimilarity measures have been applied to these vectors. Some methods have
used simple constant metrics, whereas others have utilised models to obtain
optimised metrics. Some have created models based on local colour or texture
information, and others have built models based on the gait of people. In
general, the main objective of all these approaches is to achieve a
higher-accuracy rate and lowercomputational costs. This study summarises
several developments in recent literature and discusses the various available
methods used in person re-identification. Specifically, their advantages and
disadvantages are mentioned and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02321</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02321</id><created>2015-11-07</created><authors><author><keyname>Curticapean</keyname><forenames>Radu</forenames></author><author><keyname>Xia</keyname><forenames>Mingji</forenames></author></authors><title>Parameterizing the Permanent: Genus, Apices, Minors, Evaluation mod 2^k</title><categories>cs.CC</categories><comments>35 pages, appears in FOCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify and study relevant structural parameters for the problem
PerfMatch of counting perfect matchings in a given input graph $G$. These
generalize the well-known tractable planar case, and they include the genus of
$G$, its apex number (the minimum number of vertices whose removal renders $G$
planar), and its Hadwiger number (the size of a largest clique minor).
  To study these parameters, we first introduce the notion of combined
matchgates, a general technique that bridges parameterized counting problems
and the theory of so-called Holants and matchgates: Using combined matchgates,
we can simulate certain non-existing gadgets $F$ as linear combinations of
$t=O(1)$ existing gadgets. If a graph $G$ features $k$ occurrences of $F$, we
can then reduce $G$ to $t^k$ graphs that feature only existing gadgets, thus
enabling parameterized reductions.
  As applications of this technique, we simplify known $4^g n^{O(1)}$ time
algorithms for PerfMatch on graphs of genus $g$. Orthogonally to this, we show
#W[1]-hardness of the permanent on $k$-apex graphs, implying its #W[1]-hardness
under the Hadwiger number. Additionally, we rule out $n^{o(k/\log k)}$ time
algorithms under the counting exponential-time hypothesis #ETH.
  Finally, we use combined matchgates to prove parity-W[1]-hardness of
evaluating the permanent modulo $2^k$, complementing an $O(n^{4k-3})$ time
algorithm by Valiant and answering an open question of Bj\&quot;orklund. We also
obtain a lower bound of $n^{\Omega(k/\log k)}$ under the parity version of the
exponential-time hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02325</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02325</id><created>2015-11-07</created><authors><author><keyname>Xiao</keyname><forenames>Zhenyu</forenames></author><author><keyname>Bai</keyname><forenames>Lin</forenames></author><author><keyname>Choi</keyname><forenames>Jinho</forenames></author></authors><title>Iterative Joint Beamforming Training with Constant-Amplitude Phased
  Arrays in Millimeter-Wave Communication</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><journal-ref>IEEE Communications Letters, vol. 18, no. 5, pp. 829-832, May 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In millimeter-wave communications (MMWC), in order to compensate for high
propagation attenuation, phased arrays are favored to achieve array gain by
beamforming, where transmitting and receiving antenna arrays need to be jointly
trained to obtain appropriate antenna weight vectors (AWVs). Since the
amplitude of each element of the AWV is usually constraint constant to simplify
the design of phased arrays in MMWC, the existing singular vector based
beamforming training scheme cannot be used for such devices. Thus, in this
letter, a steering vector based iterative beamforming training scheme, which
exploits the directional feature of MMWC channels, is proposed for devices with
constant-amplitude phased arrays. Performance evaluations show that the
proposed scheme achieves a fast convergence rate as well as a near optimal
array gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02326</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02326</id><created>2015-11-07</created><authors><author><keyname>Xiao</keyname><forenames>Zhenyu</forenames></author></authors><title>Suboptimal Spatial Diversity Scheme for 60 GHz Millimeter-Wave WLAN</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><journal-ref>IEEE Communications Letters, vol. 17, no. 9, pp. 1790-1793, Sep.
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter revisits the equal-gain (EG) spatial diversity technique, which
was proposed to combat the human-induced shadowing for 60 GHz wireless local
area network, under a more practical frequency-selective multi-input
multi-output channel. Subsequently, a suboptimal spatial diversity scheme
called maximal selection (MS) is proposed by tracing the shadowing process,
owing to a considerably high data rate. Comparisons show that MS outperforms EG
in terms of link margin and saves computation complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02338</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02338</id><created>2015-11-07</created><authors><author><keyname>Hirota</keyname><forenames>Osamu</forenames></author></authors><title>Towards Quantum Enigma Cipher II-A protocol based on quantum
  illumination-</title><categories>quant-ph cs.CR</categories><comments>Submitted to Quantum ICT Research Institute Bulletin</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research note II introduces a way to understand a basic concept of the
quantum enigma cipher. The conventional cipher is designed by a mathematical
algorithm and its security is evaluated by the complexity of the algorithm in
security analysis and ability of computers. This kind of cipher can be
decrypted with probability one in principle by the Brute force attack in which
an eavesdropper tries all the possible keys based on the correct ciphertext and
some known plaintext. A cipher with quantum effects in physical layer may
protect the system from the Brute force attack by means of the quantum no
cloning theorem and randomizations based on quantum noise effect. The
randomizations for the ciphertext which is the output from the mathematical
encryption box is crucial to realize a quantum enigma cipher. Especially, by
randomizations, it is necessary to make a substantial difference in accuracy of
ciphertext in eavesdropper's observation and legitimate user's observation. The
quantum illumination protocol can make a difference in error performance of the
legitimate's receiver and the eavesdropper's receiver. This difference is due
to differences in ability of the legitimate's receiver with entanglement and
the eavesdropper's receiver without entanglement. It is shown in this note that
the quantum illumination can be employed as an element of the most simple
quantum enigma cipher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02339</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02339</id><created>2015-11-07</created><authors><author><keyname>Papapetrou</keyname><forenames>Maria</forenames></author><author><keyname>Kugiumtzis</keyname><forenames>Dimitris</forenames></author></authors><title>Markov chain order estimation with parametric significance tests of
  conditional mutual information</title><categories>stat.ME cs.IT math.IT physics.data-an</categories><comments>19 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Besides the different approaches suggested in the literature, accurate
estimation of the order of a Markov chain from a given symbol sequence is an
open issue, especially when the order is moderately large. Here, parametric
significance tests of conditional mutual information (CMI) of increasing order
$m$, $I_c(m)$, on a symbol sequence are conducted for increasing orders $m$ in
order to estimate the true order $L$ of the underlying Markov chain. CMI of
order $m$ is the mutual information of two variables in the Markov chain being
$m$ time steps apart, conditioning on the intermediate variables of the chain.
The null distribution of CMI is approximated with a normal and gamma
distribution deriving analytic expressions of their parameters, and a gamma
distribution deriving its parameters from the mean and variance of the normal
distribution. The accuracy of order estimation is assessed with the three
parametric tests, and the parametric tests are compared to the randomization
significance test and other known order estimation criteria using Monte Carlo
simulations of Markov chains with different order $L$, length of symbol
sequence $N$ and number of symbols $K$. The parametric test using the gamma
distribution (with directly defined parameters) is consistently better than the
other two parametric tests and matches well the performance of the
randomization test. The tests are applied to genes and intergenic regions of
DNA sequences, and the estimated orders are interpreted in view of the results
from the simulation study. The application shows the usefulness of the
parametric gamma test for long symbol sequences where the randomization test
becomes prohibitively slow to compute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02348</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02348</id><created>2015-11-07</created><authors><author><keyname>Zhang</keyname><forenames>Jianhui</forenames></author><author><keyname>Tang</keyname><forenames>Shao-jie</forenames></author><author><keyname>Shen</keyname><forenames>Xingfa</forenames></author><author><keyname>Dai</keyname><forenames>Guojun</forenames></author><author><keyname>Nayak</keyname><forenames>Amiya</forenames></author></authors><title>Quorum-based Localized Scheme for Duty Cycling in Asynchronous Sensor
  Networks</title><categories>cs.NI</categories><comments>11 pages, 13 figures, IEEE 8th International Conference on Mobile Ad
  hoc and Sensor Systems (MASS, 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many TDMA- and CSMA-based protocols try to obtain fair channel access and to
increase channel utilization. It is still challenging and crucial in Wireless
Sensor Networks (WSNs), especially when the time synchronization cannot be well
guaranteed and consumes much extra energy. This paper presents a localized and
ondemand scheme ADC to adaptively adjust duty cycle based on quorum systems.
ADC takes advantages of TDMA and CSMA and guarantees that (1) each node can
fairly access channel based on its demand, (2) channel utilization can be
increased by reducing competition for channel access among neighboring nodes,
(3) every node has at least one rendezvous active time slot with each of its
neighboring nodes even under asynchronization. The latency bound of data
aggregation is analyzed under ADC to show that ADC can bound the latency under
both synchronization and asynchronization. We conduct extensive experiments in
TinyOS on a real test-bed with TelosB nodes to evaluate the performance of ADC.
Comparing with B-MAC, ADC substantially reduces the contention for channel
access and energy consumption, and improves network throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02352</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02352</id><created>2015-11-07</created><updated>2015-11-16</updated><authors><author><keyname>Wiharto</keyname><forenames>Wiharto</forenames></author><author><keyname>Kusnanto</keyname><forenames>Hari</forenames></author><author><keyname>Herianto</keyname><forenames>Herianto</forenames></author></authors><title>Performance Analysis of Multiclass Support Vector Machine Classification
  for Diagnosis of Coronary Heart Diseases</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic diagnosis of coronary heart disease helps the doctor to support in
decision making a diagnosis. Coronary heart disease have some types or levels.
Referring to the UCI Repository dataset, it divided into 4 types or levels that
are labeled numbers 1-4 (low, medium, high and serious). The diagnosis models
can be analyzed with multiclass classification approach. One of multiclass
classification approach used, one of which is a support vector machine (SVM).
The SVM use due to strong performance of SVM in binary classification. This
research study multiclass performance classification support vector machine to
diagnose the type or level of coronary heart disease. Coronary heart disease
patient data taken from the UCI Repository. Stages in this study is
preprocessing, which consist of, to normalizing the data, divide the data into
data training and testing. The next stage of multiclass classification and
performance analysis. This study uses multiclass SVM algorithm, namely: Binary
Tree Support Vector Machine (BTSVM), One-Against-One (OAO), One-Against-All
(OAA), Decision Direct Acyclic Graph (DDAG) and Exhaustive Output Error
Correction Code (ECOC). Performance parameter used is recall, precision,
F-measure and Overall accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02354</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02354</id><created>2015-11-07</created><authors><author><keyname>Ludwig</keyname><forenames>Arne</forenames></author><author><keyname>Fuerst</keyname><forenames>Carlo</forenames></author><author><keyname>Henze</keyname><forenames>Alexander</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author></authors><title>Opposites Attract: Virtual Cluster Embedding for Profit</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that cloud application performance critically depends on the
network. Accordingly, new abstractions for cloud applications are proposed
which extend the performance isolation guarantees to the network. The most
common abstraction is the Virtual Cluster V C(n, b): the n virtual machines of
a customer are connected to a virtual switch at bandwidth b. However, today,
not much is known about how to efficiently embed and price virtual clusters.
This paper makes two contributions. (1) We present an algorithm called Tetris
that efficiently embeds virtual clusters arriving in an online fashion, by
jointly optimizing the node and link resources. We show that this algorithm
allows to multiplex more virtual clusters over the same physical infrastructure
compared to existing algorithms, hence improving the provider profit. (2) We
present the first demand-specific pricing model called DSP for virtual
clusters. Our pricing model is fair in the sense that a customer only needs to
pay for what he or she asked. Moreover, it features other desirable properties,
such as price independence from mapping locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02369</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02369</id><created>2015-11-07</created><authors><author><keyname>Cao</keyname><forenames>Yuan</forenames></author><author><keyname>Cao</keyname><forenames>Yonglin</forenames></author><author><keyname>Gao</keyname><forenames>Jian</forenames></author></authors><title>On a class of $(\delta+\alpha u^2)$-constacyclic codes over
  $\mathbb{F}_{q}[u]/\langle u^4\rangle$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbb{F}_{q}$ be a finite field of cardinality $q$,
$R=\mathbb{F}_{q}[u]/\langle
u^4\rangle=\mathbb{F}_{q}+u\mathbb{F}_{q}+u^2\mathbb{F}_{q}+u^3\mathbb{F}_{q}$
$(u^4=0)$ which is a finite chain ring, and $n$ be a positive integer
satisfying ${\rm gcd}(q,n)=1$. For any $\delta,\alpha\in
\mathbb{F}_{q}^{\times}$, an explicit representation for all distinct
$(\delta+\alpha u^2)$-constacyclic codes over $R$ of length $n$ is given, and
the dual code for each of these codes is determined. For the case of $q=2^m$
and $\delta=1$, all self-dual $(1+\alpha u^2)$-constacyclic codes over $R$ of
odd length $n$ are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02375</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02375</id><created>2015-11-07</created><authors><author><keyname>Zhou</keyname><forenames>Kai</forenames></author><author><keyname>Ren</keyname><forenames>Jian</forenames></author></authors><title>CASO: Cost-Aware Secure Outsourcing of General Computational Problems</title><categories>cs.CR</categories><comments>32 pages, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation outsourcing is an integral part of cloud computing. It enables
end-users to outsource their computational tasks to the cloud and utilize the
shared cloud resources in a pay-per-use manner. However, once the tasks are
outsourced, the end-users will lose control of their data, which may result in
severe security issues especially when the data is sensitive. To address this
problem, secure outsourcing mechanisms have been proposed to ensure security of
the end-users' outsourced data. In this paper, we investigate outsourcing of
general computational problems which constitute the mathematical basics for
problems emerged from various fields such as engineering and finance. To be
specific, we propose affine mapping based schemes for the problem
transformation and outsourcing so that the cloud is unable to learn any key
information from the transformed problem. Meanwhile, the overhead for the
transformation is limited to an acceptable level compared to the computational
savings introduced by the outsourcing itself. Furthermore, we develop
cost-aware schemes to balance the trade-offs between end-users' various
security demands and computational overhead. We also propose a verification
scheme to ensure that the end-users will always receive a valid solution from
the cloud. Our extensive complexity and security analysis show that our
proposed Cost-Aware Secure Outsourcing (CASO) scheme is both practical and
effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02378</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02378</id><created>2015-11-07</created><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Li</keyname><forenames>Tongtong</forenames></author><author><keyname>Ren</keyname><forenames>Jian</forenames></author></authors><title>Optimal Construction of Regenerating Code through Rate-matching in
  Hostile Networks</title><categories>cs.IT cs.CR math.IT</categories><comments>31 pages, 7 figures, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regenerating code is a class of code very suitable for distributed storage
systems, which can maintain optimal bandwidth and storage space. Two types of
important regenerating code have been constructed: the minimum storage
regeneration (MSR) code and the minimum bandwidth regeneration (MBR) code.
However, in hostile networks where adversaries can compromise storage nodes,
the storage capacity of the network can be significantly affected. In this
paper, we propose two optimal constructions of regenerating codes through
rate-matching that can combat against this kind of adversaries in hostile
networks: 2-layer rate-matched regenerating code and $m$-layer rate-matched
regenerating code. For the 2-layer code, we can achieve the optimal storage
efficiency for given system requirements. Our comprehensive analysis shows that
our code can detect and correct malicious nodes with higher storage efficiency
compared to the universally resilient regenerating code which is a
straightforward extension of regenerating code with error detection and
correction capability. Then we propose the $m$-layer code by extending the
2-layer code and achieve the optimal error correction efficiency by matching
the code rate of each layer's regenerating code. We also demonstrate that the
optimized parameter can achieve the maximum storage capacity under the same
constraint. Compared to the universally resilient regenerating code, our code
can achieve much higher error correction efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02381</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02381</id><created>2015-11-07</created><updated>2016-01-17</updated><authors><author><keyname>Asoodeh</keyname><forenames>Shahab</forenames></author><author><keyname>Diaz</keyname><forenames>Mario</forenames></author><author><keyname>Alajaji</keyname><forenames>Fady</forenames></author><author><keyname>Linder</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Information Extraction Under Privacy Constraints</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>55 pages, 6 figures. Improved the organization and added detailed
  literature review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A privacy-constrained information extraction problem is considered where for
a pair of correlated discrete random variables $(X,Y)$ governed by a given
joint distribution, an agent observes $Y$ and wants to convey to a potentially
public user as much information about $Y$ as possible without compromising the
amount of information revealed about $X$. To this end, the so-called {\em
rate-privacy function} is introduced to quantify the maximal amount of
information (measured in terms of mutual information) that can be extracted
from $Y$ under a privacy constraint between $X$ and the extracted information,
where privacy is measured using either mutual information or maximal
correlation. Properties of the rate-privacy function are analyzed and
information-theoretic and estimation-theoretic interpretations of it are
presented for both the mutual information and maximal correlation privacy
measures. It is also shown that the rate-privacy function admits a closed-form
expression for a large family of joint distributions of $(X,Y)$. Finally, the
rate-privacy function under the mutual information privacy measure is
considered for the case where $(X,Y)$ has a joint probability density function
by studying the problem where the extracted information is a uniform
quantization of $Y$ corrupted by additive Gaussian noise. The asymptotic
behavior of the rate-privacy function is studied as the quantization resolution
grows without bound and it is observed that not all of the properties of the
rate-privacy function carry over from the discrete to the continuous case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02385</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02385</id><created>2015-11-07</created><authors><author><keyname>Orimaye</keyname><forenames>Sylvester Olubolu</forenames></author><author><keyname>Alhashmi</keyname><forenames>Saadat M.</forenames></author><author><keyname>Siew</keyname><forenames>Eu-Gene</forenames></author><author><keyname>Kang</keyname><forenames>Sang Jung</forenames></author></authors><title>Review-Level Sentiment Classification with Sentence-Level Polarity
  Correction</title><categories>cs.CL cs.AI cs.LG</categories><comments>15 pages. This paper is based on the same sentence-level technique
  proposed in Orimaye, S. O., Alhashmi, S. M., and Siew, E. G. Buy it-dont buy
  it: sentiment classification on Amazon reviews using sentence polarity shift.
  In PRICAI 2012: Trends in Artificial Intelligence, pp. 386-399. Springer
  Berlin Heidelberg</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an effective technique to solving review-level sentiment
classification problem by using sentence-level polarity correction. Our
polarity correction technique takes into account the consistency of the
polarities (positive and negative) of sentences within each product review
before performing the actual machine learning task. While sentences with
inconsistent polarities are removed, sentences with consistent polarities are
used to learn state-of-the-art classifiers. The technique achieved better
results on different types of products reviews and outperforms baseline models
without the correction technique. Experimental results show an average of 82%
F-measure on four different product review domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02386</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02386</id><created>2015-11-07</created><authors><author><keyname>Ranganath</keyname><forenames>Rajesh</forenames></author><author><keyname>Tran</keyname><forenames>Dustin</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Hierarchical Variational Models</title><categories>stat.ML cs.LG stat.CO stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Black box inference allows researchers to easily prototype and evaluate an
array of models. Recent advances in variational inference allow such algorithms
to scale to high dimensions. However, a central question remains: How to
specify an expressive variational distribution which maintains efficient
computation? To address this, we develop hierarchical variational models. In a
hierarchical variational model, the variational approximation is augmented with
a prior on its parameters, such that the latent variables are conditionally
independent given this shared structure. This preserves the computational
efficiency of the original approximation, while admitting hierarchically
complex distributions for both discrete and continuous latent variables. We
study hierarchical variational models on a variety of deep discrete latent
variable models. Hierarchical variational models generalize other expressive
variational distributions and maintains higher fidelity to the posterior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02393</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02393</id><created>2015-11-07</created><authors><author><keyname>Xu</keyname><forenames>Bojian</forenames></author></authors><title>On Stabbing Queries for Generalized Longest Repeat</title><categories>cs.DS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A longest repeat query on a string, motivated by its applications in many
subfields including computational biology, asks for the longest repetitive
substring(s) covering a particular string position (point query). In this
paper, we extend the longest repeat query from point query to \emph{interval
query}, allowing the search for longest repeat(s) covering any position
interval, and thus significantly improve the usability of the solution. Our
method for interval query takes a different approach using the insight from a
recent work on \emph{shortest unique substrings} [1], as the prior work's
approach for point query becomes infeasible in the setting of interval query.
Using the critical insight from [1], we propose an indexing structure, which
can be constructed in the optimal $O(n)$ time and space for a string of size
$n$, such that any future interval query can be answered in $O(1)$ time.
Further, our solution can find \emph{all} longest repeats covering any given
interval using optimal $O(occ)$ time, where $occ$ is the number of longest
repeats covering that given interval, whereas the prior $O(n)$-time and space
work can find only one candidate for each point query. Experiments with
real-world biological data show that our proposal is competitive with prior
works, both time and space wise, while providing with the new functionality of
interval queries as opposed to point queries provided by prior works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02399</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02399</id><created>2015-11-07</created><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>On Welfare Approximation and Stable Pricing</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the power of item-pricing as a tool for approximately optimizing
social welfare in a combinatorial market. We consider markets with $m$
indivisible items and $n$ buyers. The goal is to set prices to the items so
that, when agents purchase their most demanded sets simultaneously, no
conflicts arise and the obtained allocation has nearly optimal welfare. For
gross substitutes valuations, it is well known that it is possible to achieve
optimal welfare in this manner. We ask: can one achieve approximately efficient
outcomes for valuations beyond gross substitutes? We show that even for
submodular valuations, and even with only two buyers, one cannot guarantee an
approximation better than $\Omega(\sqrt{m})$. The same lower bound holds for
the class of single-minded buyers as well. Beyond the negative results on
welfare approximation, our results have daunting implications on revenue
approximation for these valuation classes: in order to obtain good
approximation to the collected revenue, one would necessarily need to abandon
the common approach of comparing the revenue to the optimal welfare; a
fundamentally new approach would be required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02402</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02402</id><created>2015-11-07</created><authors><author><keyname>Zadeh</keyname><forenames>Sepehr Abbasi</forenames></author><author><keyname>Ghadiri</keyname><forenames>Mehrdad</forenames></author></authors><title>Max-Sum Diversification, Monotone Submodular Functions and Semi-metric
  Spaces</title><categories>cs.LG</categories><comments>This article draws heavily from arXiv:1203.6397 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications such as web-based search, document summarization,
facility location and other applications, the results are preferable to be both
representative and diversified subsets of documents. The goal of this study is
to select a good &quot;quality&quot;, bounded-size subset of a given set of items, while
maintaining their diversity relative to a semi-metric distance function. This
problem was first studied by Borodin et al\cite{borodin}, but a crucial
property used throughout their proof is the triangle inequality. In this
modified proof, we want to relax the triangle inequality and relate the
approximation ratio of max-sum diversification problem to the parameter of the
relaxed triangle inequality in the normal form of the problem (i.e., a uniform
matroid) and also in an arbitrary matroid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02407</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02407</id><created>2015-11-07</created><updated>2015-12-05</updated><authors><author><keyname>Roychowdhury</keyname><forenames>Sohini</forenames></author><author><keyname>Emmons</keyname><forenames>Michelle</forenames></author></authors><title>A Survey of the Trends in Facial and Expression Recognition Databases
  and Methods</title><categories>cs.CV</categories><comments>16 pages, 4 figures, 3 tables, International Journal of Computer
  Science and Engineering Survey, October, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated facial identification and facial expression recognition have been
topics of active research over the past few decades. Facial and expression
recognition find applications in human-computer interfaces, subject tracking,
real-time security surveillance systems and social networking. Several holistic
and geometric methods have been developed to identify faces and expressions
using public and local facial image databases. In this work we present the
evolution in facial image data sets and the methodologies for facial
identification and recognition of expressions such as anger, sadness,
happiness, disgust, fear and surprise. We observe that most of the earlier
methods for facial and expression recognition aimed at improving the
recognition rates for facial feature-based methods using static images.
However, the recent methodologies have shifted focus towards robust
implementation of facial/expression recognition from large image databases that
vary with space (gathered from the internet) and time (video recordings). The
evolution trends in databases and methodologies for facial and expression
recognition can be useful for assessing the next-generation topics that may
have applications in security systems or personal identification systems that
involve &quot;Quantitative face&quot; assessments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02411</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02411</id><created>2015-11-07</created><authors><author><keyname>Burstein</keyname><forenames>David</forenames></author><author><keyname>Rubin</keyname><forenames>Jonathan</forenames></author></authors><title>Sufficient Conditions for Graphicality of Bidegree Sequences</title><categories>math.CO cs.DM</categories><comments>17 pages</comments><msc-class>05C20, 05C80, 05C82</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are a variety of existing conditions for a degree sequence to be
graphic. When a degree sequence satisfies any of these conditions, there exists
a graph that realizes the sequence. We formulate several novel sufficient
graphicality criteria that depend on the number of elements in the sequence,
corresponding to the number of nodes in an associated graph, and the mean
degree of the sequence. These conditions, which are stated in terms of bidegree
sequences for directed graphs, are easier to apply than classic necessary and
sufficient graphicality conditions involving multiple inequalities. They are
also more flexible than more recent graphicality conditions, in that they imply
graphicality of some degree sequences not covered by those conditions. The form
of our results will allow them to be easily used for the generation of graphs
with particular degree sequences for applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02415</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02415</id><created>2015-11-07</created><authors><author><keyname>Trtik</keyname><forenames>Marek</forenames></author></authors><title>Anonymous On-line Communication Between Program Analyses</title><categories>cs.PL</categories><comments>Regular paper, 20 pages. arXiv admin note: text overlap with
  arXiv:1504.07862</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a light-weight client-server model of communication between
program analyses. Clients are individual analyses and the server mediates their
communication. A client cannot see properties of any other and the
communication is anonymous. There is no central algorithm standing above
clients which would tell them when to communicate what information. Clients
communicate with others spontaneously, according to their actual personal
needs. The model is based on our observation that a piece of information
provided to an analysis at a right place may (substantially) improve its
result. We evaluated the proposed communication model for all possible
combinations of three clients on more than 400 benchmarks and the results show
that the communication model performs well in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02420</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02420</id><created>2015-11-07</created><authors><author><keyname>Lotfi</keyname><forenames>Ehsan</forenames></author></authors><title>Design of an Alarm System for Isfahan Ozone Level based on Artificial
  Intelligence Predictor Models</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ozone level prediction is an important task of air quality agencies of
modern cities. In this paper, we design an ozone level alarm system (OLP) for
Isfahan city and test it through the real word data from 1-1-2000 to 7-6-2011.
We propose a computer based system with three inputs and single output. The
inputs include three sensors of solar ultraviolet (UV), total solar radiation
(TSR) and total ozone (O3). And the output of the system is the predicted O3 of
the next day and the alarm massages. A developed artificial intelligence (AI)
algorithm is applied to determine the output, based on the inputs variables.
For this issue, AI models, including supervised brain emotional learning (BEL),
adaptive neuro-fuzzy inference system (ANFIS) and artificial neural networks
(ANNs), are compared in order to find the best model. The simulation of the
proposed system shows that it can be used successfully in prediction of major
cities ozone level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02425</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02425</id><created>2015-11-07</created><authors><author><keyname>Park</keyname><forenames>Jeonghun</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Low Complexity Antenna Selection in Dense Cloud Radio Access Networks</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a low complexity antenna selection algorithm for cloud radio
access networks, which consists of two phases. In the first phase, each remote
radio head (RRH) determines whether to be included in a candidate set by using
a predefined selection threshold. In the second phase, RRHs are randomly
selected within the candidate set made in the first phase. To analyze the
performance of the proposed algorithm, we model RRHs and users locations by a
homogeneous Poisson point process. In such assumption, the
signal-to-interference ratio (SIR) complementary cumulative distribution
function is derived. By approximating the derived expression, an approximate
optimum selection threshold that maximizes the SIR coverage probability is
obtained. The obtained threshold can be modified depending on various algorithm
setups. With the obtained threshold, we characterize the performance of the
algorithm in an asymptotic regime where the RRH density goes to infinity.
Assuming that a user stays at a specific location during multiple transmissions
of data, we propose a selection threshold adaptation method. A nice feature of
the proposed algorithm is that the algorithm complexity is independent to the
RRH density, which reduces the computation burden in baseband units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02426</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02426</id><created>2015-11-07</created><authors><author><keyname>Lotfi</keyname><forenames>E.</forenames></author></authors><title>A Winner-Take-All Approach to Emotional Neural Networks with Universal
  Approximation Property</title><categories>cs.AI</categories><comments>Information Sciences (2015), Elsevier Publisher</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, we propose a brain-inspired winner-take-all emotional neural network
(WTAENN) and prove the universal approximation property for the novel
architecture. WTAENN is a single layered feedforward neural network that
benefits from the excitatory, inhibitory, and expandatory neural connections as
well as the winner-take-all (WTA) competitions in the human brain s nervous
system. The WTA competition increases the information capacity of the model
without adding hidden neurons. The universal approximation capability of the
proposed architecture is illustrated on two example functions, trained by a
genetic algorithm, and then applied to several competing recent and benchmark
problems such as in curve fitting, pattern recognition, classification and
prediction. In particular, it is tested on twelve UCI classification datasets,
a facial recognition problem, three real world prediction problems (2 chaotic
time series of geomagnetic activity indices and wind farm power generation
data), two synthetic case studies with constant and nonconstant noise variance
as well as k-selector and linear programming problems. Results indicate the
general applicability and often superiority of the approach in terms of higher
accuracy and lower model complexity, especially where low computational
complexity is imperative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02429</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02429</id><created>2015-11-07</created><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Ahuja</keyname><forenames>Kartik</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>A Micro-foundation of Social Capital in Evolving Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Centrality, homophily, network formation, popularity, preferential
  attachment, social capital, social networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A social network confers benefits and advantages on individuals (and on
groups), the literature refers to these advantages as social capital. This
paper presents a micro-founded mathematical model of the evolution of a social
network and of the social capital of individuals within the network. The
evolution of the network is influenced by the extent to which individuals are
homophilic, structurally opportunistic, socially gregarious and by the
distribution of types in the society. In the analysis, we identify different
kinds of social capital: bonding capital, popularity capital, and bridging
capital. Bonding capital is created by forming a circle of connections,
homophily increases bonding capital because it makes this circle of connections
more homogeneous. Popularity capital leads to preferential attachment:
individuals who become popular tend to become more popular because others are
more likely to link to them. Homophily creates asymmetries in the levels of
popularity attained by different social groups, more gregarious types of agents
are more likely to become popular. However, in homophilic societies,
individuals who belong to less gregarious, less opportunistic, or major types
are likely to be more central in the network and thus acquire a bridging
capital.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02432</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02432</id><created>2015-11-07</created><authors><author><keyname>Kwak</keyname><forenames>Son-Il</forenames></author><author><keyname>Choe</keyname><forenames>Gang</forenames></author><author><keyname>Kim</keyname><forenames>In-Song</forenames></author><author><keyname>Jo</keyname><forenames>Gyong-Ho</forenames></author><author><keyname>Hwang</keyname><forenames>Chol-Jun</forenames></author></authors><title>A Study of an Modeling Method of T-S fuzzy System Based on Moving Fuzzy
  Reasoning and Its Application</title><categories>cs.AI</categories><comments>24 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To improve the effectiveness of the fuzzy identification, a structure
identification method based on moving rate is proposed for T-S fuzzy model. The
proposed method is called &quot;T-S modeling (or T-S fuzzy identification method)
based on moving rate&quot;. First, to improve the shortcomings of existing fuzzy
reasoning methods based on matching degree, the moving rates for s-type, z-type
and trapezoidal membership functions of T-S fuzzy model were defined. Then, the
differences between proposed moving rate and existing matching degree were
explained. Next, the identification method based on moving rate is proposed for
T-S model. Finally, the proposed identification method is applied to the fuzzy
modeling for the precipitation forecast and security situation prediction. Test
results show that the proposed method significantly improves the effectiveness
of fuzzy identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02433</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02433</id><created>2015-11-07</created><authors><author><keyname>Rodrigues</keyname><forenames>Andr&#xe9; Valente</forenames></author><author><keyname>Jorge</keyname><forenames>Al&#xed;pio</forenames></author><author><keyname>Dutra</keyname><forenames>In&#xea;s</forenames></author></authors><title>Accelerating Recommender Systems using GPUs</title><categories>cs.IR cs.DC</categories><journal-ref>SAC '15 Proceedings of the 30th Annual ACM Symposium on Applied
  Computing Pages 879-884 ACM New York, NY, USA</journal-ref><doi>10.1145/2695664.2695850</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe GPU implementations of the matrix recommender algorithms CCD++
and ALS. We compare the processing time and predictive ability of the GPU
implementations with existing multi-core versions of the same algorithms.
Results on the GPU are better than the results of the multi-core versions
(maximum speedup of 14.8).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02435</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02435</id><created>2015-11-07</created><authors><author><keyname>Kwak</keyname><forenames>Son-Il</forenames></author><author><keyname>Kown</keyname><forenames>O-Chol</forenames></author><author><keyname>Kim</keyname><forenames>Chang-Sin</forenames></author><author><keyname>Pak</keyname><forenames>Yong-Il</forenames></author><author><keyname>Son</keyname><forenames>Gum-Chol</forenames></author><author><keyname>Hwang</keyname><forenames>Chol-Jun</forenames></author><author><keyname>Kim</keyname><forenames>Hyon-Chol</forenames></author><author><keyname>Sin</keyname><forenames>Hyok-Chol</forenames></author><author><keyname>Hyon</keyname><forenames>Gyong-Il</forenames></author><author><keyname>Han</keyname><forenames>Sok-Min</forenames></author></authors><title>A Chinese POS Decision Method Using Korean Translation Information</title><categories>cs.CL</categories><comments>6 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a method that imitates a translation expert using
the Korean translation information and analyse the performance. Korean is good
at tagging than Chinese, so we can use this property in Chinese POS tagging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02436</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02436</id><created>2015-11-07</created><updated>2015-12-09</updated><authors><author><keyname>Orimaye</keyname><forenames>Sylvester Olubolu</forenames></author><author><keyname>Tai</keyname><forenames>Kah Yee</forenames></author><author><keyname>Wong</keyname><forenames>Jojo Sze-Meng</forenames></author><author><keyname>Wong</keyname><forenames>Chee Piau</forenames></author></authors><title>Learning Linguistic Biomarkers for Predicting Mild Cognitive Impairment
  using Compound Skip-grams</title><categories>cs.CL cs.AI</categories><comments>Accepted and presented at the 2015 NIPS Workshop on Machine Learning
  in Healthcare (MLHC), Montreal, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting Mild Cognitive Impairment (MCI) is currently a challenge as
existing diagnostic criteria rely on neuropsychological examinations. Automated
Machine Learning (ML) models that are trained on verbal utterances of MCI
patients can aid diagnosis. Using a combination of skip-gram features, our
model learned several linguistic biomarkers to distinguish between 19 patients
with MCI and 19 healthy control individuals from the DementiaBank language
transcript clinical dataset. Results show that a model with compound of
skip-grams has better AUC and could help ML prediction on small MCI data
sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02454</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02454</id><created>2015-11-08</created><authors><author><keyname>Cohen</keyname><forenames>Tamar</forenames></author><author><keyname>Yedidsion</keyname><forenames>Liron</forenames></author></authors><title>The Periodic Joint Replenishment Problem is Strongly NP-Hard</title><categories>cs.CC</categories><comments>29 Pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the long-standing open question regarding the
computational complexity of one of the core problems in supply chains
management, the periodic joint replenishment problem. This problem has received
a lot of attention over the years and many heuristic and approximation
algorithms were suggested. However, in spite of the vast effort, the complexity
of the problem remained unresolved. In this paper, we provide a proof that the
problem is indeed strongly NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02455</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02455</id><created>2015-11-08</created><updated>2016-02-15</updated><authors><author><keyname>Virie</keyname><forenames>Patrick</forenames></author></authors><title>(Yet) Another Theoretical Model of Thinking</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a theoretical, idealized model of the thinking process
with the following characteristics: 1) the model can produce complex thought
sequences and can be generalized to new inputs, 2) it can receive and maintain
input information indefinitely for the generation of thoughts and later use,
and 3) it supports learning while executing. The crux of the model lies within
the concept of internal consistency, or the generated thoughts should always be
consistent with the inputs from which they are created. Its merit, apart from
the capability to generate new creative thoughts from an internal mechanism,
depends on the potential to help training to generalize better. This is
consequently enabled by separating input information into several parts to be
handled by different processing components with a focus mechanism to fetch
information for each. This modularized view with the focus binds the model with
the computationally capable Turing machines. And as a final remark, this paper
constructively shows that the computational complexity of the model is at
least, if not surpass, that of a universal Turing machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02459</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02459</id><created>2015-11-08</created><authors><author><keyname>Xie</keyname><forenames>Duorui</forenames></author><author><keyname>Liang</keyname><forenames>Lingyu</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Li</keyname><forenames>Mengru</forenames></author></authors><title>SCUT-FBP: A Benchmark Dataset for Facial Beauty Perception</title><categories>cs.CV</categories><comments>6 pages, 8 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel face dataset with attractiveness ratings, namely, the
SCUT-FBP dataset, is developed for automatic facial beauty perception. This
dataset provides a benchmark to evaluate the performance of different methods
for facial attractiveness prediction, including the state-of-the-art deep
learning method. The SCUT-FBP dataset contains face portraits of 500 Asian
female subjects with attractiveness ratings, all of which have been verified in
terms of rating distribution, standard deviation, consistency, and
self-consistency. Benchmark evaluations for facial attractiveness prediction
were performed with different combinations of facial geometrical features and
texture features using classical statistical learning methods and the deep
learning method. The best Pearson correlation (0.8187) was achieved by the CNN
model. Thus, the results of our experiments indicate that the SCUT-FBP dataset
provides a reliable benchmark for facial beauty perception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02460</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02460</id><created>2015-11-08</created><authors><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author></authors><title>Graph Isomorphism for Bounded Genus Graphs In Linear Time</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every integer $g$, isomorphism of graphs of Euler genus at most $g$ can
be decided in linear time.
  This improves previously known algorithms whose time complexity is $n^{O(g)}$
(shown in early 1980's), and in fact, this is the first fixed-parameter
tractable algorithm for the graph isomorphism problem for bounded genus graphs
in terms of the Euler genus $g$. Our result also generalizes the seminal result
of Hopcroft and Wong in 1974, which says that the graph isomorphism problem can
be decided in linear time for planar graphs.
  Our proof is quite lengthly and complicated, but if we are satisfied with an
$O(n^3)$ time algorithm for the same problem, the proof is shorter and easier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02462</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02462</id><created>2015-11-08</created><updated>2015-11-13</updated><authors><author><keyname>Hoi</keyname><forenames>Steven C. H.</forenames></author><author><keyname>Wu</keyname><forenames>Xiongwei</forenames></author><author><keyname>Liu</keyname><forenames>Hantang</forenames></author><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Wang</keyname><forenames>Huiqiong</forenames></author><author><keyname>Xue</keyname><forenames>Hui</forenames></author><author><keyname>Wu</keyname><forenames>Qiang</forenames></author></authors><title>LOGO-Net: Large-scale Deep Logo Detection and Brand Recognition with
  Deep Region-based Convolutional Networks</title><categories>cs.CV</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logo detection from images has many applications, particularly for brand
recognition and intellectual property protection. Most existing studies for
logo recognition and detection are based on small-scale datasets which are not
comprehensive enough when exploring emerging deep learning techniques. In this
paper, we introduce &quot;LOGO-Net&quot;, a large-scale logo image database for logo
detection and brand recognition from real-world product images. To facilitate
research, LOGO-Net has two datasets: (i)&quot;logos-18&quot; consists of 18 logo classes,
10 brands, and 16,043 logo objects, and (ii) &quot;logos-160&quot; consists of 160 logo
classes, 100 brands, and 130,608 logo objects. We describe the ideas and
challenges for constructing such a large-scale database. Another key
contribution of this work is to apply emerging deep learning techniques for
logo detection and brand recognition tasks, and conduct extensive experiments
by exploring several state-of-the-art deep region-based convolutional networks
techniques for object detection tasks. The LOGO-net will be released at
http://logo-net.org/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02465</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02465</id><created>2015-11-08</created><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author><author><keyname>Liang</keyname><forenames>Lingyu</forenames></author><author><keyname>Feng</keyname><forenames>Ziyong</forenames></author><author><keyname>Xie</keyname><forenames>Duorui</forenames></author></authors><title>A new humanlike facial attractiveness predictor with cascaded
  fine-tuning deep learning model</title><categories>cs.CV</categories><comments>5 pages, 3 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a deep leaning method to address the challenging facial
attractiveness prediction problem. The method constructs a convolutional neural
network of facial beauty prediction using a new deep cascaded fine-turning
scheme with various face inputting channels, such as the original RGB face
image, the detail layer image, and the lighting layer image. With a carefully
designed CNN model of deep structure, large input size and small convolutional
kernels, we have achieved a high prediction correlation of 0.88. This result
convinces us that the problem of facial attractiveness prediction can be solved
by deep learning approach, and it also shows the important roles of the facial
smoothness, lightness, and color information that were involved in facial
beauty perception, which is consistent with the result of recent psychology
studies. Furthermore, we analyze the high-level features learnt by CNN through
visualization of its hidden layers, and some interesting phenomena were
observed. It is found that the contours and appearance of facial features,
especially eyes and moth, are the most significant facial attributes for facial
attractiveness prediction, which is also consistent with the visual perception
intuition of human.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02475</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02475</id><created>2015-11-08</created><authors><author><keyname>Hakobyan</keyname><forenames>Anush</forenames></author><author><keyname>Mkrtchyan</keyname><forenames>Vahan</forenames></author></authors><title>On Sylvester Colorings of Cubic Graphs</title><categories>math.CO cs.DM</categories><comments>15 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If $G$ and $H$ are two cubic graphs, then an $H$-coloring of $G$ is a proper
edge-coloring $f$ with edges of $H$, such that for each vertex $x$ of $G$,
there is a vertex $y$ of $H$ with $f(\partial_G(x))=\partial_H(y)$. If $G$
admits an $H$-coloring, then we will write $H\prec G$. The Petersen coloring
conjecture of Jaeger states that for any bridgeless cubic graph $G$, one has:
$P\prec G$. The second author has recently introduced the Sylvester coloring
conjecture, which states that for any cubic graph $G$ one has: $S\prec G$. Here
$S$ is the Sylvester graph on ten vertices. In this paper, we prove the
analogue of Sylvester coloring conjecture for cubic pseudo-graphs. Moreover, we
show that if $G$ is any connected simple cubic graph $G$ with $G\prec P$, then
$G = P$. This implies that the Petersen graph does not admit an
$S_{16}$-coloring, where $S_{16}$ is the Sylvester graph on $16$ vertices.
Finally, we show that any cubic graph $G$ has a coloring with edges of
Sylvester graph $S$ such that at least $\frac45$ of vertices of $G$ meet the
conditions of Sylvester coloring conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02476</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02476</id><created>2015-11-08</created><updated>2016-03-07</updated><authors><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author></authors><title>Statistical physics of inference: Thresholds and algorithms</title><categories>cond-mat.stat-mech cs.DS stat.ML</categories><comments>80 pages, review article based on HDR thesis of the first author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many questions of fundamental interest in todays science can be formulated as
inference problems: Some partial, or noisy, observations are performed over a
set of variables and the goal is to recover, or infer, the values of the
variables based on the indirect information contained in the measurements. For
such problems, the central scientific questions are: Under what conditions is
the information contained in the measurements sufficient for a satisfactory
inference to be possible? What are the most efficient algorithms for this task?
A growing body of work has shown that often we can understand and locate these
fundamental barriers by thinking of them as phase transitions in the sense of
statistical physics. Moreover, it turned out that we can use the gained
physical insight to develop new promising algorithms. Connection between
inference and statistical physics is currently witnessing an impressive
renaissance and we review here the current state-of-the-art, with a pedagogical
focus on the Ising model which formulated as an inference problem we call the
planted spin glass. In terms of applications we review two classes of problems:
(i) inference of clusters on graphs and networks, with community detection as a
special case and (ii) estimating a signal from its noisy linear measurements,
with compressed sensing as a case of sparse estimation. Our goal is to provide
a pedagogical review for researchers in physics and other fields interested in
this fascinating topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02484</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02484</id><created>2015-11-08</created><authors><author><keyname>Chestnut</keyname><forenames>Stephen R.</forenames></author><author><keyname>Zenklusen</keyname><forenames>Rico</forenames></author></authors><title>Interdicting Structured Combinatorial Optimization Problems with
  {0,1}-Objectives</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interdiction problems ask about the worst-case impact of a limited change to
an underlying optimization problem. They are a natural way to measure the
robustness of a system, or to identify its weakest spots. Interdiction problems
have been studied for a wide variety of classical combinatorial optimization
problems, including maximum $s$-$t$ flows, shortest $s$-$t$ paths, maximum
weight matchings, minimum spanning trees, maximum stable sets, and graph
connectivity. Most interdiction problems are NP-hard, and furthermore, even
designing efficient approximation algorithms that allow for estimating the
order of magnitude of a worst-case impact, has turned out to be very difficult.
Not very surprisingly, the few known approximation algorithms are heavily
tailored for specific problems.
  Inspired by an approach of Burch et al. (2003), we suggest a general method
to obtain pseudoapproximations for many interdiction problems. More precisely,
for any $\alpha&gt;0$, our algorithm will return either a
$(1+\alpha)$-approximation, or a solution that may overrun the interdiction
budget by a factor of at most $1+\alpha^{-1}$ but is also at least as good as
the optimal solution that respects the budget. Furthermore, our approach can
handle submodular interdiction costs when the underlying problem is to find a
maximum weight independent set in a matroid, as for example the maximum weight
forest problem. The approach can sometimes be refined by exploiting additional
structural properties of the underlying optimization problem to obtain stronger
results. We demonstrate this by presenting a PTAS for interdicting $b$-stable
sets in bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02486</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02486</id><created>2015-11-08</created><authors><author><keyname>Chestnut</keyname><forenames>Stephen R.</forenames></author><author><keyname>Zenklusen</keyname><forenames>Rico</forenames></author></authors><title>Hardness and Approximation for Network Flow Interdiction</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Network Flow Interdiction problem an adversary attacks a network in
order to minimize the maximum s-t-flow. Very little is known about the
approximatibility of this problem despite decades of interest in it. We present
the first approximation hardness, showing that Network Flow Interdiction and
several of its variants cannot be much easier to approximate than Densest
k-Subgraph. In particular, any $n^{o(1)}$-approximation algorithm for Network
Flow Interdiction would imply an $n^{o(1)}$-approximation algorithm for Densest
k-Subgraph. We complement this hardness results with the first approximation
algorithm for Network Flow Interdiction, which has approximation ratio 2(n-1).
We also show that Network Flow Interdiction is essentially the same as the
Budgeted Minimum s-t-Cut problem, and transferring our results gives the first
approximation hardness and algorithm for that problem, as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02490</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02490</id><created>2015-11-08</created><updated>2016-01-06</updated><authors><author><keyname>Cummins</keyname><forenames>Chris</forenames></author><author><keyname>Petoumenos</keyname><forenames>Pavlos</forenames></author><author><keyname>Steuwer</keyname><forenames>Michel</forenames></author><author><keyname>Leather</keyname><forenames>Hugh</forenames></author></authors><title>Autotuning OpenCL Workgroup Size for Stencil Patterns</title><categories>cs.DC</categories><comments>8 pages, 6 figures, presented at the 6th International Workshop on
  Adaptive Self-tuning Computing Systems (ADAPT '16)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selecting an appropriate workgroup size is critical for the performance of
OpenCL kernels, and requires knowledge of the underlying hardware, the data
being operated on, and the implementation of the kernel. This makes portable
performance of OpenCL programs a challenging goal, since simple heuristics and
statically chosen values fail to exploit the available performance. To address
this, we propose the use of machine learning-enabled autotuning to
automatically predict workgroup sizes for stencil patterns on CPUs and
multi-GPUs.
  We present three methodologies for predicting workgroup sizes. The first,
using classifiers to select the optimal workgroup size. The second and third
proposed methodologies employ the novel use of regressors for performing
classification by predicting the runtime of kernels and the relative
performance of different workgroup sizes, respectively. We evaluate the
effectiveness of each technique in an empirical study of 429 combinations of
architecture, kernel, and dataset, comparing an average of 629 different
workgroup sizes for each. We find that autotuning provides a median 3.79x
speedup over the best possible fixed workgroup size, achieving 94% of the
maximum performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02492</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02492</id><created>2015-11-08</created><authors><author><keyname>Habibian</keyname><forenames>Amirhossein</forenames></author><author><keyname>Mensink</keyname><forenames>Thomas</forenames></author><author><keyname>Snoek</keyname><forenames>Cees G. M.</forenames></author></authors><title>VideoStory Embeddings Recognize Events when Examples are Scarce</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims for event recognition when video examples are scarce or even
completely absent. The key in such a challenging setting is a semantic video
representation. Rather than building the representation from individual
attribute detectors and their annotations, we propose to learn the entire
representation from freely available web videos and their descriptions using an
embedding between video features and term vectors. In our proposed embedding,
which we call VideoStory, the correlations between the terms are utilized to
learn a more effective representation by optimizing a joint objective balancing
descriptiveness and predictability.We show how learning the VideoStory using a
multimodal predictability loss, including appearance, motion and audio
features, results in a better predictable representation. We also propose a
variant of VideoStory to recognize an event in video from just the important
terms in a text query by introducing a term sensitive descriptiveness loss. Our
experiments on three challenging collections of web videos from the NIST
TRECVID Multimedia Event Detection and Columbia Consumer Videos datasets
demonstrate: i) the advantages of VideoStory over representations using
attributes or alternative embeddings, ii) the benefit of fusing video
modalities by an embedding over common strategies, iii) the complementarity of
term sensitive descriptiveness and multimodal predictability for event
recognition without examples. By it abilities to improve predictability upon
any underlying video feature while at the same time maximizing semantic
descriptiveness, VideoStory leads to state-of-the-art accuracy for both few-
and zero-example recognition of events in video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02494</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02494</id><created>2015-11-08</created><updated>2016-01-10</updated><authors><author><keyname>Elafrou</keyname><forenames>Athena</forenames></author><author><keyname>Goumas</keyname><forenames>Georgios</forenames></author><author><keyname>Koziris</keyname><forenames>Nectarios</forenames></author></authors><title>A lightweight optimization selection method for Sparse Matrix-Vector
  Multiplication</title><categories>cs.PF</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an optimization selection methodology for the
ubiquitous sparse matrix-vector multiplication (SpMV) kernel. We propose two
models that attempt to identify the major performance bottleneck of the kernel
for every instance of the problem and then select an appropriate optimization
to tackle it. Our first model requires online profiling of the input matrix in
order to detect its most prevailing performance issue, while our second model
only uses comprehensive structural features of the sparse matrix. Our method
delivers high performance stability for SpMV across different platforms and
sparse matrices, due to its application and architecture awareness. Our
experimental results demonstrate that a) our approach is able to distinguish
and appropriately optimize special matrices in multicore platforms that fall
out of the standard class of memory bandwidth bound matrices, and b) lead to a
significant performance gain of 29% in a manycore platform compared to an
architecture-centric optimization, as a result of the successful selection of
the appropriate optimization for the great majority of the matrices. With a
runtime overhead equivalent to a couple dozen SpMV operations, our approach is
practical for use in iterative numerical solvers of real-life applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02498</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02498</id><created>2015-11-08</created><authors><author><keyname>Dash</keyname><forenames>Amar Ranjan</forenames></author><author><keyname>Sahu</keyname><forenames>Sandipta Kumar</forenames></author><author><keyname>Samantra</keyname><forenames>Sanjay Kumar</forenames></author><author><keyname>Sabat</keyname><forenames>Sradhanjali</forenames></author></authors><title>Characteristic specific prioritized dynamic average burst round robin
  scheduling for uniprocessor and multiprocessor environment</title><categories>cs.OS</categories><comments>20 Pages, 10 Figures, 18 Tables, 20 References, International Journal
  of Computer Science, Engineering and Applications (IJCSEA) Vol.5, No.4/5,
  October 2015</comments><doi>10.5121/ijcsea.2015.5501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CPU scheduling is one of the most crucial operations performed by operating
systems. Different conventional algorithms like FCFS, SJF, Priority, and RR
(Round Robin) are available for CPU Scheduling. The effectiveness of Priority
and Round Robin scheduling algorithm completely depends on selection of
priority features of processes and on the choice of time quantum. In this paper
a new CPU scheduling algorithm has been proposed, named as CSPDABRR
(Characteristic specific Prioritized Dynamic Average Burst Round Robin), that
uses seven priority features for calculating priority of processes and uses
dynamic time quantum instead of static time quantum used in RR. The performance
of the proposed algorithm is experimentally compared with traditional RR and
Priority scheduling algorithm in both uni-processor and multi-processor
environment. The results of our approach presented in this paper demonstrate
improved performance in terms of average waiting time, average turnaround time,
and optimal priority feature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02500</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02500</id><created>2015-11-08</created><authors><author><keyname>Rond</keyname><forenames>Arie</forenames></author><author><keyname>Giryes</keyname><forenames>Raja</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Poisson Inverse Problems by the Plug-and-Play scheme</title><categories>cs.CV math.OC</categories><msc-class>94A08, 68U10, 47N10, 49N45, 65J22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Anscombe transform offers an approximate conversion of a Poisson random
variable into a Gaussian one. This transform is important and appealing, as it
is easy to compute, and becomes handy in various inverse problems with Poisson
noise contamination. Solution to such problems can be done by first applying
the Anscombe transform, then applying a Gaussian-noise-oriented restoration
algorithm of choice, and finally applying an inverse Anscombe transform. The
appeal in this approach is due to the abundance of high-performance restoration
algorithms designed for white additive Gaussian noise (we will refer to these
hereafter as &quot;Gaussian-solvers&quot;). This process is known to work well for high
SNR images, where the Anscombe transform provides a rather accurate
approximation. When the noise level is high, the above path loses much of its
effectiveness, and the common practice is to replace it with a direct treatment
of the Poisson distribution. Naturally, with this we lose the ability to
leverage on vastly available Gaussian-solvers.
  In this work we suggest a novel method for coupling Gaussian denoising
algorithms to Poisson noisy inverse problems, which is based on a general
approach termed &quot;Plug-and-Play&quot;. Deploying the Plug-and-Play approach to such
problems leads to an iterative scheme that repeats several key steps: 1) A
convex programming task of simple form that can be easily treated; 2) A
powerful Gaussian denoising algorithm of choice; and 3) A simple update step.
  Such a modular method, just like the Anscombe transform, enables other
developers to plug their own Gaussian denoising algorithms to our scheme in an
easy way. While the proposed method bares some similarity to the Anscombe
operation, it is in fact based on a different mathematical basis, which holds
true for all SNR ranges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02503</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02503</id><created>2015-11-08</created><updated>2016-02-03</updated><authors><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Qiu</keyname><forenames>Mingquan</forenames></author><author><keyname>Zhu</keyname><forenames>Zhencai</forenames></author><author><keyname>Wu</keyname><forenames>Bo</forenames></author><author><keyname>Zhou</keyname><forenames>Gongbo</forenames></author></authors><title>Bearing fault diagnosis based on spectrum images of vibration signals</title><categories>cs.CV cs.SD</categories><journal-ref>Measurement Science and Technology, Volume 27, Number 3, 2016</journal-ref><doi>10.1088/0957-0233/27/3/035005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bearing fault diagnosis has been a challenge in the monitoring activities of
rotating machinery, and it's receiving more and more attention. The
conventional fault diagnosis methods usually extract features from the
waveforms or spectrums of vibration signals in order to realize fault
classification. In this paper, a novel feature in the form of images is
presented, namely the spectrum images of vibration signals. The spectrum images
are simply obtained by doing fast Fourier transformation. Such images are
processed with two-dimensional principal component analysis (2DPCA) to reduce
the dimensions, and then a minimum distance method is applied to classify the
faults of bearings. The effectiveness of the proposed method is verified with
experimental data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02505</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02505</id><created>2015-11-08</created><authors><author><keyname>Cesmelioglu</keyname><forenames>Ayca</forenames></author><author><keyname>Meidl</keyname><forenames>Wilfried</forenames></author><author><keyname>Pott</keyname><forenames>Alexander</forenames></author></authors><title>There are infinitely many bent functions for which the dual is not bent</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bent functions can be classified into regular bent functions, weakly regular
but not regular bent functions, and non-weakly regular bent functions. Regular
and weakly regular bent functions always appear in pairs since their duals are
also bent functions. In general this does not apply to non-weaky regular bent
functions. However, the first known construction of non-weakly regular bent
functions by Ce\c{s}melio\u{g}lu et {\it al.}, 2012, yields bent functions for
which the dual is also bent. In this paper the first construction of non-weakly
regular bent functions for which the dual is not bent is presented. We call
such functions non-dual-bent functions. Until now, only sporadic examples found
via computer search were known. We then show that with the direct sum of bent
functions and with the construction by Ce\c{s}melio\u{g}lu et {\it al.} one can
obtain infinitely many non-dual-bent functions once one example of a
non-dual-bent function is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02506</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02506</id><created>2015-11-08</created><authors><author><keyname>Liao</keyname><forenames>Yi-Hsiu</forenames></author><author><keyname>Lee</keyname><forenames>Hung-yi</forenames></author><author><keyname>Lee</keyname><forenames>Lin-shan</forenames></author></authors><title>Towards Structured Deep Neural Network for Automatic Speech Recognition</title><categories>cs.CL cs.LG cs.NE</categories><comments>arXiv admin note: text overlap with arXiv:1506.01163</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose the Structured Deep Neural Network (structured DNN)
as a structured and deep learning framework. This approach can learn to find
the best structured object (such as a label sequence) given a structured input
(such as a vector sequence) by globally considering the mapping relationships
between the structures rather than item by item.
  When automatic speech recognition is viewed as a special case of such a
structured learning problem, where we have the acoustic vector sequence as the
input and the phoneme label sequence as the output, it becomes possible to
comprehensively learn utterance by utterance as a whole, rather than frame by
frame.
  Structured Support Vector Machine (structured SVM) was proposed to perform
ASR with structured learning previously, but limited by the linear nature of
SVM. Here we propose structured DNN to use nonlinear transformations in
multi-layers as a structured and deep learning approach. This approach was
shown to beat structured SVM in preliminary experiments on TIMIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02511</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02511</id><created>2015-11-08</created><updated>2015-11-10</updated><authors><author><keyname>Lee</keyname><forenames>Jeffrey S.</forenames></author><author><keyname>Cleaver</keyname><forenames>Gerald B.</forenames></author></authors><title>The Cosmic Microwave Background Radiation Power Spectrum as a Random Bit
  Generator for Symmetric and Asymmetric-Key Cryptography</title><categories>cs.CR hep-th</categories><comments>9 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, the Cosmic Microwave Background (CMB) Radiation is shown to be
capable of functioning as a Random Bit Generator, and constitutes an
effectively infinite supply of truly random one-time pad values of arbitrary
length. It is further argued that the CMB power spectrum potentially conforms
to the FIPS 140-2 standard. Additionally, its applicability to the generation
of a (n x n) random key matrix for a Vernam cipher is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02512</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02512</id><created>2015-11-08</created><authors><author><keyname>Dorch</keyname><forenames>S. B. F.</forenames></author><author><keyname>Drachen</keyname><forenames>T. M.</forenames></author><author><keyname>Ellegaard</keyname><forenames>O.</forenames></author></authors><title>The data sharing advantage in astrophysics</title><categories>astro-ph.IM cs.DL</categories><comments>4 pages, 2 figures, Conference proceedings of Focus Meeting 3 on
  Scholarly Publication in Astronomy, IAU GA 2015, Honolulu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present here evidence for the existence of a citation advantage within
astrophysics for papers that link to data. Using simple measures based on
publication data from NASA Astrophysics Data System we find a citation
advantage for papers with links to data receiving on the average significantly
more citations per paper than papers without links to data. Furthermore, using
INSPEC and Web of Science databases we investigate whether either papers of an
experimental or theoretical nature display different citation behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02513</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02513</id><created>2015-11-08</created><authors><author><keyname>Bassily</keyname><forenames>Raef</forenames></author><author><keyname>Nissim</keyname><forenames>Kobbi</forenames></author><author><keyname>Smith</keyname><forenames>Adam</forenames></author><author><keyname>Steinke</keyname><forenames>Thomas</forenames></author><author><keyname>Stemmer</keyname><forenames>Uri</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>Algorithmic Stability for Adaptive Data Analysis</title><categories>cs.LG cs.CR cs.DS</categories><comments>This work unifies and subsumes the two arXiv manuscripts
  arXiv:1503.04843 and arXiv:1504.05800</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptivity is an important feature of data analysis---the choice of questions
to ask about a dataset often depends on previous interactions with the same
dataset. However, statistical validity is typically studied in a nonadaptive
model, where all questions are specified before the dataset is drawn. Recent
work by Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014) initiated
the formal study of this problem, and gave the first upper and lower bounds on
the achievable generalization error for adaptive data analysis.
  Specifically, suppose there is an unknown distribution $\mathbf{P}$ and a set
of $n$ independent samples $\mathbf{x}$ is drawn from $\mathbf{P}$. We seek an
algorithm that, given $\mathbf{x}$ as input, accurately answers a sequence of
adaptively chosen queries about the unknown distribution $\mathbf{P}$. How many
samples $n$ must we draw from the distribution, as a function of the type of
queries, the number of queries, and the desired level of accuracy?
  In this work we make two new contributions:
  (i) We give upper bounds on the number of samples $n$ that are needed to
answer statistical queries. The bounds improve and simplify the work of Dwork
et al. (STOC, 2015), and have been applied in subsequent work by those authors
(Science, 2015, NIPS, 2015).
  (ii) We prove the first upper bounds on the number of samples required to
answer more general families of queries. These include arbitrary
low-sensitivity queries and an important class of optimization queries.
  As in Dwork et al., our algorithms are based on a connection with algorithmic
stability in the form of differential privacy. We extend their work by giving a
quantitatively optimal, more general, and simpler proof of their main theorem
that stability implies low generalization error. We also study weaker stability
guarantees such as bounded KL divergence and total variation distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02525</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02525</id><created>2015-11-08</created><authors><author><keyname>Efrat</keyname><forenames>Alon</forenames></author><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Polishchuk</keyname><forenames>Valentin</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Improved Approximation Algorithms for Relay Placement</title><categories>cs.DS</categories><comments>1+29 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the relay placement problem the input is a set of sensors and a number $r
\ge 1$, the communication range of a relay. In the one-tier version of the
problem the objective is to place a minimum number of relays so that between
every pair of sensors there is a path through sensors and/or relays such that
the consecutive vertices of the path are within distance $r$ if both vertices
are relays and within distance 1 otherwise. The two-tier version adds the
restrictions that the path must go through relays, and not through sensors. We
present a 3.11-approximation algorithm for the one-tier version and a PTAS for
the two-tier version. We also show that the one-tier version admits no PTAS,
assuming P $\ne$ NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02528</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02528</id><created>2015-11-08</created><authors><author><keyname>van Glabbeek</keyname><forenames>Rob</forenames></author><author><keyname>Groote</keyname><forenames>Jan Friso</forenames></author><author><keyname>H&#xf6;fner</keyname><forenames>Peter</forenames></author></authors><title>Proceedings Workshop on Models for Formal Analysis of Real Systems</title><categories>cs.LO cs.CR cs.OS cs.SY</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 196, 2015</journal-ref><doi>10.4204/EPTCS.196</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of MARS 2015, the first workshop on
Models for Formal Analysis of Real Systems, held on November 23, 2015 in Suva,
Fiji, as an affiliated workshop of LPAR 2015, the 20th International Conference
on Logic for Programming, Artificial Intelligence and Reasoning.
  The workshop emphasises modelling over verification. It aims at discussing
the lessons learned from making formal methods for the verification and
analysis of realistic systems. Examples are:
  (1) Which formalism is chosen, and why?
  (2) Which abstractions have to be made and why?
  (3) How are important characteristics of the system modelled?
  (4) Were there any complications while modelling the system?
  (5) Which measures were taken to guarantee the accuracy of the model?
  We invited papers that present full models of real systems, which may lay the
basis for future comparison and analysis. An aim of the workshop is to present
different modelling approaches and discuss pros and cons for each of them.
Alternative formal descriptions of the systems presented at this workshop are
encouraged, which should foster the development of improved specification
formalisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02529</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02529</id><created>2015-11-08</created><updated>2015-12-15</updated><authors><author><keyname>Cervesato</keyname><forenames>Iliano</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Sch&#xfc;rmann</keyname><forenames>Carsten</forenames><affiliation>IT University of Copenhagen</affiliation></author></authors><title>Proceedings First International Workshop on Focusing</title><categories>cs.LO cs.PL</categories><proxy>Selena Clancy</proxy><acm-class>F.4.1; D.1.6; I.2.3</acm-class><journal-ref>EPTCS 197, 2015</journal-ref><doi>10.4204/EPTCS.197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume constitutes the proceedings of WoF'15, the First International
Workshop on Focusing, held on November 23rd, 2015 in Suva, Fiji. The workshop
was a half-day satellite event of LPAR-20, the 20th International Conferences
on Logic for Programming, Artificial Intelligence and Reasoning.
  The program committee selected four papers for presentation at WoF'15, and
inclusion in this volume. In addition, the program included an invited talk by
Elaine Pimentel.
  Focusing is a proof search strategy that alternates two phases: an inversion
phase where invertible sequent rules are applied exhaustively and a chaining
phase where it selects a formula and decomposes it maximally using
non-invertible rules. Focusing is one of the most exciting recent developments
in computational logic: it is complete for many logics of interest and provides
a foundation for their use as programming languages and rewriting calculi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02537</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02537</id><created>2015-11-08</created><authors><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Kleinberg</keyname><forenames>Bobby</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author><author><keyname>Zadomighaddam</keyname><forenames>Morteza</forenames></author></authors><title>Exponential Segregation in a Two-Dimensional Schelling Model with
  Tolerant Individuals</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the two-dimensional Schelling segregation model yields
monochromatic regions of size exponential in the area of individuals'
neighborhoods, provided that the tolerance parameter is a constant strictly
less than 1/2 but sufficiently close to it. Our analysis makes use of a
connection with the first-passage percolation model from the theory of
stochastic processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02540</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02540</id><created>2015-11-08</created><authors><author><keyname>Mass&#xe9;</keyname><forenames>Pierre-Yves</forenames></author><author><keyname>Ollivier</keyname><forenames>Yann</forenames></author></authors><title>Speed learning on the fly</title><categories>math.OC cs.LG stat.ML</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The practical performance of online stochastic gradient descent algorithms is
highly dependent on the chosen step size, which must be tediously hand-tuned in
many applications. The same is true for more advanced variants of stochastic
gradients, such as SAGA, SVRG, or AdaGrad. Here we propose to adapt the step
size by performing a gradient descent on the step size itself, viewing the
whole performance of the learning trajectory as a function of step size.
Importantly, this adaptation can be computed online at little cost, without
having to iterate backward passes over the full data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02543</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02543</id><created>2015-11-08</created><authors><author><keyname>Grosse</keyname><forenames>Roger B.</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Sandwiching the marginal likelihood using bidirectional Monte Carlo</title><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the marginal likelihood (ML) of a model requires marginalizing out
all of the parameters and latent variables, a difficult high-dimensional
summation or integration problem. To make matters worse, it is often hard to
measure the accuracy of one's ML estimates. We present bidirectional Monte
Carlo, a technique for obtaining accurate log-ML estimates on data simulated
from a model. This method obtains stochastic lower bounds on the log-ML using
annealed importance sampling or sequential Monte Carlo, and obtains stochastic
upper bounds by running these same algorithms in reverse starting from an exact
posterior sample. The true value can be sandwiched between these two stochastic
bounds with high probability. Using the ground truth log-ML estimates obtained
from our method, we quantitatively evaluate a wide variety of existing ML
estimators on several latent variable models: clustering, a low rank
approximation, and a binary attributes model. These experiments yield insights
into how to accurately estimate marginal likelihoods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02547</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02547</id><created>2015-11-08</created><authors><author><keyname>Singh</keyname><forenames>Sumeet</forenames></author><author><keyname>Schmerling</keyname><forenames>Edward</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Decentralized Algorithms for 3D Symmetric Formations in Robotic
  Networks: a Contraction Theory Approach</title><categories>cs.SY cs.MA cs.RO</categories><comments>Submitted to IEEE Transactions in Robotics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents decentralized algorithms for formation control of
multiple robots in three dimensions. Specifically, we leverage the mathematical
properties of cyclic pursuit along with results from contraction and partial
contraction theory to design decentralized control algorithms that ensure
global convergence to symmetric formations. We first consider regular polygon
formations as a base case, and then extend the results to Johnson solid and
other polygonal mesh formations. The algorithms are further augmented to allow
control over formation size and avoid collisions with other robots in the
formation. The robustness properties of the algorithms are assessed in the
presence of bounded additive disturbances and their effect on the quality of
the formation is quantified. Finally, we present a general methodology for
embedding the control laws on complex dynamical systems, in this case,
quadcopters, and validate this approach via simulations and experiments on a
fleet of quadcopters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02548</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02548</id><created>2015-11-08</created><authors><author><keyname>Amini</keyname><forenames>M. Hadi</forenames></author><author><keyname>Jaddivada</keyname><forenames>R.</forenames></author><author><keyname>Mishra</keyname><forenames>S.</forenames></author><author><keyname>Karabasoglu</keyname><forenames>O.</forenames></author></authors><title>Distributed Security Constrained Economic Dispatch</title><categories>cs.SY cs.DC math.OC</categories><comments>6 pages, 8 figures, IEEE Innovative Smart Grid Technologies
  Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate two decomposition methods for their convergence
rate which are used to solve security constrained economic dispatch (SCED): 1)
Lagrangian Relaxation (LR), and 2) Augmented Lagrangian Relaxation (ALR).
First, the centralized SCED problem is posed for a 6-bus test network and then
it is decomposed into subproblems using both of the methods. In order to model
the tie-line between decomposed areas of the test network, a novel method is
proposed. The advantages and drawbacks of each method are discussed in terms of
accuracy and information privacy. We show that there is a tradeoff between the
information privacy and the convergence rate. It has been found that ALR
converges faster compared to LR, due to the large amount of shared data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02554</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02554</id><created>2015-11-08</created><updated>2016-01-16</updated><authors><author><keyname>Pouladi</keyname><forenames>Farhad</forenames></author><author><keyname>Salehinejad</keyname><forenames>Hojjat</forenames></author><author><keyname>Gilani</keyname><forenames>Amir Mohammad</forenames></author></authors><title>Deep Recurrent Neural Networks for Sequential Phenotype Prediction in
  Genomics</title><categories>cs.NE cs.CE cs.LG</categories><comments>The articles is accepted at DeSE 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In analyzing of modern biological data, we are often dealing with ill-posed
problems and missing data, mostly due to high dimensionality and
multicollinearity of the dataset. In this paper, we have proposed a system
based on matrix factorization (MF) and deep recurrent neural networks (DRNNs)
for genotype imputation and phenotype sequences prediction. In order to model
the long-term dependencies of phenotype data, the new Recurrent Linear Units
(ReLU) learning strategy is utilized for the first time. The proposed model is
implemented for parallel processing on central processing units (CPUs) and
graphic processing units (GPUs). Performance of the proposed model is compared
with other training algorithms for learning long-term dependencies as well as
the sparse partial least square (SPLS) method on a set of genotype and
phenotype data with 604 samples, 1980 single-nucleotide polymorphisms (SNPs),
and two traits. The results demonstrate performance of the ReLU training
algorithm in learning long-term dependencies in RNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02556</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02556</id><created>2015-11-08</created><authors><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Castanon</keyname><forenames>Jorge A.</forenames></author></authors><title>Sentiment Expression via Emoticons on Social Media</title><categories>cs.CL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emoticons (e.g., :) and :( ) have been widely used in sentiment analysis and
other NLP tasks as features to ma- chine learning algorithms or as entries of
sentiment lexicons. In this paper, we argue that while emoticons are strong and
common signals of sentiment expression on social media the relationship between
emoticons and sentiment polarity are not always clear. Thus, any algorithm that
deals with sentiment polarity should take emoticons into account but extreme
cau- tion should be exercised in which emoticons to depend on. First, to
demonstrate the prevalence of emoticons on social media, we analyzed the
frequency of emoticons in a large re- cent Twitter data set. Then we carried
out four analyses to examine the relationship between emoticons and sentiment
polarity as well as the contexts in which emoticons are used. The first
analysis surveyed a group of participants for their perceived sentiment
polarity of the most frequent emoticons. The second analysis examined
clustering of words and emoti- cons to better understand the meaning conveyed
by the emoti- cons. The third analysis compared the sentiment polarity of
microblog posts before and after emoticons were removed from the text. The last
analysis tested the hypothesis that removing emoticons from text hurts
sentiment classification by training two machine learning models with and
without emoticons in the text respectively. The results confirms the arguments
that: 1) a few emoticons are strong and reliable signals of sentiment polarity
and one should take advantage of them in any senti- ment analysis; 2) a large
group of the emoticons conveys com- plicated sentiment hence they should be
treated with extreme caution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02562</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02562</id><created>2015-11-08</created><authors><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author><author><keyname>Dong</keyname><forenames>Yuxiao</forenames></author><author><keyname>Mei</keyname><forenames>Qiaozhu</forenames></author><author><keyname>Johnson</keyname><forenames>Reid A.</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>Modeling the Interplay Between Individual Behavior and Network
  Distributions</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that many networks follow a power-law degree distribution;
however, the factors that influence the formation of their distributions are
still unclear. How can one model the connection between individual actions and
network distributions? How can one explain the formation of group phenomena and
their evolutionary patterns?
  In this paper, we propose a unified framework, M3D, to model human dynamics
in social networks from three perspectives: macro, meso, and micro. At the
micro-level, we seek to capture the way in which an individual user decides
whether to perform an action. At the meso-level, we study how group behavior
develops and evolves over time, based on individual actions. At the
macro-level, we try to understand how network distributions such as power-law
(or heavy-tailed phenomena) can be explained by group behavior. We provide
theoretical analysis for the proposed framework, and discuss the connection of
our framework with existing work.
  The framework offers a new, flexible way to explain the interplay between
individual user actions and network distributions, and can benefit many
applications. To model heavy-tailed distributions from partially observed
individual actions and to predict the formation of group behaviors, we apply
M3D to three different genres of networks: Tencent Weibo, Citation, and Flickr.
We also use information-burst prediction as a particular application to
quantitatively evaluate the predictive power of the proposed framework. Our
results on the Weibo indicate that M3D's prediction performance exceeds that of
several alternative methods by up to 30\%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02564</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02564</id><created>2015-11-08</created><authors><author><keyname>Noseevich</keyname><forenames>George</forenames></author><author><keyname>Gamayunov</keyname><forenames>Dennis</forenames></author></authors><title>Towards automated web application logic reconstruction for application
  level security</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern overlay security mechanisms like Web Application Firewalls (WAF)
suffer from inability to recognize custom high-level application logic and data
objects, which results in low accuracy, high false positives rates, and
overhelming manual effort for fine tuning. In this paper we propose an approach
to web application modeling for security purposes that could help
next-generation WAFs to adapt to specific web applications, and do it
automatically whenever possible. We aim at creating multi-layer models that
adequately simulate various aspects of web application functionality that are
significant for intrusion detection and prevention, including request parsing
and routing, reconstruction of actions and data objects, and action
interdependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02566</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02566</id><created>2015-11-08</created><authors><author><keyname>Rashidi</keyname><forenames>Zeynab</forenames></author></authors><title>Properties of Relationships among objects in Object-Oriented Software
  Design</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the modern paradigms to develop a system is object oriented analysis
and design. In this paradigm, there are several objects and each object plays
some specific roles. After identifying objects, the various relationships among
objects must be identified. This paper makes a literature review over
relationships among objects. Mainly, the relationships are three basic types,
including generalization/specialization, aggregation and association.This paper
presents five taxonomies for properties of the relationships. The first
taxonomy is based on temporal view. The second taxonomy is based on structure
and the third one relies on behavioral. The fourth taxonomy is specified on
mathematical view and fifth one related to the interface. Additionally, the
properties of the relationships are evaluated in a case study and several
recommendations are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02570</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02570</id><created>2015-11-09</created><updated>2015-11-11</updated><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Wu</keyname><forenames>Qi</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Dick</keyname><forenames>Anthony</forenames></author></authors><title>Explicit Knowledge-based Reasoning for Visual Question Answering</title><categories>cs.CV cs.CL</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for visual question answering which is capable of
reasoning about contents of an image on the basis of information extracted from
a large-scale knowledge base. The method not only answers natural language
questions using concepts not contained in the image, but can provide an
explanation of the reasoning by which it developed its answer. The method is
capable of answering far more complex questions than the predominant long
short-term memory-based approach, and outperforms it significantly in the
testing. We also provide a dataset and a protocol by which to evaluate such
methods, thus addressing one of the key issues in general visual ques- tion
answering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02574</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02574</id><created>2015-11-09</created><authors><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Ji</keyname><forenames>Mingyue</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author></authors><title>Wireless Multihop Device-to-Device Caching Networks</title><categories>cs.IT math.IT</categories><comments>21 pages, 5 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless device-to-device (D2D) network where $n$ nodes are
uniformly distributed at random over the network area. We let each node with
storage capacity $M$ cache files from a library of size $m \geq M$. Each node
in the network requests a file from the library independently at random,
according to a popularity distribution, and is served by other nodes having the
requested file in their local cache via (possibly) multihop transmissions.
Under the classical &quot;protocol model&quot; of wireless networks, we characterize the
optimal per-node capacity scaling law for a broad class of heavy-tailed
popularity distributions including Zipf distributions with exponent less than
one. In the parameter regimes of interest, we show that a decentralized random
caching strategy with uniform probability over the library yields the optimal
per-node capacity scaling of $\Theta(\sqrt{M/m})$, which is constant with $n$,
thus yielding throughput scalability with the network size. Furthermore, the
multihop capacity scaling can be significantly better than for the case of
single-hop caching networks, for which the per-node capacity is $\Theta(M/m)$.
The multihop capacity scaling law can be further improved for a Zipf
distribution with exponent larger than some threshold $&gt; 1$, by using a
decentralized random caching uniformly across a subset of most popular files in
the library. Namely, ignoring a subset of less popular files (i.e., effectively
reducing the size of the library) can significantly improve the throughput
scaling while guaranteeing that all nodes will be served with high probability
as $n$ increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02575</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02575</id><created>2015-11-09</created><authors><author><keyname>Ginosar</keyname><forenames>Shiry</forenames></author><author><keyname>Rakelly</keyname><forenames>Kate</forenames></author><author><keyname>Sachs</keyname><forenames>Sarah</forenames></author><author><keyname>Yin</keyname><forenames>Brian</forenames></author><author><keyname>Efros</keyname><forenames>Alexei A.</forenames></author></authors><title>A Century of Portraits: A Visual Historical Record of American High
  School Yearbooks</title><categories>cs.CV</categories><comments>ICCV 2015 Extreme Imaging Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many details about our world are not captured in written records because they
are too mundane or too abstract to describe in words. Fortunately, since the
invention of the camera, an ever-increasing number of photographs capture much
of this otherwise lost information. This plethora of artifacts documenting our
&quot;visual culture&quot; is a treasure trove of knowledge as yet untapped by
historians. We present a dataset of 37,921 frontal-facing American high school
yearbook photos that allow us to use computation to glimpse into the historical
visual record too voluminous to be evaluated manually. The collected portraits
provide a constant visual frame of reference with varying content. We can
therefore use them to consider issues such as a decade's defining style
elements, or trends in fashion and social norms over time. We demonstrate that
our historical image dataset may be used together with weakly-supervised
data-driven techniques to perform scalable historical analysis of large image
corpora with minimal human effort, much in the same way that large text corpora
together with natural language processing revolutionized historians' workflow.
Furthermore, we demonstrate the use of our dataset in dating grayscale
portraits using deep learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02580</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02580</id><created>2015-11-09</created><authors><author><keyname>Lin</keyname><forenames>Zhouhan</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author><author><keyname>Konda</keyname><forenames>Kishore</forenames></author></authors><title>How far can we go without convolution: Improving fully-connected
  networks</title><categories>cs.LG cs.NE</categories><comments>10 pages, 11 figures, submitted for ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose ways to improve the performance of fully connected networks. We
found that two approaches in particular have a strong effect on performance:
linear bottleneck layers and unsupervised pre-training using autoencoders
without hidden unit biases. We show how both approaches can be related to
improving gradient flow and reducing sparsity in the network. We show that a
fully connected network can yield approximately 70% classification accuracy on
the permutation-invariant CIFAR-10 task, which is much higher than the current
state-of-the-art. By adding deformations to the training data, the fully
connected network achieves 78% accuracy, which is just 10% short of a decent
convolutional network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02583</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02583</id><created>2015-11-09</created><authors><author><keyname>Chang</keyname><forenames>Jia-Ren</forenames></author><author><keyname>Chen</keyname><forenames>Yong-Sheng</forenames></author></authors><title>Batch-normalized Maxout Network in Network</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports a novel deep architecture referred to as Maxout network In
Network (MIN), which can enhance model discriminability and facilitate the
process of information abstraction within the receptive field. The proposed
network adopts the framework of the recently developed Network In Network
structure, which slides a universal approximator, multilayer perceptron (MLP)
with rectifier units, to exact features. Instead of MLP, we employ maxout MLP
to learn a variety of piecewise linear activation functions and to mediate the
problem of vanishing gradients that can occur when using rectifier units.
Moreover, batch normalization is applied to reduce the saturation of maxout
units by pre-conditioning the model and dropout is applied to prevent
overfitting. Finally, average pooling is used in all pooling layers to
regularize maxout MLP in order to facilitate information abstraction in every
receptive field while tolerating the change of object position. Because average
pooling preserves all features in the local patch, the proposed MIN model can
enforce the suppression of irrelevant information during training. Our
experiments demonstrated the state-of-the-art classification performance when
the MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and
comparable performance for SVHN dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02586</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02586</id><created>2015-11-09</created><authors><author><keyname>Xie</keyname><forenames>Cong</forenames></author><author><keyname>Li</keyname><forenames>Wu-Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>S-PowerGraph: Streaming Graph Partitioning for Natural Graphs by
  Vertex-Cut</title><categories>cs.SI cs.DC</categories><acm-class>A.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One standard solution for analyzing large natural graphs is to adopt
distributed computation on clusters. In distributed computation, graph
partitioning (GP) methods assign the vertices or edges of a graph to different
machines in a balanced way so that some distributed algorithms can be adapted
for. Most of traditional GP methods are offline, which means that the whole
graph has been observed before partitioning. However, the offline methods often
incur high computation cost. Hence, streaming graph partitioning (SGP) methods,
which can partition graphs in an online way, have recently attracted great
attention in distributed computation. There exist two typical GP strategies:
edge-cut and vertex-cut. Most SGP methods adopt edge-cut, but few vertex-cut
methods have been proposed for SGP. However, the vertex-cut strategy would be a
better choice than the edge-cut strategy because the degree of a natural graph
in general follows a highly skewed power-law distribution. Thus, we propose a
novel method, called S-PowerGraph, for SGP of natural graphs by vertex-cut. Our
S-PowerGraph method is simple but effective. Experiments on several large
natural graphs and synthetic graphs show that our S-PowerGraph can outperform
the state-of-the-art baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02589</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02589</id><created>2015-11-09</created><authors><author><keyname>Pachoulakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Spanaki</keyname><forenames>Cleanthe</forenames></author></authors><title>Parkinson's disease patient rehabilitation using gaming platforms:
  lessons learnt</title><categories>cs.CY cs.CV</categories><comments>12 pages, 3 figures</comments><acm-class>I.6.3; I.6.8</acm-class><doi>10.5121/ijbes.2015.2401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parkinson's disease (PD) is a progressive neurodegenerative movement disorder
where motor dysfunction gradually increases as the disease progress. In
addition to administering dopaminergic PD-specific drugs, attending
neurologists strongly recommend regular exercise combined with physiotherapy.
However, because of the long-term nature of the disease, patients following
traditional rehabilitation programs may get bored, lose interest and eventually
drop out as a direct result of the repeatability and predictability of the
prescribed exercises. Technology supported opportunities to liven up a daily
exercise schedule have appeared in the form of character-based, virtual reality
games which promote physical training in a non-linear and looser fashion and
provide an experience that varies from one game loop the next. Such
&quot;exergames&quot;, a word that results from the amalgamation of the words &quot;exercise&quot;
and &quot;game&quot; challenge patients into performing movements of varying complexity
in a playful and immersive virtual environment. Today's game consoles such as
Nintendo's Wii, Sony PlayStation Eye and Microsoft's Kinect sensor present new
opportunities to infuse motivation and variety to an otherwise mundane
physiotherapy routine. In this paper we present some of these approaches,
discuss their suitability for these PD patients, mainly on the basis of demands
made on balance, agility and gesture precision, and present design principles
that exergame platforms must comply with in order to be suitable for PD
patients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02590</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02590</id><created>2015-11-09</created><authors><author><keyname>Johanson</keyname><forenames>Mathias</forenames></author></authors><title>The Turing Test for Telepresence</title><categories>cs.HC</categories><comments>The International Journal of Multimedia and its Applications (IJMA),
  Vol.7, No.4/5, October 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of high-end videoconferencing systems has improved significantly
over the last few years enabling a class of applications known as
&quot;telepresence&quot; wherein the users engaged in a communication session experience
a feeling of mutual presence in a shared virtual space. Telepresence systems
have reached a maturity level that seriously challenges the old familiar truism
that a face-to-face meeting is always better than a technology-mediated
alternative. To explore the state of the art in telepresence technology and
outline future opportunities, this paper proposes an optimality condition,
expressed as a &quot;Turing Test,&quot; whereby the subjective experience of using a
telepresence system is compared to the corresponding face-to-face situation.
The requirements and challenges of designing a system passing such a Turing
Test for telepresence are analyzed with respect to the limits of human
perception, and the feasibility of achieving this goal with currently available
or near future technology is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02591</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02591</id><created>2015-11-09</created><updated>2016-03-06</updated><authors><author><keyname>Krithika</keyname><forenames>R.</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author></authors><title>Faster Randomized Branching Algorithms for $r$-SAT</title><categories>cs.DS</categories><comments>This paper has been withdrawn due to a gap in the algorithm analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of determining if an $r$-CNF boolean formula $F$ over $n$
variables is satisifiable reduces to the problem of determining if $F$ has a
satisfying assignment with a Hamming distance of at most $d$ from a fixed
assignment $\alpha$. This problem is also a very important subproblem in
Schoning's local search algorithm for $r$-SAT. While Schoning described a
randomized algorithm solves this subproblem in $O((r-1)^d)$ time, Dantsin et
al. presented a deterministic branching algorithm with $O^*(r^d)$ running time.
In this paper we present a simple randomized branching algorithm that runs in
time $O^*({(\frac{r+1}{2})}^d)$. As a consequence we get a randomized algorithm
for $r$-SAT that runs in $O^*({(\frac{2(r+1)}{r+3})}^n)$ time. This algorithm
matches the running time of Schoning's algorithm for 3-SAT and is an
improvement over Schoning's algorithm for all $r \geq 4$.
  For $r$-uniform hitting set parameterized by solution size $k$, we describe a
randomized FPT algorithm with a running time of $O^*({(\frac{r+1}{2})}^k)$. For
the above LP guarantee parameterization of vertex cover, we have a randomized
FPT algorithm to find a vertex cover of size $k$ in a running time of
$O^*(2.25^{k-vc^*})$, where $vc^*$ is the LP optimum of the natural LP
relaxation of vertex cover. In both the cases, these randomized algorithms have
a better running time than the current best deterministic algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02592</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02592</id><created>2015-11-09</created><authors><author><keyname>Gong</keyname><forenames>Bo</forenames></author><author><keyname>Qin</keyname><forenames>Qibo</forenames></author><author><keyname>Ren</keyname><forenames>Xiang</forenames></author><author><keyname>Gui</keyname><forenames>Lin</forenames></author><author><keyname>Luo</keyname><forenames>Hanwen</forenames></author><author><keyname>Chen</keyname><forenames>Wen</forenames></author></authors><title>Distributed Compressive Sensing Based Doubly Selective Channel
  Estimation for Large-Scale MIMO Systems</title><categories>cs.IT math.IT</categories><comments>conference,7 pages,5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Doubly selective (DS) channel estimation in largescale multiple-input
multiple-output (MIMO) systems is a challenging problem due to the requirement
of unaffordable pilot overheads and prohibitive complexity. In this paper, we
propose a novel distributed compressive sensing (DCS) based channel estimation
scheme to solve this problem. In the scheme, we introduce the basis expansion
model (BEM) to reduce the required channel coefficients and pilot overheads.
And due to the common sparsity of all the transmit-receive antenna pairs in
delay domain, we estimate the BEM coefficients by considering the DCS
framework, which has a simple linear structure with low complexity. Further
more, a linear smoothing method is proposed to improve the estimation accuracy.
Finally, we conduct various simulations to verify the validity of the proposed
scheme and demonstrate the performance gains of the proposed scheme compared
with conventional schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02595</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02595</id><created>2015-11-09</created><authors><author><keyname>Xie</keyname><forenames>Cong</forenames></author><author><keyname>Li</keyname><forenames>Wu-Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>A New Relaxation Approach to Normalized Hypergraph Cut</title><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Normalized graph cut (NGC) has become a popular research topic due to its
wide applications in a large variety of areas like machine learning and very
large scale integration (VLSI) circuit design. Most of traditional NGC methods
are based on pairwise relationships (similarities). However, in real-world
applications relationships among the vertices (objects) may be more complex
than pairwise, which are typically represented as hyperedges in hypergraphs.
Thus, normalized hypergraph cut (NHC) has attracted more and more attention.
Existing NHC methods cannot achieve satisfactory performance in real
applications. In this paper, we propose a novel relaxation approach, which is
called relaxed NHC (RNHC), to solve the NHC problem. Our model is defined as an
optimization problem on the Stiefel manifold. To solve this problem, we resort
to the Cayley transformation to devise a feasible learning algorithm.
Experimental results on a set of large hypergraph benchmarks for clustering and
partitioning in VLSI domain show that RNHC can outperform the state-of-the-art
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02597</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02597</id><created>2015-11-09</created><authors><author><keyname>Safina</keyname><forenames>Larisa</forenames></author><author><keyname>Mazzara</keyname><forenames>Manuel</forenames></author><author><keyname>Montesi</keyname><forenames>Fabrizio</forenames></author></authors><title>Data-driven Workflows for Microservices</title><categories>cs.PL</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microservices is an architectural style inspired by service-oriented
computing that has recently started gaining popularity. Jolie is a programming
language based on the microservices paradigm: the main building block of Jolie
systems are services, in contrast to, e.g., functions or objects. The
primitives offered by the Jolie language elicit many of the recurring patterns
found in microservices, like load balancers and structured processes. However,
Jolie still lacks some useful constructs for dealing with message types and
data manipulation that are present in service-oriented computing. In this
paper, we focus on the possibility of expressing choices at the level of data
types, a feature well represented in standards for Web Services, e.g., WSDL. We
extend Jolie to support such type choices and show the impact of our
implementation on some of the typical scenarios found in microservice systems.
This shows how computation can move from a process-driven to a data-driven
approach, and leads to the preliminary identification of recurring
communication patterns that can be shaped as design patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02599</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02599</id><created>2015-11-09</created><updated>2015-12-20</updated><authors><author><keyname>Segal-Halevi</keyname><forenames>Erel</forenames></author><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Aumann</keyname><forenames>Yonatan</forenames></author></authors><title>Waste Makes Haste: Bounded Time Protocols for Envy-Free Cake Cutting
  with Free Disposal</title><categories>cs.DS cs.GT</categories><comments>The first version was presented at AAMAS 2015:
  http://dl.acm.org/citation.cfm?id=2773268 . The current version is
  substantially revised and extended</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classic problem of envy-free division of a heterogeneous good
(&quot;cake&quot;) among several agents. It is known that, when the allotted pieces must
be connected, the problem cannot be solved by a finite algorithm for 3 or more
agents. Even when the pieces may be disconnected, no bounded-time algorithm is
known for 5 or more agents. The impossibility result, however, assumes that the
entire cake must be allocated. In this paper we replace the entire-allocation
requirement with a weaker \emph{partial-proportionality} requirement: the piece
given to each agent must be worth for it at least a certain positive fraction
of the entire cake value. We prove that this version of the problem is solvable
in bounded time even when the pieces must be connected. We present bounded-time
envy-free cake-cutting algorithms for: (1) giving each of $n$ agents a
connected piece with a positive value; (2) giving each of 3 agents a connected
piece worth at least 1/3; (3) giving each of 4 agents a connected piece worth
at least 1/7; (4) giving each of 4 agents a disconnected piece worth at least
1/4; (5) giving each of $n$ agents a disconnected piece worth at least
$(1-\epsilon)/n$ for any positive $\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02603</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02603</id><created>2015-11-09</created><updated>2016-01-06</updated><authors><author><keyname>Mpeis</keyname><forenames>Paschalis</forenames></author><author><keyname>Petoumenos</keyname><forenames>Pavlos</forenames></author><author><keyname>Leather</keyname><forenames>Hugh</forenames></author></authors><title>Iterative compilation on mobile devices</title><categories>cs.PL</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The abundance of poorly optimized mobile applications coupled with their
increasing centrality in our digital lives make a framework for mobile app
optimization an imperative. While tuning strategies for desktop and server
applications have a long history, it is difficult to adapt them for use on
mobile phones.
  Reference inputs which trigger behavior similar to a mobile application's
typical are hard to construct. For many classes of applications the very
concept of typical behavior is nonexistent, each user interacting with the
application in very different ways. In contexts like this, optimization
strategies need to evaluate their effectiveness against real user input, but
doing so online runs the risk of user dissatisfaction when suboptimal
optimizations are evaluated.
  In this paper we present an iterative compiler which employs a novel capture
and replay technique in order to collect real user input and use it later to
evaluate different transformations offline. The proposed mechanism identifies
and stores only the set of memory pages needed to replay the most heavily used
functions of the application. At idle periods, this minimal state is combined
with different binaries of the application, each one build with different
optimizations enabled. Replaying the targeted functions allows us to evaluate
the effectiveness of each set of optimizations for the actual way the user
interacts with the application.
  For the BEEBS benchmark suite, our approach was able to improve performance
by up to 57%, while keeping the slowdown experienced by the user on average at
0.8%. By focusing only on heavily used functions, we are able to conserve
storage space by between two and three orders of magnitude compared to typical
capture and replay implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02612</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02612</id><created>2015-11-09</created><authors><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Karczmarz</keyname><forenames>Adam</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>&#x141;&#x105;cki</keyname><forenames>Jakub</forenames></author><author><keyname>Sankowski</keyname><forenames>Piotr</forenames></author></authors><title>Optimal Dynamic Strings</title><categories>cs.DS</categories><msc-class>68P05, 68W32</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the fundamental problem of maintaining a dynamic
collection of strings under the following operations: concat - concatenates two
strings, split - splits a string into two at a given position, compare - finds
the lexicographical order (less, equal, greater) between two strings, LCP -
calculates the longest common prefix of two strings. We present an efficient
data structure for this problem, where an update requires only $O(\log n)$
worst-case time with high probability, with $n$ being the total length of all
strings in the collection, and a query takes constant worst-case time. On the
lower bound side, we prove that even if the only possible query is checking
equality of two strings, either updates or queries take amortized $\Omega(\log
n)$ time; hence our implementation is optimal.
  Such operations can be used as a basic building block to solve other string
problems. We provide two examples. First, we can augment our data structure to
provide pattern matching queries that may locate occurrences of a specified
pattern $p$ in the strings in our collection in optimal $O(|p|)$ time, at the
expense of increasing update time to $O(\log^2 n)$. Second, we show how to
maintain a history of an edited text, processing updates in $O(\log t \log \log
t)$ time, where $t$ is the number of edits, and support pattern matching
queries against the whole history in $O(|p| \log t \log \log t)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02615</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02615</id><created>2015-11-09</created><updated>2015-12-29</updated><authors><author><keyname>Daca</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Gupta</keyname><forenames>Ashutosh</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author></authors><title>Abstraction-driven Concolic Testing</title><categories>cs.LO</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concolic testing is a promising method for generating test suites for large
programs. However, it suffers from the path-explosion problem and often fails
to find tests that cover difficult-to-reach parts of programs. In contrast,
model checkers based on counterexample-guided abstraction refinement explore
programs exhaustively, while failing to scale on large programs with precision.
In this paper, we present a novel method that iteratively combines concolic
testing and model checking to find a test suite for a given coverage criterion.
If concolic testing fails to cover some test goals, then the model checker
refines its program abstraction to prove more paths infeasible, which reduces
the search space for concolic testing. We have implemented our method on top of
the concolic-testing tool CREST and the model checker CpaChecker. We evaluated
our tool on a collection of programs and a category of SvComp benchmarks. In
our experiments, we observed an improvement in branch coverage compared to
CREST from 48% to 63% in the best case, and from 66% to 71% on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02619</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02619</id><created>2015-11-09</created><authors><author><keyname>Ping</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Qiang</forenames></author><author><keyname>Ihler</keyname><forenames>Alexander</forenames></author></authors><title>Decomposition Bounds for Marginal MAP</title><categories>cs.LG cs.AI cs.IT math.IT stat.ML</categories><comments>NIPS 2015 (full-length)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Marginal MAP inference involves making MAP predictions in systems defined
with latent variables or missing information. It is significantly more
difficult than pure marginalization and MAP tasks, for which a large class of
efficient and convergent variational algorithms, such as dual decomposition,
exist. In this work, we generalize dual decomposition to a generic power sum
inference task, which includes marginal MAP, along with pure marginalization
and MAP, as special cases. Our method is based on a block coordinate descent
algorithm on a new convex decomposition bound, that is guaranteed to converge
monotonically, and can be parallelized efficiently. We demonstrate our approach
on marginal MAP queries defined on real-world problems from the UAI approximate
inference challenge, showing that our framework is faster and more reliable
than previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02623</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02623</id><created>2015-11-09</created><authors><author><keyname>Droulez</keyname><forenames>Jacques</forenames></author><author><keyname>Colliaux</keyname><forenames>David</forenames></author><author><keyname>Houillon</keyname><forenames>Audrey</forenames></author><author><keyname>Bessi&#xe8;re</keyname><forenames>Pierre</forenames></author></authors><title>Toward Biochemical Probabilistic Computation</title><categories>cs.ET cs.NE q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Living organisms survive and multiply even though they have uncertain and
incomplete information about their environment and imperfect models to predict
the consequences of their actions. Bayesian models have been proposed to face
this challenge. Indeed, Bayesian inference is a way to do optimal reasoning
when only uncertain and incomplete information is available. Various
perceptive, sensory-motor, and cognitive functions have been successfully
modeled this way. However, the biological mechanisms allowing animals and
humans to represent and to compute probability distributions are not known. It
has been proposed that neurons and assemblies of neurons could be the
appropriate scale to search for clues to probabilistic reasoning. In contrast,
in this paper, we propose that interacting populations of macromolecules and
diffusible messengers can perform probabilistic computation. This suggests that
probabilistic reasoning, based on cellular signaling pathways, is a fundamental
skill of living organisms available to the simplest unicellular organisms as
well as the most complex brains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02627</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02627</id><created>2015-11-09</created><authors><author><keyname>Cheng</keyname><forenames>Hao-Chung</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author></authors><title>Exponential Decay of Matrix $\Phi$-Entropies on Markov Semigroups with
  Applications to Dynamical Evolutions of Quantum Ensembles</title><categories>quant-ph cs.IT math-ph math.IT math.MP math.OA math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of Markovian processes, one of the principal achievements is the
equivalence between the $\Phi$-Sobolev inequalities and an exponential decrease
of the $\Phi$-entropies. In this work, we develop a framework of Markov
semigroups on matrix-valued functions and generalize the above equivalence to
the exponential decay of matrix $\Phi$-entropies. This result also specializes
to spectral gap inequalities and modified logarithmic Sobolev inequalities in
the random matrix setting. To establish the main result, we define a
non-commutative generalization of the carr\'e du champ operator, and prove a de
Bruijn's identity for matrix-valued functions.
  The proposed Markov semigroups acting on matrix-valued functions have
immediate applications in the characterization of the dynamical evolution of
quantum ensembles. We consider two special cases of quantum unital channels,
namely, the depolarizing channel and the phase-damping channel. In the former,
since there exists a unique equilibrium state, we show that the matrix
$\Phi$-entropy of the resulting quantum ensemble decays exponentially as time
goes on. Consequently, we obtain a stronger notion of monotonicity of the
Holevo quantity - the Holevo quantity of the quantum ensemble decays
exponentially in time and the convergence rate is determined by the modified
log-Sobolev inequalities. However, in the latter, the matrix $\Phi$-entropy of
the quantum ensemble that undergoes the phase-damping Markovian evolution
generally will not decay exponentially. This is because there are multiple
equilibrium states for such a channel.
  Finally, we also consider examples of statistical mixing of Markov semigroups
on matrix-valued functions. We can explicitly calculate the convergence rate of
a Markovian jump process defined on Boolean hypercubes, and provide upper
bounds of the mixing time on these types of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02629</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02629</id><created>2015-11-09</created><authors><author><keyname>Ong</keyname><forenames>C. -H. Luke</forenames></author></authors><title>Normalisation by Traversals</title><categories>cs.PL cs.LO</categories><acm-class>F.4.1; I.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method of computing the beta-normal eta-long form of a
simply-typed lambda-term by constructing traversals over a variant abstract
syntax tree of the term. In contrast to beta-reduction, which changes the term
by substitution, this method of normalisation by traversals leaves the original
term intact. We prove the correctness of the normalisation procedure by game
semantics. As an application, we establish a path-traversal correspondence
theorem which is the basis of a key decidability result in higher-order model
checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02633</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02633</id><created>2015-11-09</created><authors><author><keyname>Seel</keyname><forenames>A.</forenames></author><author><keyname>Davtyan</keyname><forenames>A.</forenames></author><author><keyname>Pietsch</keyname><forenames>U.</forenames></author><author><keyname>Loffeld</keyname><forenames>O.</forenames></author></authors><title>Norm minimized Scattering Data from Intensity Spectra: Feasible
  Computations</title><categories>cs.IT math.IT</categories><comments>25 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the $l_1$ minimizing technique of compressive sensing (CS) to
non-linear quadratic observations. For the example of Coherent X-ray scattering
we provide the formulae for a Kalman filter approach to quadratic CS and show
how to reconstruct the scattering data from their spatial intensity
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02647</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02647</id><created>2015-11-09</created><updated>2015-12-14</updated><authors><author><keyname>Kerckhove</keyname><forenames>Corentin Vande</forenames></author><author><keyname>Martin</keyname><forenames>Samuel</forenames></author><author><keyname>Gend</keyname><forenames>Pascal</forenames></author><author><keyname>Rentfrow</keyname><forenames>Peter J.</forenames></author><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent D.</forenames></author></authors><title>Modelling influence and opinion evolution in online collective behaviour</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opinion evolution and judgment revision are mediated through social
influence. Based on a crowdsourced in vitro experiment, it is shown how a
consensus model can be used to predict opinion evolution in online collective
behaviour. It is the first time the predictive power of a quantitative model of
opinion dynamics is tested against a real dataset. The model is parametrized by
the influenceability of each individuals, a factor representing to what extent
individuals incorporate external judgments. Judgment revision includes
unpredictable variations which limit the potential for prediction. This level
of unpredictability is measured via a specific control experiment. More than
two thirds of the prediction errors are found to occur due to unpredictability
of the human judgment revision process rather than to model imperfection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02650</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02650</id><created>2015-11-09</created><authors><author><keyname>Bsaybes</keyname><forenames>Sahar</forenames></author><author><keyname>Quilliot</keyname><forenames>Alain</forenames></author><author><keyname>Wagler</keyname><forenames>Annegret K.</forenames></author><author><keyname>Wegener</keyname><forenames>Jan-Thierry</forenames></author></authors><title>Two Flow-Based Approaches for the Static Relocation Problem in
  Carsharing Systems</title><categories>cs.DS</categories><comments>12 pages, 1 table</comments><acm-class>F.2; G.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a carsharing system, a fleet of cars is distributed at stations in an
urban area, customers can take and return cars at any time and station. For
operating such a system in a satisfactory way, the stations have to keep a good
ratio between the numbers of free places and cars in each station. This leads
to the problem of relocating cars between stations, which can be modeled within
the framework of a metric task system. In this paper, we focus on the Static
Relocation Problem, where the system has to be set into a certain state,
outgoing from the current state. We present two approaches to solve this
problem, a fast heuristic approach and an exact integer programming based
method using flows in time-expanded networks, and provide some computational
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02656</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02656</id><created>2015-11-09</created><authors><author><keyname>Le</keyname><forenames>Hung. T</forenames></author><author><keyname>Nguyen</keyname><forenames>Hai N.</forenames></author><author><keyname>Ngoc</keyname><forenames>Nam Pham</forenames></author><author><keyname>Pham</keyname><forenames>Anh T.</forenames></author><author><keyname>Thang</keyname><forenames>Truong Cong</forenames></author></authors><title>A Novel Adaptation Method for HTTP Streaming of VBR Videos over Mobile
  Networks</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, HTTP streaming has become very popular for delivering video over
the Internet. For adaptivity, a provider should generate multiple versions of a
video as well as the related metadata. Various adaptation methods have been
proposed to support a streaming client in coping with strong bandwidth
variations. However, most of existing methods target at constant bitrate (CBR)
videos only. In this paper, we present a new method for quality adaptation in
on-demand streaming of variable bitrate (VBR) videos. To cope with strong
variations of VBR bitrate, we use a local average bitrate as the representative
bitrate of a version. A buffer-based algorithm is then proposed to
conservatively adapt video quality. Through experiments, we show that our
method can provide quality stability as well as buffer stability even under
very strong variations of bandwidth and video bitrates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02667</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02667</id><created>2015-11-09</created><authors><author><keyname>Nguyen</keyname><forenames>Quynh</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author><author><keyname>Gautier</keyname><forenames>Antoine</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author></authors><title>An Efficient Multilinear Optimization Framework for Hypergraph Matching</title><categories>cs.CV cs.DS math.OC</categories><comments>21 pages, submitted to TPAMI, November 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hypergraph matching has recently become a popular approach for solving
correspondence problems in computer vision as it allows to integrate
higher-order geometric information. Hypergraph matching can be formulated as a
third-order optimization problem subject to the assignment constraints which
turns out to be NP-hard. In recent work, we have proposed an algorithm for
hypergraph matching which first lifts the third-order problem to a fourth-order
problem and then solves the fourth-order problem via optimization of the
corresponding multilinear form. This leads to a tensor block coordinate ascent
scheme which has the guarantee of providing monotonic ascent in the original
matching score function and leads to state-of-the-art performance both in terms
of achieved matching score and accuracy. In this paper we show that the lifting
step to a fourth-order problem can be avoided yielding a third-order scheme
with the same guarantees and performance but being two times faster. Moreover,
we introduce a homotopy type method which further improves the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02669</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02669</id><created>2015-11-09</created><authors><author><keyname>Groza</keyname><forenames>Adrian</forenames></author><author><keyname>Szabo</keyname><forenames>Roxana</forenames></author></authors><title>Enacting textual entailment and ontologies for automated essay grading
  in chemical domain</title><categories>cs.AI cs.CL</categories><comments>16th Int. Symposium on Computational Intelligence and Informatics
  (CINTI2015), Budapest, Hungary, 19-21 November, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a system for automated essay grading using ontologies and textual
entailment. The process of textual entailment is guided by hypotheses, which
are extracted from a domain ontology. Textual entailment checks if the truth of
the hypothesis follows from a given text. We enact textual entailment to
compare students answer to a model answer obtained from ontology. We validated
the solution against various essays written by students in the chemistry
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02674</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02674</id><created>2015-11-09</created><authors><author><keyname>Bertasius</keyname><forenames>Gedas</forenames></author><author><keyname>Shi</keyname><forenames>Jianbo</forenames></author><author><keyname>Torresani</keyname><forenames>Lorenzo</forenames></author></authors><title>Semantic Segmentation with Boundary Neural Fields</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state-of-the-art in semantic segmentation is currently represented by
fully convolutional networks (FCNs). However, FCNs use large receptive fields
and many pooling layers, both of which cause blurring and low spatial
resolution in the deep layers. As a result FCNs tend to produce segmentations
that are poorly localized around object boundaries. Prior work has attempted to
address this issue in post-processing steps, for example using a color-based
CRF on top of the FCN predictions. However, these approaches require additional
parameters and low-level features that are difficult to tune and integrate into
the original network architecture. Additionally, most CRFs use color-based
pixel affinities, which are not well suited for semantic segmentation and lead
to spatially disjoint predictions.
  To overcome these problems, we introduce a Boundary Neural Field (BNF), which
is a global energy model integrating FCN predictions with boundary cues. The
boundary information is used to enhance semantic segment coherence and to
improve object localization. Specifically, we first show that the convolutional
filters of semantic FCNs provide good features for boundary detection. We then
employ the predicted boundaries to define pairwise potentials in our energy.
Finally, we show that our energy decomposes semantic segmentation into multiple
binary problems, which can be relaxed for efficient global optimization. We
report extensive experiments demonstrating that minimization of our global
boundary-based energy yields results superior to prior globalization methods,
both quantitatively as well as qualitatively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02679</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02679</id><created>2015-11-09</created><authors><author><keyname>Thramboulidis</keyname><forenames>Kleanthis</forenames></author></authors><title>Comments on Bridging Service-Oriented Architecture and IEC 61499 for
  Flexibility and Interoperability</title><categories>cs.SE</categories><comments>3 pages. arXiv admin note: substantial text overlap with
  arXiv:1506.04615</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper by W. Dai et al. (IEEE Trans. On Industrial Informatics, vol.
11, no. 3, pp. 771-781, June 2015), a formal mapping between IEC 61499 and SOA
is presented and a SOA-based execution environment architecture is described.
In this letter, the proposed in the above paper mapping and the execution
environment architecture are discussed and their potential for the exploitation
is disputed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02680</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02680</id><created>2015-11-09</created><authors><author><keyname>Kendall</keyname><forenames>Alex</forenames></author><author><keyname>Badrinarayanan</keyname><forenames>Vijay</forenames></author><author><keyname>Cipolla</keyname><forenames>Roberto</forenames></author></authors><title>Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder
  Architectures for Scene Understanding</title><categories>cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel deep learning framework for probabilistic pixel-wise
semantic segmentation, which we term Bayesian SegNet. Pixel-wise semantic
segmentation is an important step for visual scene understanding. It is a
complex task requiring knowledge of support relationships and contextual
information, as well as visual appearance. Our contribution is a practical
system which is able to predict pixel-wise class labels with a measure of model
uncertainty. We achieve this by Monte Carlo sampling with dropout at test time
to generate a posterior distribution of pixel class labels. We show this
Bayesian neural network provides a significant performance improvement in
segmentation, with no additional parameterisation. We set a new benchmark with
state-of-the-art performance on both the indoor SUN Scene Understanding and
outdoor CamVid driving scenes datasets. Bayesian SegNet also performs
competitively on Pascal VOC 2012 object segmentation challenge. For our web
demo and source code, see http://mi.eng.cam.ac.uk/projects/segnet/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02682</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02682</id><created>2015-11-09</created><authors><author><keyname>Bertasius</keyname><forenames>Gedas</forenames></author><author><keyname>Park</keyname><forenames>Hyun Soo</forenames></author><author><keyname>Shi</keyname><forenames>Jianbo</forenames></author></authors><title>Exploiting Egocentric Object Prior for 3D Saliency Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On a minute-to-minute basis people undergo numerous fluid interactions with
objects that barely register on a conscious level. Recent neuroscientific
research demonstrates that humans have a fixed size prior for salient objects.
This suggests that a salient object in 3D undergoes a consistent transformation
such that people's visual system perceives it with an approximately fixed size.
This finding indicates that there exists a consistent egocentric object prior
that can be characterized by shape, size, depth, and location in the first
person view.
  In this paper, we develop an EgoObject Representation, which encodes these
characteristics by incorporating shape, location, size and depth features from
an egocentric RGBD image. We empirically show that this representation can
accurately characterize the egocentric object prior by testing it on an
egocentric RGBD dataset for three tasks: the 3D saliency detection, future
saliency prediction, and interaction classification. This representation is
evaluated on our new Egocentric RGBD Saliency dataset that includes various
activities such as cooking, dining, and shopping. By using our EgoObject
representation, we outperform previously proposed models for saliency detection
(relative 30% improvement for 3D saliency detection task) on our dataset.
Additionally, we demonstrate that this representation allows us to predict
future salient objects based on the gaze cue and classify people's interactions
with objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02683</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02683</id><created>2015-11-09</created><authors><author><keyname>Wu</keyname><forenames>Xiang</forenames></author><author><keyname>He</keyname><forenames>Ran</forenames></author><author><keyname>Sun</keyname><forenames>Zhenan</forenames></author></authors><title>A Lightened CNN for Deep Face Representation</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1507.04844</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolution neural network (CNN) has significantly pushed forward the
development of face recognition techniques. To achieve ultimate accuracy, CNN
models tend to be deeper or multiple local facial patch ensemble, which result
in a waste of time and space. To alleviate this issue, this paper studies a
lightened CNN framework to learn a compact embedding for face representation.
First, we introduce the concept of maxout in the fully connected layer to the
convolution layer, which leads to a new activation function, named
Max-Feature-Map (MFM). Compared with widely used ReLU, MFM can simultaneously
capture compact representation and competitive information. Then, one shallow
CNN model is constructed by 4 convolution layers and totally contains about 4M
parameters; and the other is constructed by reducing the kernel size of
convolution layers and adding Network in Network (NIN) layers between
convolution layers based on the previous one. These models are trained on the
CASIA-WebFace dataset and evaluated on the LFW and YTF datasets. Experimental
results show that the proposed models achieve state-of-the-art results. At the
same time, a reduction of computational cost is reached by over 9 times in
comparison with the released VGG model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02689</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02689</id><created>2015-11-09</created><authors><author><keyname>Gesing</keyname><forenames>Sandra</forenames></author><author><keyname>Connor</keyname><forenames>Thomas Richard</forenames></author><author><keyname>Taylor</keyname><forenames>Ian</forenames></author></authors><title>Genomics and Biological Big Data: Facing Current and Future Challenges
  around Data and Software Sharing and Reproducibility</title><categories>cs.DC cs.CY</categories><comments>Position paper at BDAC-15 (Big Data Analytics: Challenges and
  Opportunities), workshop in cooperation with ACM/IEEE SC15, November 16,
  2015, Austin, TX, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novel technologies in genomics allow creating data in exascale dimension with
relatively minor effort of human and laboratory and thus monetary resources
compared to capabilities only a decade ago. While the availability of this data
salvage to find answers for research questions, which would not have been
feasible before, maybe even not feasible to ask before, the amount of data
creates new challenges, which obviously need new software and data management
systems. Such new solutions have to consider integrative approaches, which are
not only considering the effectiveness and efficiency of data processing but
improve reusability, reproducibility and usability especially tailored to the
target user communities of genomic big data. In our opinion, current solutions
tackle part of the challenges and have each their strengths but lack to provide
a complete solution. We present in this paper the key challenges and the
characteristics cutting-edge developments should possess for fulfilling the
needs of the user communities to allow for seamless sharing and data analysis
on a large scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02705</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02705</id><created>2015-11-09</created><authors><author><keyname>Vacher</keyname><forenames>Jonathan</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Meso</keyname><forenames>Andrew</forenames><affiliation>INT</affiliation></author><author><keyname>Perrinet</keyname><forenames>Laurent U</forenames><affiliation>INT</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author></authors><title>Biologically Inspired Dynamic Textures for Probing Motion Perception</title><categories>cs.CV math.ST stat.TH</categories><comments>Twenty-ninth Annual Conference on Neural Information Processing
  Systems (NIPS), Dec 2015, Montreal, Canada</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perception is often described as a predictive process based on an optimal
inference with respect to a generative model. We study here the principled
construction of a generative model specifically crafted to probe motion
perception. In that context, we first provide an axiomatic, biologically-driven
derivation of the model. This model synthesizes random dynamic textures which
are defined by stationary Gaussian distributions obtained by the random
aggregation of warped patterns. Importantly, we show that this model can
equivalently be described as a stochastic partial differential equation. Using
this characterization of motion in images, it allows us to recast motion-energy
models into a principled Bayesian inference framework. Finally, we apply these
textures in order to psychophysically probe speed perception in humans. In this
framework, while the likelihood is derived from the generative model, the prior
is estimated from the observed results and accounts for the perceptual bias in
a principled fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02725</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02725</id><created>2015-11-09</created><updated>2016-01-08</updated><authors><author><keyname>Lascu</keyname><forenames>Andrei</forenames></author><author><keyname>Donaldson</keyname><forenames>Alastair F.</forenames></author></authors><title>Integrating a large-scale testing campaign in the CK framework</title><categories>cs.SE</categories><comments>7 pages, 4 figures, Adapt 2016 workshop (co-located with HiPEAC 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of conducting large experimental campaigns in
programming languages research. Most research efforts require a certain level
of bookkeeping of results. This is manageable via quick, on-the-fly
infrastructure implementations. However, it becomes a problem for large-scale
testing initiatives, especially as the needs of the project evolve along the
way. We look at how the Collective Knowledge generalized testing framework can
help with such a project and its overall applicability and ease of use. The
project in question is an OpenCL compiler testing campaign. We investigate how
to use the Collective Knowledge framework to lead the experimental campaign, by
providing storage and representation of test cases and their results. We also
provide an initial implementation, publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02727</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02727</id><created>2015-11-09</created><authors><author><keyname>Liao</keyname><forenames>Wenjing</forenames></author></authors><title>MUSIC for multidimensional spectral estimation: stability and
  super-resolution</title><categories>cs.IT math.IT math.NA</categories><comments>To appear in IEEE Transactions on Signal Processing</comments><msc-class>65T40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a performance analysis of the MUltiple SIgnal
Classification (MUSIC) algorithm applied on $D$ dimensional single-snapshot
spectral estimation while $s$ true frequencies are located on the continuum of
a bounded domain. Inspired by the matrix pencil form, we construct a D-fold
Hankel matrix from the measurements and exploit its Vandermonde decomposition
in the noiseless case. MUSIC amounts to identifying a noise subspace,
evaluating a noise-space correlation function, and localizing frequencies by
searching the $s$ smallest local minima of the noise-space correlation
function.
  In the noiseless case, $(2s)^D$ measurements guarantee an exact
reconstruction by MUSIC as the noise-space correlation function vanishes
exactly at true frequencies. When noise exists, we provide an explicit estimate
on the perturbation of the noise-space correlation function in terms of noise
level, dimension $D$, the minimum separation among frequencies, the maximum and
minimum amplitudes while frequencies are separated by two Rayleigh Length (RL)
at each direction. As a by-product the maximum and minimum non-zero singular
values of the multidimensional Vandermonde matrix whose nodes are on the unit
sphere are estimated under a gap condition of the nodes. Under the 2-RL
separation condition, if noise is i.i.d. gaussian, we show that perturbation of
the noise-space correlation function decays like
$\sqrt{\log(\#(\mathbf{N}))/\#(\mathbf{N})}$ as the sample size
$\#(\mathbf{N})$ increases.
  When the separation among frequencies drops below 2 RL, our numerical
experiments show that the noise tolerance of MUSIC obeys a power law with the
minimum separation of frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02743</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02743</id><created>2015-11-09</created><updated>2015-11-10</updated><authors><author><keyname>Cao</keyname><forenames>Yuan</forenames></author><author><keyname>Li</keyname><forenames>Qingguo</forenames></author></authors><title>On $(\alpha+u\beta)$-constacyclic codes of length $p^sn$ over
  $\mathbb{F}_{p^m}+u\mathbb{F}_{p^m}$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbb{F}_{p^m}$ be a finite field of cardinality $p^m$ and
$R=\mathbb{F}_{p^m}[u]/\langle u^2\rangle=\mathbb{F}_{p^m}+u\mathbb{F}_{p^m}$
$(u^2=0)$, where $p$ is an odd prime and $m$ is a positive integer. For any
$\alpha,\beta\in \mathbb{F}_{p^m}^{\times}$, the aim of this paper is to
represent all distinct $(\alpha+u\beta)$-constacyclic codes over $R$ of length
$p^sn$ and their dual codes, where $s$ is a nonnegative integer and $n$ is a
positive integer satisfying ${\rm gcd}(p,n)=1$. Especially, all distinct
$(2+u)$-constacyclic codes of length $6\cdot 5^t$ over
$\mathbb{F}_{3}+u\mathbb{F}_3$ and their dual codes are listed, where $t$ is a
positive integer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02751</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02751</id><created>2015-11-09</created><authors><author><keyname>Bsaybes</keyname><forenames>Sahar</forenames></author><author><keyname>Krumke</keyname><forenames>Sven O.</forenames></author><author><keyname>Quilliot</keyname><forenames>Alain</forenames></author><author><keyname>Wagler</keyname><forenames>Annegret K.</forenames></author><author><keyname>Wegener</keyname><forenames>Jan-Thierry</forenames></author></authors><title>ReOpt: an Algorithm with a Quality Guaranty for Solving the Static
  Relocation Problem</title><categories>cs.DS cs.DM</categories><comments>39 pages, 8 figures, 2 tables, 3 algorithms</comments><acm-class>F.2; G.1.6; G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a carsharing system, a fleet of cars is distributed at stations in an
urban area, customers can take and return cars at any time and station. For
operating such a system in a satisfactory way, the stations have to keep a good
ratio between the numbers of free places and cars in each station. This leads
to the problem of relocating cars between stations, which can be modeled within
the framework of a metric task system. In this paper, we focus on the Static
Relocation Problem, where the system has to be set into a certain state,
outgoing from the current state. We present a combinatorial approach and
provide approximation factors for several different situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02770</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02770</id><created>2015-11-05</created><authors><author><keyname>Khakimzyanov</keyname><forenames>Gayaz</forenames><affiliation>LAMA</affiliation></author><author><keyname>Dutykh</keyname><forenames>Denys</forenames><affiliation>LAMA</affiliation></author></authors><title>On supraconvergence phenomenon for second order centered finite
  differences on non-uniform grids</title><categories>math.NA cs.NA math.CA physics.class-ph physics.comp-ph</categories><comments>17 pages, 2 figures, 2 tables, 27 references. Other author's papers
  can be downloaded at http://www.denys-dutykh.com/</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present note we consider an example of a boundary value problem for a
simple second order ordinary differential equation, which may exhibit a
boundary layer phenomenon. We show that usual central finite differences, which
are second order accurate on a uniform grid, can be substantially upgraded to
the fourth order by a suitable choice of the underlying non-uniform grid. This
example is quite pedagogical and may give some ideas for more complex problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02771</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02771</id><created>2015-11-05</created><updated>2016-02-26</updated><authors><author><keyname>Khakimzyanov</keyname><forenames>Gayaz</forenames><affiliation>LAMA</affiliation></author><author><keyname>Dutykh</keyname><forenames>Denys</forenames><affiliation>LAMA</affiliation></author><author><keyname>Mitsotakis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Shokina</keyname><forenames>Nina</forenames></author></authors><title>Numerical solution of conservation laws on moving grids</title><categories>math.NA cs.NA physics.class-ph physics.comp-ph</categories><comments>34 pages, 7 figures, 5 tables, 45 references. Other author's papers
  can be downloaded at http://www.denys-dutykh.com/</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present article we describe a few simple and efficient finite volume
type schemes on moving grids in one spatial dimension. The underlying finite
volume scheme is conservative and it is accurate up to the second order in
space. The main novelty consists in the motion of the grid. This new dynamic
aspect can be used to resolve better the areas with high solution gradients or
any other special features. No interpolation procedure is employed, thus an
unnecessary solution smearing is avoided. Thus, our method enjoys excellent
conservation properties. The resulting grid is completely redistributed
according the choice of the so-called monitor function. Several more or less
universal choices of the monitor function are provided. Finally, the
performance of the proposed algorithm is illustrated on several examples
stemming from the simple linear advection to the simulation of complex shallow
water waves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02784</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02784</id><created>2015-11-09</created><authors><author><keyname>Del Pia</keyname><forenames>Alberto</forenames></author><author><keyname>Ferris</keyname><forenames>Michael</forenames></author><author><keyname>Michini</keyname><forenames>Carla</forenames></author></authors><title>Totally Unimodular Congestion Games</title><categories>cs.GT cs.DM</categories><acm-class>G.1.6; G.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a new class of congestion games, called Totally Unimodular
Congestion Games, in which the strategies of each player are expressed as
binary vectors lying in a polyhedron defined using a totally unimodular
constraint matrix and an integer right-hand side. We study both the symmetric
and the asymmetric variants of the game. In the symmetric variant, all players
have the same strategy set, i.e. the same constraint matrix and right-hand
side. Network congestion games are an example of this class. Fabrikant et al.
proved that a pure Nash equilibrium of symmetric network congestion games can
be found in strongly polynomial time, while the asymmetric network congestion
games are PLS-complete. We give a strongly polynomial-time algorithm to find a
pure Nash equilibrium of any symmetric totally unimodular congestion game. We
also identify four totally unimodular congestion games, where the players'
strategy sets are matchings, vertex covers, edge covers and stable sets of a
given bipartite graph. For these games we derive specialized combinatorial
algorithms to find a pure Nash equilibrium in the symmetric variant, and show
the asymmetric variant is PLS-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02786</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02786</id><created>2015-11-09</created><authors><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Rezaei</keyname><forenames>Alireza</forenames></author></authors><title>Approximation Algorithms for Finding Maximum Induced Expanders</title><categories>cs.DS</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of approximating the largest induced expander in a
given graph $G$. Given a $\Delta$-regular graph $G$ with $n$ vertices, the goal
is to find the set with the largest induced expansion of size at least $\delta
\cdot n$. We design a bi-criteria approximation algorithm for this problem; if
the optimum has induced spectral expansion $\lambda$ our algorithm returns a
$\frac{\lambda}{\log^2\delta \exp(\Delta/\lambda)}$-(spectral) expander of size
at least $\delta n$ (up to constants).
  Our proof introduces and employs a novel semidefinite programming relaxation
for the largest induced expander problem. We expect to see further applications
of our SDP relaxation in graph partitioning problems. In particular, because of
the close connection to the small set expansion problem, one may be able to
obtain new insights into the unique games problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02793</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02793</id><created>2015-11-09</created><updated>2016-02-29</updated><authors><author><keyname>Mansimov</keyname><forenames>Elman</forenames></author><author><keyname>Parisotto</keyname><forenames>Emilio</forenames></author><author><keyname>Ba</keyname><forenames>Jimmy Lei</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Generating Images from Captions with Attention</title><categories>cs.LG cs.CV</categories><comments>Published as a conference paper at ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the recent progress in generative models, we introduce a model
that generates images from natural language descriptions. The proposed model
iteratively draws patches on a canvas, while attending to the relevant words in
the description. After training on Microsoft COCO, we compare our model with
several baseline generative models on image generation and retrieval tasks. We
demonstrate that our model produces higher quality samples than other
approaches and generates images with novel scene compositions corresponding to
previously unseen captions in the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02799</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02799</id><created>2015-11-09</created><updated>2015-11-23</updated><authors><author><keyname>Andreas</keyname><forenames>Jacob</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Klein</keyname><forenames>Dan</forenames></author></authors><title>Deep Compositional Question Answering with Neural Module Networks</title><categories>cs.CV cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual question answering is fundamentally compositional in nature---a
question like &quot;where is the dog?&quot; shares substructure with questions like &quot;what
color is the dog?&quot; and &quot;where is the cat?&quot; This paper seeks to simultaneously
exploit the representational capacity of deep networks and the compositional
linguistic structure of questions. We describe a procedure for constructing and
learning *neural module networks*, which compose collections of jointly-trained
neural &quot;modules&quot; into deep networks for question answering. Our approach
decomposes questions into their linguistic substructures, and uses these
structures to dynamically instantiate modular networks (with reusable
components for recognizing dogs, classifying colors, etc.). The resulting
compound networks are jointly trained. We evaluate our approach on two
challenging datasets for visual question answering, achieving state-of-the-art
results on both the VQA natural image dataset and a new dataset of complex
questions about abstract shapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02801</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02801</id><created>2015-11-09</created><authors><author><keyname>Knop</keyname><forenames>Du&#x161;an</forenames></author><author><keyname>Dvo&#x159;&#xe1;k</keyname><forenames>Pavel</forenames></author></authors><title>Parameterized complexity of length-bounded cuts and multi-cuts</title><categories>cs.DS</categories><comments>20 pages, 7 figures, TAMC 2015 proceedings</comments><msc-class>05C21, 05C85</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the Minimal Length-Bounded L-But problem can be computed in
linear time with respect to L and the tree-width of the input graph as
parameters. In this problem the task is to find a set of edges of a graph such
that after removal of this set, the shortest path between two prescribed
vertices is at least L long. We derive an FPT algorithm for a more general
multi-commodity length bounded cut problem when parameterized by the number of
terminals also.
  For the former problem we show a W[1]-hardness result when the
parameterization is done by the path-width only (instead of the tree-width) and
that this problem does not admit polynomial kernel when parameterized by
tree-width and L. We also derive an FPT algorithm for the Minimal
Length-Bounded Cut problem when parameterized by the tree-depth. Thus showing
an interesting paradigm for this problem and parameters tree-depth and
path-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02821</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02821</id><created>2015-11-09</created><authors><author><keyname>Chen</keyname><forenames>Chao</forenames></author><author><keyname>Zare</keyname><forenames>Alina</forenames></author><author><keyname>Cobb</keyname><forenames>J. Tory</forenames></author></authors><title>Partial Membership Latent Dirichlet Allocation</title><categories>stat.ML cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many years, topic models (e.g., pLSA, LDA, SLDA) have been widely used
for segmenting and recognizing objects in imagery simultaneously. However,
these models are confined to the analysis of categorical data, forcing a visual
word to belong to one and only one topic. There are many images in which some
regions cannot be assigned a crisp categorical label (e.g., transition regions
between a foggy sky and the ground or between sand and water at a beach). In
these cases, a visual word is best represented with partial memberships across
multiple topics. To address this, we present a partial membership latent
Dirichlet allocation (PM-LDA) model and associated parameter estimation
algorithms. PM-LDA defines a novel partial membership model for word and
document generation. We employ Gibbs sampling for parameter estimation.
Experimental results on two natural image datasets and one SONAR image dataset
show that PM-LDA can produce both crisp and soft semantic image segmentations;
a capability existing methods do not have.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02825</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02825</id><created>2015-11-09</created><authors><author><keyname>Jiao</keyname><forenames>Changzhe</forenames></author><author><keyname>Zare</keyname><forenames>Alina</forenames></author></authors><title>Multiple Instance Dictionary Learning using Functions of Multiple
  Instances</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multiple instance dictionary learning method using functions of multiple
instances (DL-FUMI) is proposed to address target detection and two-class
classification problems with inaccurate training labels. Given inaccurate
training labels, DL-FUMI learns a set of target dictionary atoms that describe
the most distinctive and representative features of the true positive class as
well as a set of nontarget dictionary atoms that account for the shared
information found in both the positive and negative instances. Experimental
results show that the estimated target dictionary atoms found by DL-FUMI are
more representative prototypes and identify better discriminative features of
the true positive class than existing methods in the literature. DL-FUMI is
shown to have significantly better performance on several target detection and
classification problems as compared to other multiple instance learning (MIL)
dictionary learning algorithms on a variety of MIL problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02831</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02831</id><created>2015-11-09</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Mao</keyname><forenames>Jieming</forenames></author><author><keyname>Weinberg</keyname><forenames>S. Matthew</forenames></author></authors><title>Interpolating Between Truthful and non-Truthful Mechanisms for
  Combinatorial Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the communication complexity of combinatorial auctions via
interpolation mechanisms that interpolate between non-truthful and truthful
protocols. Specifically, an interpolation mechanism has two phases. In the
first phase, the bidders participate in some non-truthful protocol whose output
is itself a truthful protocol. In the second phase, the bidders participate in
the truthful protocol selected during phase one. Note that virtually all
existing auctions have either a non-existent first phase (and are therefore
truthful mechanisms), or a non-existent second phase (and are therefore just
traditional protocols, analyzed via the Price of Anarchy/Stability).
  The goal of this paper is to understand the benefits of interpolation
mechanisms versus truthful mechanisms or traditional protocols, and develop the
necessary tools to formally study them. Interestingly, we exhibit settings
where interpolation mechanisms greatly outperform the optimal traditional and
truthful protocols. Yet, we also exhibit settings where interpolation
mechanisms are provably no better than truthful ones. Finally, we apply our new
machinery to prove that the recent single-bid mechanism of Devanur et.
al.~\cite{DevanurMSW15} (the only pre-existing interpolation mechanism in the
literature) achieves the optimal price of anarchy among a wide class of
protocols, a claim that simply can't be addressed by appealing just to
machinery from communication complexity or the study of truthful mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02833</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02833</id><created>2015-11-09</created><authors><author><keyname>Liu</keyname><forenames>Yuanwei</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Cooperative Non-Orthogonal Multiple Access with Simultaneous Wireless
  Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Journal on Selected Areas in Communications (JSAC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the application of simultaneous wireless information and power
transfer (SWIPT) to non-orthogonal multiple access (NOMA) networks in which
users are spatially randomly located is investigated. A new cooperative SWIPT
NOMA protocol is proposed, in which near NOMA users that are close to the
source act as energy harvesting relays to help far NOMA users. Since the
locations of users have a significant impact on the performance, three user
selection schemes based on the user distances from the base station are
proposed. To characterize the performance of the proposed selection schemes,
closed-form expressions for the outage probability and system throughput are
derived. These analytical results demonstrate that the use of SWIPT will not
jeopardize the diversity gain compared to the conventional NOMA. The proposed
results confirm that the opportunistic use of node locations for user selection
can achieve low outage probability and deliver superior throughput in
comparison to the random selection scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02841</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02841</id><created>2015-11-09</created><updated>2015-11-16</updated><authors><author><keyname>Georgiev</keyname><forenames>Galin</forenames></author></authors><title>Symmetries and control in generative neural nets</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study generative nets which can control and modify observations, after
being trained on real-life datasets. In order to zoom-in on an object, some
spatial, color and other attributes are learned by classifiers in specialized
attention nets. In field-theoretical terms, these learned symmetry statistics
form the gauge group of the data set. Plugging them in the generative layers of
auto-classifiers-encoders (ACE) appears to be the most direct way to
simultaneously: i) generate new observations with arbitrary attributes, from a
given class, ii) describe the low-dimensional manifold encoding the &quot;essence&quot;
of the data, after superfluous attributes are factored out, and iii)
organically control, i.e., move or modify objects within given observations. We
demonstrate the sharp improvement of the generative qualities of shallow ACE,
with added spatial and color symmetry statistics, on the distorted MNIST and
CIFAR10 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02853</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02853</id><created>2015-11-09</created><updated>2015-11-27</updated><authors><author><keyname>Bilen</keyname><forenames>Hakan</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author></authors><title>Weakly Supervised Deep Detection Networks</title><categories>cs.CV</categories><comments>9 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weakly supervised learning of object detection is an important problem in
image understanding that still does not have a satisfactory solution. In this
paper, we address this problem by exploiting the power of deep convolutional
neural networks pre-trained on large-scale image-level classification tasks. We
propose a weakly supervised deep detection architecture that modifies one such
network to operate at the level of image regions, performing simultaneously
region selection and classification. Trained as an image classifier, the
architecture implicitly learns object detectors that are better than
alternative weakly supervised detection systems on the PASCAL VOC data. The
model, which is a simple and elegant end-to-end architecture, outperforms
standard data augmentation and fine-tuning techniques for the task of
image-level classification as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02872</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02872</id><created>2015-11-09</created><authors><author><keyname>Kato</keyname><forenames>Hiroharu</forenames></author><author><keyname>Harada</keyname><forenames>Tatsuya</forenames></author></authors><title>Visual Language Modeling on CNN Image Representations</title><categories>cs.CV cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring the naturalness of images is important to generate realistic images
or to detect unnatural regions in images. Additionally, a method to measure
naturalness can be complementary to Convolutional Neural Network (CNN) based
features, which are known to be insensitive to the naturalness of images.
However, most probabilistic image models have insufficient capability of
modeling the complex and abstract naturalness that we feel because they are
built directly on raw image pixels. In this work, we assume that naturalness
can be measured by the predictability on high-level features during eye
movement. Based on this assumption, we propose a novel method to evaluate the
naturalness by building a variant of Recurrent Neural Network Language Models
on pre-trained CNN representations. Our method is applied to two tasks,
demonstrating that 1) using our method as a regularizer enables us to generate
more understandable images from image features than existing approaches, and 2)
unnaturalness maps produced by our method achieve state-of-the-art eye fixation
prediction performance on two well-studied datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02889</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02889</id><created>2015-11-09</created><authors><author><keyname>B&#xe1;tfai</keyname><forenames>Norbert</forenames></author></authors><title>A disembodied developmental robotic agent called Samu B\'atfai</title><categories>cs.AI</categories><comments>21 pages, 16 figures</comments><msc-class>68T05</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The agent program, called Samu, is an experiment to build a disembodied
DevRob (Developmental Robotics) chatter bot that can talk in a natural language
like humans do. One of the main design feature is that Samu can be interacted
with using only a character terminal. This is important not only for practical
aspects of Turing test or Loebner prize, but also for the study of basic
principles of Developmental Robotics. Our purpose is to create a rapid
prototype of Q-learning with neural network approximators for Samu. We sketch
out the early stages of the development process of this prototype, where Samu's
task is to predict the next sentence of tales or conversations. The basic
objective of this paper is to reach the same results using reinforcement
learning with general function approximators that can be achieved by using the
classical Q lookup table on small input samples. The paper is closed by an
experiment that shows a significant improvement in Samu's learning when using
LZW tree to narrow the number of possible Q-actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02899</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02899</id><created>2015-11-09</created><authors><author><keyname>Chumbalov</keyname><forenames>Daniyar</forenames></author><author><keyname>Romashchenko</keyname><forenames>Andrei</forenames></author></authors><title>On the Combinatorial Version of the Slepian-Wolf Problem</title><categories>cs.IT cs.DM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following combinatorial version of the Slepian-Wolf coding
scheme. Two isolated Senders are given binary strings $X$ and $Y$ respectively;
the length of each string is equal to $n$, and the Hamming distance between the
strings is at most $\alpha n$. The Senders compress their strings and
communicate the results to the Receiver. Then the Receiver must reconstruct
both strings $X$ and $Y$. The aim is to minimise the lengths of the transmitted
messages.
  For an asymmetric variant of this problem (where one of the Senders transmits
the input string to the Receiver without compression) with deterministic
encoding a nontrivial lower bound was found by A.Orlitsky and K.Viswanathany.
In our paper we prove a new lower bound for the schemes with syndrome coding,
where at least one of the Senders uses linear encoding of the input string.
  For the combinatorial Slepian-Wolf problem with randomized encoding the
theoretical optimum of communication complexity was recently found by the first
author, though effective protocols with optimal lengths of messages remained
unknown. We close this gap and present a polynomial time randomized protocol
that achieves the optimal communication complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02900</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02900</id><created>2015-10-26</created><authors><author><keyname>Batra</keyname><forenames>Nipun</forenames></author><author><keyname>Singh</keyname><forenames>Amarjeet</forenames></author><author><keyname>Whitehouse</keyname><forenames>Kamin</forenames></author></authors><title>Neighbourhood NILM: A Big-data Approach to Household Energy
  Disaggregation</title><categories>cs.LG</categories><comments>Under submission to IPSN 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate whether &quot;big-data&quot; is more valuable than
&quot;precise&quot; data for the problem of energy disaggregation: the process of
breaking down aggregate energy usage on a per-appliance basis. Existing
techniques for disaggregation rely on energy metering at a resolution of 1
minute or higher, but most power meters today only provide a reading once per
month, and at most once every 15 minutes. In this paper, we propose a new
technique called Neighbourhood NILM that leverages data from 'neighbouring'
homes to disaggregate energy given only a single energy reading per month. The
key intuition behind our approach is that 'similar' homes have 'similar' energy
consumption on a per-appliance basis. Neighbourhood NILM matches every home
with a set of 'neighbours' that have direct submetering infrastructure, i.e.
power meters on individual circuits or loads. Many such homes already exist.
Then, it estimates the appliance-level energy consumption of the target home to
be the average of its K neighbours. We evaluate this approach using 25 homes
and results show that our approach gives comparable or better disaggregation in
comparison to state-of-the-art accuracy reported in the literature that depend
on manual model training, high frequency power metering, or both. Results show
that Neighbourhood NILM can achieve 83% and 79% accuracy disaggregating fridge
and heating/cooling loads, compared to 74% and 73% for a technique called FHMM.
Furthermore, it achieves up to 64% accuracy on washing machine, dryer,
dishwasher, and lighting loads, which is higher than previously reported
results. Many existing techniques are not able to disaggregate these loads at
all. These results indicate a potentially substantial advantage to installing
submetering infrastructure in a select few homes rather than installing new
high-frequency smart metering infrastructure in all homes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02903</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02903</id><created>2015-09-25</created><authors><author><keyname>Shantilau</keyname><forenames>Rui</forenames></author><author><keyname>Goncalves</keyname><forenames>Antonio</forenames></author><author><keyname>Correia</keyname><forenames>Anacleto</forenames></author></authors><title>A Socio-Technical approach to address the Information security: Using
  the 27001 Manager Artefact</title><categories>cs.CR</categories><comments>16 PAGES, 10 FIGURES</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In general, the perspective customer / supplier followed by organizations,
regarding information security management, is based mainly on management
controls based on standards such as ISO / IEC 27001: 2015, resulting in the
production of especially technical analysis reports, rather than a
socio-technical approach. This leads to the perception by the customer of the
delivery of a product instead of a service.The product concerned is reduced to
a set of prescriptions, sometimes unrelated, which materialize in a descriptive
and static view of client security management. As a result, the client can
hardly use the product continuously, following the dynamics of changes in their
organization, therefore recognizing value in the provision made by the
supplier. The use of the paradigm Service Dominant Logic (LDS), in the
development of a range of security management information, helps to change the
focus of tangible resources to the intangible assets. The aspects of
tangibility, materialized in a document that describes the client's
vulnerabilities and attack vectors are referred to a secondary level, given the
importance of the intangible aspects, such as the interaction that is
established between the customer specialists and supplier. In this article we
propose to analyze in the perspective of a socio-technical theory, the Activity
Theory, the service provided by an artifact called 27001 Manager, designed to
assist the entire cycle of analysis, development and maintenance of an
information security management system (ISMS). The analysis aims at observing
the existing interaction between customer / supplier, considering that the
service is inherently dynamic and inter-subjective, ie the result of a
compromise between the customer and the supplier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02909</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02909</id><created>2015-11-09</created><authors><author><keyname>Moosavi</keyname><forenames>Azam</forenames></author><author><keyname>Stefanescu</keyname><forenames>Razvan</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author></authors><title>Efficient Construction of Local Parametric Reduced Order Models Using
  Machine Learning Techniques</title><categories>cs.LG</categories><comments>28 pages, 15 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reduced order models are computationally inexpensive approximations that
capture the important dynamical characteristics of large, high-fidelity
computer models of physical systems. This paper applies machine learning
techniques to improve the design of parametric reduced order models.
Specifically, machine learning is used to develop feasible regions in the
parameter space where the admissible target accuracy is achieved with a
predefined reduced order basis, to construct parametric maps, to chose the best
two already existing bases for a new parameter configuration from accuracy
point of view and to pre-select the optimal dimension of the reduced basis such
as to meet the desired accuracy. By combining available information using bases
concatenation and interpolation as well as high-fidelity solutions
interpolation we are able to build accurate reduced order models associated
with new parameter settings. Promising numerical results with a viscous Burgers
model illustrate the potential of machine learning approaches to help design
better reduced order models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02910</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02910</id><created>2015-11-09</created><authors><author><keyname>Curticapean</keyname><forenames>Radu</forenames></author></authors><title>Block Interpolation: A Framework for Tight Exponential-Time Counting
  Complexity</title><categories>cs.CC</categories><comments>17 pages, appears in ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise a framework for proving tight lower bounds under the counting
exponential-time hypothesis #ETH introduced by Dell et al. (ACM Transactions on
Algorithms, 2014). Our framework allows to convert many known #P-hardness
results for counting problems into tight lower bounds of the following type:
Assuming #ETH, the given problem admits no algorithm with running time
$2^{o(n)}$ on graphs with $n$ vertices and $O(n)$ edges. As exemplary
applications of this framework, we obtain such tight lower bounds for the
evaluation of the zero-one permanent, the matching polynomial, and the Tutte
polynomial on all non-easy points except for two lines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02911</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02911</id><created>2015-11-09</created><updated>2015-12-05</updated><authors><author><keyname>Remez</keyname><forenames>Tal</forenames></author><author><keyname>Avidan</keyname><forenames>Shai</forenames></author></authors><title>Spatially Coherent Random Forests</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatially Coherent Random Forest (SCRF) extends Random Forest to create
spatially coherent labeling. Each split function in SCRF is evaluated based on
a traditional information gain measure that is regularized by a spatial
coherency term. This way, SCRF is encouraged to choose split functions that
cluster pixels both in appearance space and in image space. In particular, we
use SCRF to detect contours in images, where contours are taken to be the
boundaries between different regions. Each tree in the forest produces a
segmentation of the image plane and the boundaries of the segmentations of all
trees are aggregated to produce a final hierarchical contour map. We show that
this modification improves the performance of regular Random Forest by about
10% on the standard Berkeley Segmentation Datasets. We believe that SCRF can be
used in other settings as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02913</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02913</id><created>2015-11-09</created><authors><author><keyname>Georgiadis</keyname><forenames>Loukas</forenames></author><author><keyname>Italiano</keyname><forenames>Giuseppe F.</forenames></author><author><keyname>Parotsidis</keyname><forenames>Nikos</forenames></author></authors><title>A New Framework for Strong Connectivity and 2-Connectivity in Directed
  Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate some basic problems related to the strong
connectivity and to the $2$-connectivity of a directed graph, by considering
the effect of edge and vertex deletions on its strongly connected components.
Let $G$ be a directed graph with $m$ edges and $n$ vertices. We present a
collection of $O(n)$-space data structures that, after $O(m+n)$-time
preprocessing can accomplish the following tasks: $(i)$ Report in $O(n)$
worst-case time all the strongly connected components obtained after deleting a
single edge (resp., a single vertex) in $G$. $(ii)$ Compute the total number of
strongly connected components obtained after deleting a single edge (resp.,
vertex) in total worst-case $O(n)$ time for all edges (resp., for all
vertices). $(iii)$ Compute the size of the largest and of the smallest strongly
connected components obtained after deleting a single edge (resp., vertex) in
total worst-case $O(n)$ time for all edges (resp., for all vertices).
  All our bounds are tight. After the same $O(m+n)$-time preprocessing, we can
also build an $O(n)$-space data structure that can answer in asymptotically
optimal time the following basic $2$-edge and $2$-vertex connectivity queries
on directed graphs: $(i)$ Test whether an edge or a vertex separates two query
vertices in $O(1)$ worst-case time. $(ii)$ Report all edges or all vertices
that separate two query vertices in optimal worst-case time. If there are $k&gt;0$
separating edges (resp., separating vertices), then the time is $O(k)$ in the
worst case. If there are no separating edges (resp., separating vertices), then
the time is $O(1)$ in the worst case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02916</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02916</id><created>2015-11-09</created><authors><author><keyname>Lin</keyname><forenames>Zhouhan</forenames></author><author><keyname>Chen</keyname><forenames>Yushi</forenames></author><author><keyname>Zhao</keyname><forenames>Xing</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author></authors><title>Spectral-Spatial Classification of Hyperspectral Image Using
  Autoencoders</title><categories>cs.CV cs.AI cs.LG</categories><comments>Accepted as a conference paper at ICICS 2013, an updated version.
  Codes published. 9 pages, 6 figures</comments><doi>10.1109/ICICS.2013.6782778</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral image (HSI) classification is a hot topic in the remote sensing
community. This paper proposes a new framework of spectral-spatial feature
extraction for HSI classification, in which for the first time the concept of
deep learning is introduced. Specifically, the model of autoencoder is
exploited in our framework to extract various kinds of features. First we
verify the eligibility of autoencoder by following classical spectral
information based classification and use autoencoders with different depth to
classify hyperspectral image. Further in the proposed framework, we combine PCA
on spectral dimension and autoencoder on the other two spatial dimensions to
extract spectral-spatial information for classification. The experimental
results show that this framework achieves the highest classification accuracy
among all methods, and outperforms classical classifiers such as SVM and
PCA-based SVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02917</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02917</id><created>2015-11-09</created><authors><author><keyname>Ramanathan</keyname><forenames>Vignesh</forenames></author><author><keyname>Huang</keyname><forenames>Jonathan</forenames></author><author><keyname>Abu-El-Haija</keyname><forenames>Sami</forenames></author><author><keyname>Gorban</keyname><forenames>Alexander</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Detecting events and key actors in multi-person videos</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-person event recognition is a challenging task, often with many people
active in the scene but only a small subset contributing to an actual event. In
this paper, we propose a model which learns to detect events in such videos
while automatically &quot;attending&quot; to the people responsible for the event. Our
model does not use explicit annotations regarding who or where those people are
during training and testing. In particular, we track people in videos and use a
recurrent neural network (RNN) to represent the track features. We learn
time-varying attention weights to combine these features at each time-instant.
The attended features are then processed using another RNN for event
detection/classification. Since most video datasets with multiple people are
restricted to a small number of videos, we also collected a new basketball
dataset comprising 257 basketball games with 14K event annotations
corresponding to 11 event classes. Our model outperforms state-of-the-art
methods for both event classification and detection on this new dataset.
Additionally, we show that the attention mechanism is able to consistently
localize the relevant players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02919</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02919</id><created>2015-11-09</created><authors><author><keyname>Ghadiyaram</keyname><forenames>Deepti</forenames></author><author><keyname>Bovik</keyname><forenames>Alan C.</forenames></author></authors><title>Massive Online Crowdsourced Study of Subjective and Objective Picture
  Quality</title><categories>cs.CV</categories><comments>16 pages</comments><doi>10.1109/TIP.2015.2500021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most publicly available image quality databases have been created under
highly controlled conditions by introducing graded simulated distortions onto
high-quality photographs. However, images captured using typical real-world
mobile camera devices are usually afflicted by complex mixtures of multiple
distortions, which are not necessarily well-modeled by the synthetic
distortions found in existing databases. The originators of existing legacy
databases usually conducted human psychometric studies to obtain statistically
meaningful sets of human opinion scores on images in a stringently controlled
visual environment, resulting in small data collections relative to other kinds
of image analysis databases. Towards overcoming these limitations, we designed
and created a new database that we call the LIVE In the Wild Image Quality
Challenge Database, which contains widely diverse authentic image distortions
on a large number of images captured using a representative variety of modern
mobile devices. We also designed and implemented a new online crowdsourcing
system, which we have used to conduct a very large-scale, multi-month image
quality assessment subjective study. Our database consists of over 350000
opinion scores on 1162 images evaluated by over 7000 unique human observers.
Despite the lack of control over the experimental environments of the numerous
study participants, we demonstrate excellent internal consistency of the
subjective dataset. We also evaluate several top-performing blind Image Quality
Assessment algorithms on it and present insights on how mixtures of distortions
challenge both end users as well as automatic perceptual quality prediction
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02920</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02920</id><created>2015-11-09</created><updated>2015-11-21</updated><authors><author><keyname>Lucyshyn-Wright</keyname><forenames>Rory B. B.</forenames></author></authors><title>Enriched algebraic theories and monads for a system of arities</title><categories>math.CT cs.LO math.LO</categories><msc-class>18C10, 18C15, 18C20, 18C05, 18D20, 18D15, 08A99, 08B99, 08B20,
  08C05, 08C99, 03C05, 18D35, 18D10, 18D25, 18A35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under a minimum of assumptions, we develop in generality the basic theory of
universal algebra in a symmetric monoidal closed category $\mathcal{V}$ with
respect to a specified system of arities $j:\mathcal{J} \hookrightarrow
\mathcal{V}$. Lawvere's notion of algebraic theory generalizes to this context,
resulting in the notion of single-sorted $\mathcal{V}$-enriched
$\mathcal{J}$-cotensor theory, or $\mathcal{J}$-theory for short. For suitable
choices of $\mathcal{V}$ and $\mathcal{J}$, such $\mathcal{J}$-theories include
the enriched algebraic theories of Borceux and Day, the enriched Lawvere
theories of Power, the equational theories of Linton's 1965 work, and the
$\mathcal{V}$-theories of Dubuc, which are recovered by taking $\mathcal{J} =
\mathcal{V}$ and correspond to arbitrary $\mathcal{V}$-monads on $\mathcal{V}$.
We identify a modest condition on $j$ that entails that the
$\mathcal{V}$-category of $\mathcal{T}$-algebras exists and is monadic over
$\mathcal{V}$ for every $\mathcal{J}$-theory $\mathcal{T}$, even when
$\mathcal{T}$ is not small and $\mathcal{V}$ is neither complete nor
cocomplete. We show that $j$ satisfies this condition if and only if $j$
presents $\mathcal{V}$ as a free cocompletion of $\mathcal{J}$ with respect to
the weights for left Kan extensions along $j$, and so we call such systems of
arities eleutheric. We show that $\mathcal{J}$-theories for an eleutheric
system may be equivalently described as (i) monads in a certain one-object
bicategory of profunctors on $\mathcal{J}$, and (ii) $\mathcal{V}$-monads on
$\mathcal{V}$ satisfying a certain condition. We prove a characterization
theorem for the categories of algebras of $\mathcal{J}$-theories, considered as
$\mathcal{V}$-categories $\mathcal{A}$ equipped with a specified
$\mathcal{V}$-functor $\mathcal{A} \rightarrow \mathcal{V}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02924</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02924</id><created>2015-11-09</created><updated>2015-11-11</updated><authors><author><keyname>Bao</keyname><forenames>Jingjun</forenames></author><author><keyname>Ji</keyname><forenames>Lijun</forenames></author></authors><title>Frequency hopping sequences with optimal partial Hamming correlation</title><categories>cs.IT cs.DM math.IT</categories><comments>16 pages. arXiv admin note: text overlap with arXiv:1506.07372</comments><msc-class>94A55, 94B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequency hopping sequences (FHSs) with favorable partial Hamming correlation
properties have important applications in many synchronization and
multiple-access systems. In this paper, we investigate constructions of FHSs
and FHS sets with optimal partial Hamming correlation. We first establish a
correspondence between FHS sets with optimal partial Hamming correlation and
multiple partition-type balanced nested cyclic difference packings with a
special property. By virtue of this correspondence, some FHSs and FHS sets with
optimal partial Hamming correlation are constructed from various combinatorial
structures such as cyclic difference packings, and cyclic relative difference
families. We also describe a direct construction and two recursive
constructions for FHS sets with optimal partial Hamming correlation. As a
consequence, our constructions yield new FHSs and FHS sets with optimal partial
Hamming correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02927</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02927</id><created>2015-11-09</created><updated>2015-12-01</updated><authors><author><keyname>B&#xfc;rgisser</keyname><forenames>Peter</forenames></author><author><keyname>Ikenmeyer</keyname><forenames>Christian</forenames></author></authors><title>Fundamental invariants of orbit closures</title><categories>math.AG cs.CC</categories><comments>37 pages</comments><msc-class>68Q17, 13A50, 14L24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For several objects of interest in geometric complexity theory, namely for
the determinant, the permanent, the product of variables, the power sum, the
unit tensor, and the matrix multiplication tensor, we introduce and study a
fundamental SL-invariant function that relates the coordinate ring of the orbit
with the coordinate ring of its closure. For the power sums we can write down
this fundamental invariant explicitly in most cases. Our constructions
generalize the two Aronhold invariants on ternary cubics. For the other objects
we identify the invariant function conditional on intriguing combinatorial
problems much like the well-known Alon-Tarsi conjecture on Latin squares. We
provide computer calculations in small dimensions for these cases. As a main
tool for our analysis, we determine the stabilizers, and we establish the
polystability of all the mentioned forms and tensors (including the generic
ones).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02928</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02928</id><created>2015-11-09</created><authors><author><keyname>Arablouei</keyname><forenames>Reza</forenames></author><author><keyname>de Hoog</keyname><forenames>Frank</forenames></author></authors><title>Hyperspectral Image Recovery from Incomplete and Imperfect Measurements
  via Hybrid Regularization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural images tend to mostly consist of smooth regions with individual
pixels having highly correlated spectra. This information can be exploited to
recover hyperspectral images of natural scenes from their incomplete and noisy
measurements. To perform the recovery while taking full advantage of the prior
knowledge, we formulate a composite cost function containing a square-error
data-fitting term and two distinct regularization terms pertaining to spatial
and spectral domains. The regularization for the spatial domain is the sum of
total-variation of the image frames corresponding to all spectral bands. The
regularization for the spectral domain is the l_1-norm of the coefficient
matrix obtained by applying a suitable sparsifying transform to the spectra of
the pixels. We use an accelerated proximal-subgradient method to minimize the
formulated cost function. We analyse the performance of the proposed algorithm
and prove its convergence. Numerical simulations using real hyperspectral
images exhibit that the proposed algorithm offers an excellent recovery
performance with a number of measurements that is only a small fraction of the
hyperspectral image data size. Simulation results also show that the proposed
algorithm significantly outperforms an accelerated proximal-gradient algorithm
that solves the classical basis-pursuit denoising problem to recover the
hyperspectral image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02930</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02930</id><created>2015-11-09</created><authors><author><keyname>Karwa</keyname><forenames>Vishesh</forenames></author><author><keyname>Krivitsky</keyname><forenames>Pavel N.</forenames></author><author><keyname>Slavkovi&#x107;</keyname><forenames>Aleksandra B.</forenames></author></authors><title>Sharing Social Network Data: Differentially Private Estimation of
  Exponential-Family Random Graph Models</title><categories>stat.CO cs.CR cs.SI stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by a real-life problem of sharing social network data that contain
sensitive personal information, we propose a novel approach to release and
analyze synthetic graphs in order to protect privacy of individual
relationships captured by the social network while maintaining the validity of
statistical results. Two case studies demonstrate the application and
usefulness of the proposed techniques in solving the challenging problem of
maintaining privacy \emph{and} supporting open access to network data to ensure
reproducibility of existing studies and discovering new scientific insights
that can be obtained by analyzing such data. We use a simple yet effective
randomized response mechanism to generate synthetic networks under
$\epsilon$-edge differential privacy. We combine ideas and methods from both
the statistics and the computer sciences, by utilizing likelihood based
inference for missing data and Markov chain Monte Carlo (MCMC) techniques to
fit exponential-family random graph models (ERGMs) to the generated synthetic
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02937</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02937</id><created>2015-11-09</created><authors><author><keyname>Va</keyname><forenames>Vutha</forenames></author><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>The Impact of Beamwidth on Temporal Channel Variation in Vehicular
  Channels and its Implications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) has great potential in realizing high data rate
thanks to the large spectral channels. It is considered as a key technology for
the fifth generation wireless networks and is already used in wireless LAN
(e.g., IEEE 802.11ad). Using mmWave for vehicular communications, however, is
often viewed with some skepticism due to a misconception that the Doppler
spread would become too large at these high frequencies. This is not true when
directional beam is employed for communications. In this paper, closed form
expressions relating the channel coherence time and beamwidth are derived.
Unlike prior work that assumed perfect beam pointing, the pointing error due to
the receiver motion is incorporated to show that there exists a non-zero
optimal beamwidth that maximizes the coherence time. To investigate the
mobility effect on the beam alignment which is an important feature in mmWave
systems, a novel concept of beam coherence time is defined. The beam coherence
time, which is an effective measure of beam alignment frequency, is shown to be
much larger than the conventional channel coherence time and thus results in
reduced beam alignment overhead. Using the derived correlation function, the
channel coherence time, and the beam coherence time, an overall performance
metric considering both the channel time-variation and the beam alignment
overhead is derived. Using this metric, it is shown that beam alignment in
every beam coherence time performs better than the beam alignment in every
channel coherence time due to the large overhead for the latter case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02942</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02942</id><created>2015-11-09</created><updated>2015-11-28</updated><authors><author><keyname>Zeng</keyname><forenames>Jinshan</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author></authors><title>ExtraPush for Convex Smooth Decentralized Optimization over Directed
  Networks</title><categories>math.OC cs.DC cs.RO</categories><comments>11 pages, 1 figure</comments><report-no>UCLA CAM Report 15-61, 2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we extend the existing algorithms Extra and subgradient-push to
a new algorithm called ExtraPush for convex consensus optimization over a
directed network. When the network is stationary, we propose a simplified
algorithm called Normalized ExtraPush. These algorithms use a fixed step size
like in Extra and accept the column-stochastic mixing matrices like in
subgradient-push. We present preliminary analysis for ExtraPush under a bounded
sequence assumption. For Normalized ExtraPush, we show that it naturally
produces a bounded, linearly convergent sequence provided that the objective
function is strongly convex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02954</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02954</id><created>2015-11-09</created><updated>2016-01-03</updated><authors><author><keyname>Miranda</keyname><forenames>Conrado S.</forenames></author><author><keyname>Von Zuben</keyname><forenames>Fernando J.</forenames></author></authors><title>Reducing the Training Time of Neural Networks by Partitioning</title><categories>cs.NE cs.LG</categories><comments>Figure 2b has lower quality due to file size constraints</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new method for pre-training neural networks that can
decrease the total training time for a neural network while maintaining the
final performance, which motivates its use on deep neural networks. By
partitioning the training task in multiple training subtasks with sub-models,
which can be performed independently and in parallel, it is shown that the size
of the sub-models reduces almost quadratically with the number of subtasks
created, quickly scaling down the sub-models used for the pre-training. The
sub-models are then merged to provide a pre-trained initial set of weights for
the original model. The proposed method is independent of the other aspects of
the training, such as architecture of the neural network, training method, and
objective, making it compatible with a wide range of existing approaches. The
speedup without loss of performance is validated experimentally on MNIST and on
CIFAR10 data sets, also showing that even performing the subtasks sequentially
can decrease the training time. Moreover, we show that larger models may
present higher speedups and conjecture about the benefits of the method in
distributed learning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02956</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02956</id><created>2015-11-09</created><authors><author><keyname>Chevalier-Boisvert</keyname><forenames>Maxime</forenames></author><author><keyname>Feeley</keyname><forenames>Marc</forenames></author></authors><title>Interprocedural Type Specialization of JavaScript Programs Without Type
  Analysis</title><categories>cs.PL</categories><comments>10 pages, 10 figures, submitted to CGO 2016</comments><acm-class>D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamically typed programming languages such as Python and JavaScript defer
type checking to run time. VM implementations can improve performance by
eliminating redundant dynamic type checks. However, type inference analyses are
often costly and involve tradeoffs between compilation time and resulting
precision. This has lead to the creation of increasingly complex multi-tiered
VM architectures.
  Lazy basic block versioning is a simple JIT compilation technique which
effectively removes redundant type checks from critical code paths. This novel
approach lazily generates type-specialized versions of basic blocks on-the-fly
while propagating context-dependent type information. This approach does not
require the use of costly program analyses, is not restricted by the precision
limitations of traditional type analyses.
  This paper extends lazy basic block versioning to propagate type information
interprocedurally, across function call boundaries. Our implementation in a
JavaScript JIT compiler shows that across 26 benchmarks, interprocedural basic
block versioning eliminates more type tag tests on average than what is
achievable with static type analysis without resorting to code transformations.
On average, 94.3% of type tag tests are eliminated, yielding speedups of up to
56%. We also show that our implementation is able to outperform Truffle/JS on
several benchmarks, both in terms of execution time and compilation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02960</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02960</id><created>2015-11-09</created><authors><author><keyname>Han</keyname><forenames>Rui</forenames></author><author><keyname>Wang</keyname><forenames>Junwei</forenames></author><author><keyname>Huang</keyname><forenames>Siguang</forenames></author><author><keyname>Shao</keyname><forenames>Chenrong</forenames></author><author><keyname>Zhan</keyname><forenames>Shulin</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Vazquez-Poletti</keyname><forenames>Jose Luis</forenames></author></authors><title>PCS: Predictive Component-level Scheduling for Reducing Tail Latency in
  Cloud Online Services</title><categories>cs.DC</categories><comments>10 pages, 9 figures, ICPP conference</comments><doi>10.1109/ICPP.2015.58</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern latency-critical online services often rely on composing results from
a large number of server components. Hence the tail latency (e.g. the 99th
percentile of response time), rather than the average, of these components
determines the overall service performance. When hosted on a cloud environment,
the components of a service typically co-locate with short batch jobs to
increase machine utilizations, and share and contend resources such as caches
and I/O bandwidths with them. The highly dynamic nature of batch jobs in terms
of their workload types and input sizes causes continuously changing
performance interference to individual components, hence leading to their
latency variability and high tail latency. However, existing techniques either
ignore such fine-grained component latency variability when managing service
performance, or rely on executing redundant requests to reduce the tail
latency, which adversely deteriorate the service performance when load gets
heavier. In this paper, we propose PCS, a predictive and component-level
scheduling framework to reduce tail latency for large-scale, parallel online
services. It uses an analytical performance model to simultaneously predict the
component latency and the overall service performance on different nodes. Based
on the predicted performance, the scheduler identifies straggling components
and conducts near-optimal component-node allocations to adapt to the changing
performance interferences from batch jobs. We demonstrate that, using realistic
workloads, the proposed scheduler reduces the component tail latency by an
average of 67.05\% and the average overall service latency by 64.16\% compared
with the state-of-the-art techniques on reducing tail latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02963</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02963</id><created>2015-11-09</created><authors><author><keyname>Pequito</keyname><forenames>Sergio</forenames></author><author><keyname>Khorrami</keyname><forenames>Farshad</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Prashanth</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Analysis and Design of Secured/Resilient Closed-loop Control Systems</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, structural properties of distributed control systems and
pairing of sensors and actuators are considered to generate architectures which
are resilient to attacks/hacks for industrial control systems and other complex
cyber-physical systems. In particular, we consider inherent structural
properties such as internal fixed modes of a dynamical system depending on
actuation, sensing, and interconnection/communication structure for linear
discrete time-invariant dynamical systems. We aim to attain stability and
performance objectives under disruptive scenarios such as attacks by a
malicious agent on actuators, sensors, and communication components and natural
failures. The main contributions of this paper are fourfold: (i) introduction
of the notion of resilient fixed-modes free system that ensures the
non-existence of fixed modes when the actuation-sensing-communication structure
is compromised; (ii) a graph-theoretical representation that ensures almost
always the non-existence of resilient fixed modes; (iii) a solution to the
problem of designing the minimum actuation-sensing-communication structure that
ensures a system without resilient fixed modes almost always; and (iv)
determine, for a parametrized system, the gain satisfying the sparsity of a
given information pattern by resorting to convex optimization tools. The
different strategies to deploy actuators, sensors, and establishing
communication between these are provided such that the system is resilient
fixed-modes free; in particular, in order to show to show the efficacy of the
proposed methodologies these results are applied in the context of the power
electric grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02986</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02986</id><created>2015-11-09</created><updated>2015-12-18</updated><authors><author><keyname>Yeh</keyname><forenames>Li-Hao</forenames></author><author><keyname>Dong</keyname><forenames>Jonathan</forenames></author><author><keyname>Zhong</keyname><forenames>Jingshan</forenames></author><author><keyname>Tian</keyname><forenames>Lei</forenames></author><author><keyname>Chen</keyname><forenames>Michael</forenames></author><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author><author><keyname>Soltanolkotabi</keyname><forenames>Mahdi</forenames></author><author><keyname>Waller</keyname><forenames>Laura</forenames></author></authors><title>Experimental robustness of Fourier Ptychography phase retrieval
  algorithms</title><categories>physics.optics cs.CV</categories><journal-ref>Opt. Express 23, 33214-33240 (2015)</journal-ref><doi>10.1364/OE.23.033214</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fourier ptychography is a new computational microscopy technique that
provides gigapixel-scale intensity and phase images with both wide
field-of-view and high resolution. By capturing a stack of low-resolution
images under different illumination angles, a nonlinear inverse algorithm can
be used to computationally reconstruct the high-resolution complex field. Here,
we compare and classify multiple proposed inverse algorithms in terms of
experimental robustness. We find that the main sources of error are noise,
aberrations and mis-calibration (i.e. model mis-match). Using simulations and
experiments, we demonstrate that the choice of cost function plays a critical
role, with amplitude-based cost functions performing better than
intensity-based ones. The reason for this is that Fourier ptychography datasets
consist of images from both brightfield and darkfield illumination,
representing a large range of measured intensities. Both noise (e.g. Poisson
noise) and model mis-match errors are shown to scale with intensity. Hence,
algorithms that use an appropriate cost function will be more tolerant to both
noise and model mis-match. Given these insights, we propose a global Newton's
method algorithm which is robust and computationally efficient. Finally, we
discuss the impact of procedures for algorithmic correction of aberrations and
mis-calibration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02989</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02989</id><created>2015-11-09</created><authors><author><keyname>Zeineddine</keyname><forenames>Hady</forenames></author><author><keyname>Mansour</keyname><forenames>Mohammad M.</forenames></author></authors><title>Inter-Frame Coding For Broadcast Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel inter-frame coding approach to the problem of varying channel-state
conditions in broadcast wireless communication is developed in this paper; this
problem causes the appropriate code-rate to vary across different transmitted
frames and different receivers as well. The main aspect of the proposed
approach is that it incorporates an iterative rate-matching process into the
decoding of the received set of frames, such that: throughout inter-frame
decoding, the code-rate of each frame is progressively lowered to or below the
appropriate value, prior to applying or re-applying conventional physical-layer
channel decoding on it. This iterative rate-matching process is asymptotically
analyzed in this paper. It is shown to be optimal, in the sense defined in the
paper. Consequently, the data-rates achievable by the proposed scheme are
derived. Overall, it is concluded that, compared to the existing solutions,
inter-frame coding presents a better complexity versus data-rate tradeoff. In
terms of complexity, the overhead of inter-frame decoding includes operations
that are similar in type and scheduling to those employed in the relatively-
simple iterative erasure decoding. In terms of data-rates, compared to the
state-of-the-art two-stage scheme involving both error-correcting and erasure
coding, inter-frame coding increases the data-rate by a factor that reaches up
to 1.55x.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02992</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02992</id><created>2015-11-10</created><authors><author><keyname>Haloi</keyname><forenames>Mrinal</forenames></author></authors><title>Traffic Sign Classification Using Deep Inception Based Convolutional
  Networks</title><categories>cs.CV</categories><comments>Technical Paper ( 23rd World Congress on Intelligent Transport
  Systems 2016, Submission Version)</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel deep networks for traffic sign
classification that achieves outstanding performance on GTSRB surpassing all
previous methods. Our deep network consists of spatial transformer layers and a
modified version of inception module specifically designed for capturing local
and global features together. This features adoption allows our network to
classify precisely intra class samples even under deformations. Use of spatial
transformer layer make this network more robust to deformations such as
translation, rotation, scaling of input images. Unlike existing approaches that
are developed with hand crafted features, multiple deep networks with huge
parameters and data augmentations, our method addresses the concern of
exploding parameters and augmentations. We have achieved state-of-the-art
performance of 99.81% on GTSRB dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02995</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02995</id><created>2015-11-10</created><updated>2015-11-24</updated><authors><author><keyname>Chen</keyname><forenames>Bryant</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author><author><keyname>Bareinboim</keyname><forenames>Elias</forenames></author></authors><title>Identification by Auxiliary Instrumental Sets in Linear Structural
  Equation Models</title><categories>stat.ME cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend graph-based identification methods for linear models by allowing
background knowledge in the form of externally evaluated parameters. Such
information could be obtained, for example, from a previously conducted
randomized experiment, from substantive understanding of the domain, or even
from another identification technique. To incorporate such information
systematically, we propose the addition of auxiliary variables to the model,
which are constructed so that certain paths will be conveniently cancelled.
This cancellation allows the auxiliary variables to help conventional methods
of identification (e.g., single-door criterion, instrumental variables,
half-trek criterion) and model testing (e.g., d-separation,
over-identification). Moreover, by iteratively alternating steps of
identification and adding auxiliary variables, we can improve the power of
existing identification and model testing methods, even without additional
knowledge. We operationalize this general approach for instrumental sets (a
generalization of instrumental variables) and show that the resulting procedure
subsumes the most general identification method for linear systems known to
date. We further discuss the application of this new operation in the tasks of
model testing and z-identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.02999</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.02999</id><created>2015-11-10</created><authors><author><keyname>Maity</keyname><forenames>Abhishek</forenames></author></authors><title>Improvised Salient Object Detection and Manipulation</title><categories>cs.CV</categories><comments>7 pages</comments><acm-class>I.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In case of salient subject recognition, computer algorithms have been heavily
relied on scanning of images from top-left to bottom-right systematically and
apply brute-force when attempting to locate objects of interest. Thus, the
process turns out to be quite time consuming. Here a novel approach and a
simple solution to the above problem is discussed. In this paper, we implement
an approach to object manipulation and detection through segmentation map,
which would help to desaturate or, in other words, wash out the background of
the image. Evaluation for the performance is carried out using the Jaccard
index against the well-known Ground-truth target box technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03003</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03003</id><created>2015-11-10</created><authors><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author><author><keyname>Patra</keyname><forenames>Manas K.</forenames></author></authors><title>Undecidable Cases of Model Checking Probabilistic Temporal-Epistemic
  Logic</title><categories>cs.LO</categories><comments>This is an extended version, with full proofs, of a paper that
  appeared in TARK 2015. It corrects an error in the TARK 2015 pre-proceedings
  version, in the definition of mixed-time polynomial atomic probability
  formulas</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the decidability of model-checking logics of time, knowledge
and probability, with respect to two epistemic semantics: the clock and
synchronous perfect recall semantics in partially observed discrete-time Markov
chains. Decidability results are known for certain restricted logics with
respect to these semantics, subject to a variety of restrictions that are
either unexplained or involve a longstanding unsolved mathematical problem. We
show that mild generalizations of the known decidable cases suffice to render
the model checking problem definitively undecidable. In particular, for a
synchronous perfect recall, a generalization from temporal operators with
finite reach to operators with infinite reach renders model checking
undecidable. The case of the clock semantics is closely related to a monadic
second order logic of time and probability that is known to be decidable,
except on a set of measure zero. We show that two distinct extensions of this
logic make model checking undecidable. One of these involves polynomial
combinations of probability terms, the other involves monadic second order
quantification into the scope of probability operators. These results explain
some of the restrictions in previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03005</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03005</id><created>2015-11-10</created><authors><author><keyname>Xu</keyname><forenames>Zhiwei</forenames></author><author><keyname>Chen</keyname><forenames>Bo</forenames></author><author><keyname>Wang</keyname><forenames>Ninghan</forenames></author><author><keyname>Zhang</keyname><forenames>Yujun</forenames></author><author><keyname>Li</keyname><forenames>Zhongcheng</forenames></author></authors><title>ELDA: Towards Efficient and Lightweight Detection of Cache Pollution
  Attacks in NDN</title><categories>cs.CR cs.NI cs.PF</categories><comments>A regular paper published in LCN 2015,9 pages,which includes a novel
  lightweight FM sketch for estimating the number of distinct items in data
  streams</comments><acm-class>B.2.4; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a promising architectural design for future Internet, named data
networking (NDN) relies on in-network caching to efficiently deliver name-based
content. However, the in-network caching is vulnerable to cache pollution
attacks (CPA), which can reduce cache hits by violating cache locality and
significantly degrade the overall performance of NDN. To defend against CPA
attacks, the most effective way is to first detect the attacks and then
throttle them. Since the CPA attack itself has already imposed a huge burden on
victims, to avoid exhausting the remaining resources on the victims for
detection purpose, we expect a lightweight detection solution. We thus propose
ELDA, an Efficient and Lightweight Detection scheme against cache pollution
Attacks, in which we design a Lightweight Flajolet-Martin (LFM) sketch to
monitor the interest traffic. Our analysis and simulations demonstrate that, by
consuming a few computation and memory resources, ELDA can effectively and
efficiently detect CPA attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03010</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03010</id><created>2015-11-10</created><authors><author><keyname>Li</keyname><forenames>S.</forenames></author><author><keyname>Dragicevic</keyname><forenames>S.</forenames></author><author><keyname>Anton</keyname><forenames>F.</forenames></author><author><keyname>Sester</keyname><forenames>M.</forenames></author><author><keyname>Winter</keyname><forenames>S.</forenames></author><author><keyname>Coltekin</keyname><forenames>A.</forenames></author><author><keyname>Pettit</keyname><forenames>C.</forenames></author><author><keyname>Jiang</keyname><forenames>B.</forenames></author><author><keyname>Haworth</keyname><forenames>J.</forenames></author><author><keyname>Stein</keyname><forenames>A.</forenames></author><author><keyname>Cheng</keyname><forenames>T.</forenames></author></authors><title>Geospatial Big Data Handling Theory and Methods: A Review and Research
  Challenges</title><categories>physics.soc-ph cs.CY physics.data-an</categories><comments>25 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data has now become a strong focus of global interest that is
increasingly attracting the attention of academia, industry, government and
other organizations. Big data can be situated in the disciplinary area of
traditional geospatial data handling theory and methods. The increasing volume
and varying format of collected geospatial big data presents challenges in
storing, managing, processing, analyzing, visualizing and verifying the quality
of data. This has implications for the quality of decisions made with big data.
Consequently, this position paper of the International Society for
Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II)
revisits the existing geospatial data handling methods and theories to
determine if they are still capable of handling emerging geospatial big data.
Further, the paper synthesises problems, major issues and challenges with
current developments as well as recommending what needs to be developed further
in the near future.
  Keywords: Big data, Geospatial, Data handling, Analytics, Spatial Modeling,
Review
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03012</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03012</id><created>2015-11-10</created><authors><author><keyname>Groza</keyname><forenames>Adrian</forenames></author><author><keyname>Corde</keyname><forenames>Lidia</forenames></author></authors><title>Information retrieval in folktales using natural language processing</title><categories>cs.CL cs.IR</categories><comments>IEEE 11 International Conference on Intelligent Computer
  Communication and Processing (ICCP2015), Cluj-Napoca, Romania, 3-5 September
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our aim is to extract information about literary characters in unstructured
texts. We employ natural language processing and reasoning on domain
ontologies. The first task is to identify the main characters and the parts of
the story where these characters are described or act. We illustrate the system
in a scenario in the folktale domain. The system relies on a folktale ontology
that we have developed based on Propp's model for folktales morphology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03015</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03015</id><created>2015-11-10</created><authors><author><keyname>Li</keyname><forenames>Huibin</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author><author><keyname>Chen</keyname><forenames>Liming</forenames></author></authors><title>Deep Representation of Facial Geometric and Photometric Attributes for
  Automatic 3D Facial Expression Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel approach to automatic 3D Facial Expression
Recognition (FER) based on deep representation of facial 3D geometric and 2D
photometric attributes. A 3D face is firstly represented by its geometric and
photometric attributes, including the geometry map, normal maps, normalized
curvature map and texture map. These maps are then fed into a pre-trained deep
convolutional neural network to generate the deep representation. Then the
facial expression prediction is simplyachieved by training linear SVMs over the
deep representation for different maps and fusing these SVM scores. The
visualizations show that the deep representation provides a complete and highly
discriminative coding scheme for 3D faces. Comprehensive experiments on the
BU-3DFE database demonstrate that the proposed deep representation can
outperform the widely used hand-crafted descriptors (i.e., LBP, SIFT, HOG,
Gabor) and the state-of-art approaches under the same experimental protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03019</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03019</id><created>2015-11-10</created><authors><author><keyname>Martin-Brualla</keyname><forenames>Ricardo</forenames></author><author><keyname>Gallup</keyname><forenames>David</forenames></author><author><keyname>Seitz</keyname><forenames>Steven M.</forenames></author></authors><title>3D Time-lapse Reconstruction from Internet Photos</title><categories>cs.CV</categories><comments>To appear in ICCV'15. Supplementary video at:
  http://grail.cs.washington.edu/projects/timelapse3d/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an Internet photo collection of a landmark, we compute a 3D time-lapse
video sequence where a virtual camera moves continuously in time and space.
While previous work assumed a static camera, the addition of camera motion
during the time-lapse creates a very compelling impression of parallax.
Achieving this goal, however, requires addressing multiple technical
challenges, including solving for time-varying depth maps, regularizing 3D
point color profiles over time, and reconstructing high quality, hole-free
images at every frame from the projected profiles. Our results show
photorealistic time-lapses of skylines and natural scenes over many years, with
dramatic parallax effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03020</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03020</id><created>2015-11-10</created><updated>2016-02-26</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Nerghes</keyname><forenames>Adina</forenames></author></authors><title>Co-word Maps and Topic Modeling: A Comparison Using Small and
  Medium-Sized Corpora (n &lt; 1000)</title><categories>cs.DL</categories><comments>Journal of the Association for Information Science and Technology
  (JASIST, in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Induced by &quot;big data,&quot; &quot;topic modeling&quot; has become an attractive alternative
to mapping co-words in terms of co-occurrences and co-absences using network
techniques. Does topic modeling provide an alternative for co-word mapping in
research practices using moderately sized document collections? We return to
the word/document matrix using first a single text with a strong argument (&quot;The
Leiden Manifesto&quot;) and then upscale to a sample of moderate size (n = 687) to
study the pros and cons of the two approaches in terms of the resulting
possibilities for making semantic maps that can serve an argument. The results
from co-word mapping (using two different routines) versus topic modeling are
significantly uncorrelated. Whereas components in the co-word maps can easily
be designated, the topic models provide sets of words that are very differently
organized. In these samples, the topic models seem to reveal similarities other
than semantic ones (e.g., linguistic ones). In other words, topic modeling does
not replace co-word mapping in small and medium-sized sets; but the paper
leaves open the possibility that topic modeling would work well for the
semantic mapping of large sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03028</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03028</id><created>2015-11-10</created><authors><author><keyname>Tang</keyname><forenames>Chang</forenames></author><author><keyname>Li</keyname><forenames>Wanqing</forenames></author><author><keyname>Hou</keyname><forenames>Chunping</forenames></author><author><keyname>Wang</keyname><forenames>Pichao</forenames></author><author><keyname>Hou</keyname><forenames>Yonghong</forenames></author><author><keyname>Zhang</keyname><forenames>Jing</forenames></author><author><keyname>Ogunbona</keyname><forenames>Philip O.</forenames></author></authors><title>Online Action Recognition based on Incremental Learning of Weighted
  Covariance Descriptors</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Online action recognition aims to recognize actions from unsegmented streams
of data in a continuous manner. One of the challenges in online recognition is
the accumulation of evidence for decision making. This paper presents a fast
and efficient online method to recognize actions from a stream of noisy
skeleton data. The method adopts a covariance descriptor calculated from
skeleton data and is based on a novel method developed for incrementally
learning the covariance descriptors, referred to as weighted covariance
descriptors, so that past frames have less contributions to the descriptor and
current frames and informative frames such as key frames contributes more
towards the descriptor. The online recognition is achieved using an efficient
nearest neighbour search against a set of trained actions. Experimental results
on MSRC-12 Kinect Gesture dataset and our newly collocated online action
recognition dataset have demonstrated the efficacy of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03034</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03034</id><created>2015-11-10</created><updated>2016-01-15</updated><authors><author><keyname>Huang</keyname><forenames>Ruitong</forenames></author><author><keyname>Xu</keyname><forenames>Bing</forenames></author><author><keyname>Schuurmans</keyname><forenames>Dale</forenames></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames></author></authors><title>Learning with a Strong Adversary</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robustness of neural networks to intended perturbations has recently
attracted significant attention. In this paper, we propose a new method,
\emph{learning with a strong adversary}, that learns robust classifiers from
supervised data. The proposed method takes finding adversarial examples as an
intermediate step. A new and simple way of finding adversarial examples is
presented and experimentally shown to be efficient. Experimental results
demonstrate that resulting learning method greatly improves the robustness of
the classification models produced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03036</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03036</id><created>2015-11-10</created><authors><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>Depraetere</keyname><forenames>Kristof</forenames></author><author><keyname>De Roo</keyname><forenames>Jos</forenames></author><author><keyname>Mels</keyname><forenames>Giovanni</forenames></author><author><keyname>De Vloed</keyname><forenames>Boris</forenames></author><author><keyname>Twagirumukiza</keyname><forenames>Marc</forenames></author><author><keyname>Colaert</keyname><forenames>Dirk</forenames></author></authors><title>Semantic processing of EHR data for clinical research</title><categories>cs.DB cs.IR</categories><comments>Accepted for publication in Journal of Biomedical Informatics, 2015,
  preprint version</comments><doi>10.1016/j.jbi.2015.10.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing need to semantically process and integrate clinical data
from different sources for clinical research. This paper presents an approach
to integrate EHRs from heterogeneous resources and generate integrated data in
different data formats or semantics to support various clinical research
applications. The proposed approach builds semantic data virtualization layers
on top of data sources, which generate data in the requested semantics or
formats on demand. This approach avoids upfront dumping to and synchronizing of
the data with various representations. Data from different EHR systems are
first mapped to RDF data with source semantics, and then converted to
representations with harmonized domain semantics where domain ontologies and
terminologies are used to improve reusability. It is also possible to further
convert data to application semantics and store the converted results in
clinical research databases, e.g. i2b2, OMOP, to support different clinical
research settings. Semantic conversions between different representations are
explicitly expressed using N3 rules and executed by an N3 Reasoner (EYE), which
can also generate proofs of the conversion processes. The solution presented in
this paper has been applied to real-world applications that process large scale
EHR data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03039</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03039</id><created>2015-11-10</created><authors><author><keyname>Salahat</keyname><forenames>Ehab</forenames></author></authors><title>Maximal Ratio Combining Analysis of the {\eta}_-_{\mu} Generalized
  Fading Channels with Integer {\mu}</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel performance analysis of the
{\eta}_-_{\mu} generalized radio fading channels with integer value of the
{\mu} fading parameter, i.e. with even number of multipath clusters. This
fading model includes the other fading models as special cases such as the
Nakagami-m, the Hoyt, and the Rayleigh. We obtain novel unified and generic
simple closed-form expressions for the Average Bit Error Rates and Ergodic
Channel Capacity in the Additive White Generalized Gaussian Noise (AWGGN),
which includes the AWGN, the Laplacian, the Gamma, and the impulsive noise as
special cases, deploying Maximal Ratio Combing Diversity reception. Numerical
simulation results corroborate the generality and accuracy of the derived
unified expressions under the different test scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03042</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03042</id><created>2015-11-10</created><updated>2015-11-16</updated><authors><author><keyname>Heravi</keyname><forenames>Elnaz J.</forenames></author><author><keyname>Aghdam</keyname><forenames>Hamed H.</forenames></author><author><keyname>Puig</keyname><forenames>Domenec</forenames></author></authors><title>Analyzing Stability of Convolutional Neural Networks in the Frequency
  Domain</title><categories>cs.CV</categories><comments>Under review as a conference paper at ICLR2016, minor changes in the
  text</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the internal process of ConvNets is commonly done using
visualization techniques. However, these techniques do not usually provide a
tool for estimating the stability of a ConvNet against noise. In this paper, we
show how to analyze a ConvNet in the frequency domain using a 4-dimensional
visualization technique. Using the frequency domain analysis, we show the
reason that a ConvNet might be sensitive to a very low magnitude additive
noise. Our experiments on a few ConvNets trained on different datasets revealed
that convolution kernels of a trained ConvNet usually pass most of the
frequencies and they are not able to effectively eliminate the effect of high
frequencies. Our next experiments shows that a convolution kernel which has a
more concentrated frequency response could be more stable. Finally, we show
that fine-tuning a ConvNet using a training set augmented with noisy images can
produce more stable ConvNets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03043</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03043</id><created>2015-11-10</created><authors><author><keyname>Kenyon</keyname><forenames>Anne</forenames></author><author><keyname>Tassy</keyname><forenames>Martin</forenames></author></authors><title>Tiling with Small Tiles</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We look at sets of tiles that can tile any region of size greater than 1 on
the square grid. This is not the typical tiling question, but relates closely
to it and therefore can help solve other tiling problems -- we give an example
of this. We also present a result to a more classic tiling question with
dominoes and L-shape tiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03053</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03053</id><created>2015-11-10</created><authors><author><keyname>Mpouli</keyname><forenames>Suzanne</forenames><affiliation>ACASA</affiliation></author><author><keyname>Ganascia</keyname><forenames>Jean-Gabriel</forenames><affiliation>ACASA</affiliation></author></authors><title>Investigating the stylistic relevance of adjective and verb simile
  markers</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>Corpus Linguistics 2015, Jul 2015, Lancaster, United Kingdom</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Similes play an important role in literary texts not only as rhetorical
devices and as figures of speech but also because of their evocative power,
their aptness for description and the relative ease with which they can be
combined with other figures of speech (Israel et al. 2004). Detecting all types
of simile constructions in a particular text therefore seems crucial when
analysing the style of an author. Few research studies however have been
dedicated to the study of less prominent simile markers in fictional prose and
their relevance for stylistic studies. The present paper studies the frequency
of adjective and verb simile markers in a corpus of British and French novels
in order to determine which ones are really informative and worth including in
a stylistic analysis. Furthermore, are those adjectives and verb simile markers
used differently in both languages?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03055</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03055</id><created>2015-11-10</created><authors><author><keyname>Lin</keyname><forenames>Jie</forenames></author><author><keyname>Mor&#xe8;re</keyname><forenames>Olivier</forenames></author><author><keyname>Petta</keyname><forenames>Julie</forenames></author><author><keyname>Chandrasekhar</keyname><forenames>Vijay</forenames></author><author><keyname>Veillard</keyname><forenames>Antoine</forenames></author></authors><title>Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing</title><categories>cs.IR cs.CV cs.LG</categories><msc-class>68P20</msc-class><acm-class>H.3.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A typical image retrieval pipeline starts with the comparison of global
descriptors from a large database to find a short list of candidate matches. A
good image descriptor is key to the retrieval pipeline and should reconcile two
contradictory requirements: providing recall rates as high as possible and
being as compact as possible for fast matching. Following the recent successes
of Deep Convolutional Neural Networks (DCNN) for large scale image
classification, descriptors extracted from DCNNs are increasingly used in place
of the traditional hand crafted descriptors such as Fisher Vectors (FV) with
better retrieval performances. Nevertheless, the dimensionality of a typical
DCNN descriptor --extracted either from the visual feature pyramid or the
fully-connected layers-- remains quite high at several thousands of scalar
values. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully
unsupervised method to compute extremely compact binary hashes --in the 32-256
bits range-- from high-dimensional global descriptors. UTH consists of two
successive deep learning steps. First, Stacked Restricted Boltzmann Machines
(SRBM), a type of unsupervised deep neural nets, are used to learn binary
embedding functions able to bring the descriptor size down to the desired
bitrate. SRBMs are typically able to ensure a very high compression rate at the
expense of loosing some desirable metric properties of the original DCNN
descriptor space. Then, triplet networks, a rank learning scheme based on
weight sharing nets is used to fine-tune the binary embedding functions to
retain as much as possible of the useful metric properties of the original
space. A thorough empirical evaluation conducted on multiple publicly available
dataset using DCNN descriptors shows that our method is able to significantly
outperform state-of-the-art unsupervised schemes in the target bit range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03085</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03085</id><created>2015-11-10</created><authors><author><keyname>Kimble</keyname><forenames>Chris</forenames></author><author><keyname>Milolidakis</keyname><forenames>Giannis</forenames></author></authors><title>Big Data and Business Intelligence: Debunking the Myths</title><categories>cs.CY</categories><journal-ref>Global Business and Organizational Excellence, 35(1), 23-34 (2015)</journal-ref><doi>10.1002/joe.21642</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data is one of the most discussed, and possibly least understood, terms
in use in business today. Big data is said to offer not only unprecedented
levels of business intelligence concerning the habits of consumers and rivals,
but also to herald a revolution in the way in which business are organized and
run. However, big data is not as straightforward as it might seem, particularly
when it comes to the so-called dark data from social media. It is not simply
the quantity of data that has changed, it is also the speed and the variety of
formats with which it is delivered. This article sets out to look at big data
and debunk some of the myths that surround it. It focuses on the role of data
from social media in particular and highlights two common myths about big data.
The first is that because a data set contains billions of items, traditional
methodological issues no longer matter. The second is the belief that big data
is both a complete and unbiased source of data upon which to base decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03086</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03086</id><created>2015-11-10</created><authors><author><keyname>Motl</keyname><forenames>Jan</forenames></author><author><keyname>Schulte</keyname><forenames>Oliver</forenames></author></authors><title>The CTU Prague Relational Learning Repository</title><categories>cs.LG cs.DB</categories><comments>7 pages</comments><acm-class>I.2.6; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the CTU Prague Relational Learning Repository is to support
machine learning research with multi-relational data. The repository currently
contains 50 SQL databases hosted on a public MySQL server located at
relational.fit.cvut.cz. A searchable meta-database provides metadata (e.g., the
number of tables in the database, the number of rows and columns in the tables,
the number of foreign key constraints between tables).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03088</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03088</id><created>2015-11-10</created><authors><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Augenstein</keyname><forenames>Isabelle</forenames></author><author><keyname>Bontcheva</keyname><forenames>Kalina</forenames></author></authors><title>USFD: Twitter NER with Drift Compensation and Linked Data</title><categories>cs.CL</categories><comments>Paper in ACL anthology:
  https://aclweb.org/anthology/W/W15/W15-4306.bib</comments><journal-ref>Proceedings of the ACL Workshop on Noisy User-generated Text
  (2015), pp. 48--53</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a pilot NER system for Twitter, comprising the USFD
system entry to the W-NUT 2015 NER shared task. The goal is to correctly label
entities in a tweet dataset, using an inventory of ten types. We employ
structured learning, drawing on gazetteers taken from Linked Data, and on
unsupervised clustering features, and attempting to compensate for stylistic
and topic drift - a key challenge in social media text. Our result is
competitive; we provide an analysis of the components of our methodology, and
an examination of the target dataset in the context of this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03100</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03100</id><created>2015-11-10</created><authors><author><keyname>Apollonio</keyname><forenames>Nicola</forenames></author><author><keyname>Caramia</keyname><forenames>Massimiliano</forenames></author><author><keyname>Franciosa</keyname><forenames>Paolo Giulio</forenames></author><author><keyname>Mascari</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>A tight relation between series-parallel graphs and Bipartite Distance
  Hereditary graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bandelt and Mulder's structural characterization of Bipartite Distance
Hereditary graphs asserts that such graphs can be built inductively starting
from a single vertex and by repeatedly adding either pending vertices or twins
(i.e., vertices with the same neighborhood as an existing one). Dirac and
Duffin's structural characterization of 2-connected series-parallel graphs
asserts that such graphs can be built inductively starting from a single edge
by adding either edges in series or in parallel. In this paper we prove that
the two constructions are the same construction when bipartite graphs are
viewed as the fundamental graphs of a graphic matroid. We then apply the result
to re-prove known results concerning bipartite distance hereditary graphs and
series-parallel graphs, to characterize self-dual outer-planar graphs and,
finally, to provide a new class of polynomially-solvable instances for the
integer multi commodity flow of maximum value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03108</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03108</id><created>2015-11-10</created><updated>2015-11-10</updated><authors><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Jiao</keyname><forenames>Chunxu</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author></authors><title>Differential Modulation Exploiting the Spatial-Temporal Correlation of
  Wireless Channels With Moving Antenna Array</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Communications</comments><doi>10.1109/TCOMM.2015.2495280</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provisioning reliable wireless services for railway passengers is becoming an
increasingly critical problem to be addressed with the fast development of high
speed trains (HST). In this paper, exploiting the linear mobility inherent to
the HST communication scenario, we discover a new type of spatial-temporal
correlation between the base station and moving antenna array on the roof top
of the train. Capitalizing on the new spatial-temporal correlation structure
and properties, an improved differential space-time modulation (DSTM) scheme is
proposed. Analytical expressions are obtained for the pairwise error
probability of the system. It is demonstrated that, the proposed approach
achieves superior error performance compared with the conventional DSTM scheme.
In addition, an adaptive method which dynamically adjusts the transmission
block length is proposed to further enhance the system performance. Numerical
results are provided to verify the performance of the proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03124</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03124</id><created>2015-11-10</created><authors><author><keyname>Boiten</keyname><forenames>Eerke</forenames></author></authors><title>Diversity and Adjudication</title><categories>cs.SE</categories><doi>10.1016/j.jlamp.2015.10.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper takes an axiomatic and calculational view of diversity (or
&quot;N-version programming&quot;), where multiple implementations of the same
specification are executed in parallel to increase dependability. The central
notion is &quot;adjudication&quot;: once we have multiple, potential different, outcomes,
how do we come to a single result? Adjudication operators are explicitly
defined and some general properties for these explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03125</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03125</id><created>2015-11-10</created><authors><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Wu</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author><author><keyname>Dai</keyname><forenames>Huaiyu</forenames></author><author><keyname>Kato</keyname><forenames>Nei</forenames></author></authors><title>Virtual-MIMO-Boosted Information Propagation on Highways</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2015.2489661</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In vehicular communications, traffic-related information should be spread
over the network as quickly as possible to maintain a safer transportation
system. This motivates us to develop more efficient information propagation
schemes. In this paper, we propose a novel virtual-MIMO-enabled information
dissemination scheme, in which the vehicles opportunistically form virtual
antenna arrays to boost the transmission range and therefore accelerate
information propagation along the highway. We model the information propagation
process as a renewal reward process and investigate in detail the
\emph{Information Propagation Speed} (IPS) of the proposed scheme. The
corresponding closed-form IPS is derived, which shows that the IPS increases
cubically with the vehicle density but will ultimately converge to a constant
upper bound. Moreover, increased mobility also facilitates the information
spreading by offering more communication opportunities. However, the limited
network density essentially determines the bottleneck in information spreading.
Extensive simulations are carried out to verify our analysis. We also show that
the proposed scheme exhibits a significant IPS gain over its conventional
counterpart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03137</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03137</id><created>2015-11-10</created><authors><author><keyname>Schlag</keyname><forenames>Sebastian</forenames></author><author><keyname>Henne</keyname><forenames>Vitali</forenames></author><author><keyname>Heuer</keyname><forenames>Tobias</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>k-way Hypergraph Partitioning via n-Level Recursive Bisection</title><categories>cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1505.00693</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a multilevel algorithm for hypergraph partitioning that contracts
the vertices one at a time. Using several caching and lazy-evaluation
techniques during coarsening and refinement, we reduce the running time by up
to two-orders of magnitude compared to a naive $n$-level algorithm that would
be adequate for ordinary graph partitioning. The overall performance is even
better than the widely used hMetis hypergraph partitioner that uses a classical
multilevel algorithm with few levels. Aided by a portfolio-based approach to
initial partitioning and adaptive budgeting of imbalance within recursive
bipartitioning, we achieve very high quality. We assembled a large benchmark
set with 310 hypergraphs stemming from application areas such VLSI, SAT
solving, social networks, and scientific computing. We achieve significantly
smaller cuts than hMetis and PaToH, while being faster than hMetis.
Considerably larger improvements are observed for some instance classes like
social networks, for bipartitioning, and for partitions with an allowed
imbalance of 10%. The algorithm presented in this work forms the basis of our
hypergraph partitioning framework KaHyPar (Karlsruhe Hypergraph Partitioning).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03144</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03144</id><created>2015-11-10</created><authors><author><keyname>Tsiligkaridis</keyname><forenames>Theodoros</forenames></author></authors><title>Asynchronous Decentralized 20 Questions for Adaptive Search</title><categories>cs.MA cs.IT cs.SY math.IT stat.ML</categories><comments>11 pages, Submitted. arXiv admin note: substantial text overlap with
  arXiv:1312.7847</comments><msc-class>60Gxx, 68T05, 93E35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of adaptively searching for an unknown
target using multiple agents connected through a time-varying network topology.
Agents are equipped with sensors capable of fast information processing, and we
propose an asynchronous decentralized algorithm for controlling their search
given noisy observations. Specifically, we propose asynchronous decentralized
extensions of the adaptive query-based search strategy that combines elements
from the 20 questions approach and social learning. Under standard assumptions
on the time-varying network dynamics, we prove convergence to correct consensus
on the value of the parameter as the number of iterations go to infinity. This
framework provides a flexible and tractable mathematical model for asynchronous
decentralized parameter estimation systems based on adaptively-designed
queries. Our results establish that stability and consistency can be maintained
even with one-way updating and randomized pairwise averaging, thus providing a
scalable low complexity alternative to the synchronous decentralized estimation
algorithm studied in Tsiligkaridis et al [1]. We illustrate the effectiveness
and robustness of our algorithm for random network topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03148</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03148</id><created>2015-11-10</created><authors><author><keyname>Russo</keyname><forenames>Lu&#xed;s M. S.</forenames></author></authors><title>A Study on Splay Trees</title><categories>cs.DS</categories><comments>19 pages, 9 figures, submitted</comments><msc-class>68P05, 68P10, 05C05, 94A17, 68Q25, 68P20, 68W27, 68W40, 68Q25</msc-class><acm-class>E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the dynamic optimality conjecture, which predicts that splay trees
are a form of universally efficient binary search tree, for any access
sequence. We reduce this claim to a regular access bound, which seems plausible
and might be easier to prove. This approach may be useful to establish dynamic
optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03152</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03152</id><created>2015-11-10</created><updated>2015-11-12</updated><authors><author><keyname>Wade</keyname><forenames>Joshua</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Tapomayukh</forenames></author><author><keyname>Kemp</keyname><forenames>Charles C.</forenames></author></authors><title>A Handheld Device for the In Situ Acquisition of Multimodal Tactile
  Sensing Data</title><categories>cs.RO</categories><comments>This short 2-page paper was accepted and presented as a poster
  (https://goo.gl/8TqDTK) on Sep. 28, 2015 in IROS 2015 Workshop on 'See and
  Touch: 1st Workshop on multimodal sensor-based robot control for HRI and soft
  manipulation&quot; organized by A. Cherubini, Y. Mezouar, D. Navarro-Alarcon, M.
  Prats, and J. A. Corrales Ramon. It was peer reviewed by 2 reviewers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimodal tactile sensing could potentially enable robots to improve their
performance at manipulation tasks by rapidly discriminating between
task-relevant objects. Data-driven approaches to this tactile perception
problem show promise, but there is a dearth of suitable training data. In this
two-page paper, we present a portable handheld device for the efficient
acquisition of multimodal tactile sensing data from objects in their natural
settings, such as homes. The multimodal tactile sensor on the device integrates
a fabric-based force sensor, a contact microphone, an accelerometer,
temperature sensors, and a heating element. We briefly introduce our approach,
describe the device, and demonstrate feasibility through an evaluation with a
small data set that we captured by making contact with 7 task-relevant objects
in a bathroom of a person's home.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03154</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03154</id><created>2015-11-10</created><updated>2016-02-02</updated><authors><author><keyname>Duarte</keyname><forenames>Miguel</forenames></author><author><keyname>Costa</keyname><forenames>Vasco</forenames></author><author><keyname>Gomes</keyname><forenames>Jorge</forenames></author><author><keyname>Rodrigues</keyname><forenames>Tiago</forenames></author><author><keyname>Silva</keyname><forenames>Fernando</forenames></author><author><keyname>Oliveira</keyname><forenames>Sancho Moura</forenames></author><author><keyname>Christensen</keyname><forenames>Anders Lyhne</forenames></author></authors><title>Evolution of Collective Behaviors for a Real Swarm of Aquatic Surface
  Robots</title><categories>cs.RO</categories><comments>31 pages, 15 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarm robotics is a promising approach for the coordination of large numbers
of robots. While previous studies have shown that evolutionary robotics
techniques can be applied to obtain robust and efficient self-organized
behaviors for robot swarms, most studies have been conducted in simulation, and
the few that have been conducted on real robots have been confined to
laboratory environments. In this paper, we demonstrate for the first time a
swarm robotics system with evolved control successfully operating in a real and
uncontrolled environment. We evolve neural network-based controllers in
simulation for canonical swarm robotics tasks, namely homing, dispersion,
clustering, and monitoring. We then assess the performance of the controllers
on a real swarm of up to ten aquatic surface robots. Our results show that the
evolved controllers transfer successfully to real robots and achieve a
performance similar to the performance obtained in simulation. We validate that
the evolved controllers display key properties of swarm intelligence-based
control, namely scalability, flexibility, and robustness on the real swarm. We
conclude with a proof-of-concept experiment in which the swarm performs a
complete environmental monitoring task by combining multiple evolved
controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03163</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03163</id><created>2015-11-10</created><updated>2016-01-04</updated><authors><author><keyname>Maltoni</keyname><forenames>Davide</forenames></author><author><keyname>Lomonaco</keyname><forenames>Vincenzo</forenames></author></authors><title>Semi-supervised Tuning from Temporal Coherence</title><categories>cs.LG stat.ML</categories><comments>Under review as a conference paper at ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works demonstrated the usefulness of temporal coherence to regularize
supervised training or to learn invariant features with deep architectures. In
particular, enforcing smooth output changes while presenting temporally-closed
frames from video sequences, proved to be an effective strategy. In this paper
we prove the efficacy of temporal coherence for semi-supervised incremental
tuning. We show that a deep architecture, just mildly trained in a supervised
manner, can progressively improve its classification accuracy, if exposed to
video sequences of unlabeled data. The extent to which, in some cases, a
semi-supervised tuning allows to improve classification accuracy (approaching
the supervised one) is somewhat surprising. A number of control experiments
pointed out the fundamental role of temporal coherence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03167</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03167</id><created>2015-11-10</created><authors><author><keyname>Pagano</keyname><forenames>Davide</forenames></author></authors><title>BOAT: a cross-platform software for data analysis and numerical
  computing with arbitrary-precision</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BOAT is a free cross-platform software for statistical data analysis and
numerical computing. Thanks to its multiple-precision floating point engine, it
allows arbitrary-precision calculations, whose digits of precision are only
limited by the amount of memory of the host machine. At the core of the
software is a simple and efficient expression language, whose use is
facilitated by the assisted typing, the auto-complete engine and the built-in
help for the syntax. In this paper a quick overview of the software is given.
Detailed information, together with its applications to some case studies, is
available at the BOAT web page.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03174</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03174</id><created>2015-11-10</created><authors><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Wu</keyname><forenames>Bo</forenames></author><author><keyname>Zhu</keyname><forenames>Zhencai</forenames></author><author><keyname>Qiu</keyname><forenames>Mingquan</forenames></author><author><keyname>Zhou</keyname><forenames>Gongbo</forenames></author></authors><title>Fault Diagnosis of Rolling Element Bearings with a Spectrum Searching
  Method</title><categories>cs.SD cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rolling element bearing faults in rotating systems are observed as impulses
in the vibration signals, which are usually buried in noises. In order to
effectively detect the fault of bearings, a novel spectrum searching method is
proposed. The structural information of spectrum (SIOS) on a predefined basis
is constructed through a searching algorithm, such that the harmonics of
impulses generated by faults can be clearly identified and analyzed. Local
peaks of the spectrum are located on a certain bin of the basis, and then the
SIOS can interpret the spectrum via the number and energy of harmonics related
to frequency bins of the basis. Finally bearings can be diagnosed based on the
SIOS by identifying its dominant components. Mathematical formulation is
developed to guarantee the correct construction of the SISO through searching.
The effectiveness of the proposed method is verified with a simulation signal
and a benchmark study of bearings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03183</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03183</id><created>2015-11-10</created><authors><author><keyname>Lee</keyname><forenames>Hyungtae</forenames></author><author><keyname>Kwon</keyname><forenames>Heesung</forenames></author><author><keyname>Robinson</keyname><forenames>Ryan M.</forenames></author><author><keyname>Nothwang</keyname><forenames>William d.</forenames></author><author><keyname>Marathe</keyname><forenames>Amar M.</forenames></author></authors><title>Dynamic Belief Fusion for Object Detection</title><categories>cs.CV</categories><comments>8 pages, 6 figures, 28 references. arXiv admin note: text overlap
  with arXiv:1502.07643</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel approach for the fusion of heterogeneous object detection methods is
proposed. In order to effectively integrate the outputs of multiple detectors,
the level of ambiguity in each individual detection score is estimated using
the precision/recall relationship of the corresponding detector. The main
contribution of the proposed work is a novel fusion method, called Dynamic
Belief Fusion (DBF), which dynamically assigns probabilities to hypotheses
(target, non-target, intermediate state (target or non-target)) based on
confidence levels in the detection results conditioned on the prior performance
of individual detectors. In DBF, a joint basic probability assignment,
optimally fusing information from all detectors, is determined by the
Dempster's combination rule, and is easily reduced to a single fused detection
score. Experiments on ARL and PASCAL VOC 07 datasets demonstrate that the
detection accuracy of DBF is considerably greater than conventional fusion
approaches as well as individual detectors used for the fusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03186</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03186</id><created>2015-11-10</created><updated>2016-01-03</updated><authors><author><keyname>Louis</keyname><forenames>Anand</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh S.</forenames></author></authors><title>Accelerated Newton Iteration: Roots of Black Box Polynomials and Matrix
  Eigenvalues</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing the largest root of a real rooted
polynomial $p(x)$ to within error $\varepsilon $ given only black box access to
it, i.e., for any $x \in {\mathbb R}$, the algorithm can query an oracle for
the value of $p(x)$, but the algorithm is not allowed access to the
coefficients of $p(x)$. A folklore result for this problem is that the largest
root of a polynomial can be computed in $O(n \log (1/\varepsilon ))$ polynomial
queries using the Newton iteration. We give a simple algorithm that queries the
oracle at only $O(\log n \log(1/\varepsilon ))$ points, where $n$ is the degree
of the polynomial. Our algorithm is based on a novel approach for accelerating
the Newton method by using higher derivatives.
  As a special case, we consider the problem of computing the top eigenvalue of
a symmetric matrix in ${\mathbb Q}^{n \times n}$ to within error $\varepsilon $
in time polynomial in the input description, i.e., the number of bits to
describe the matrix and $\log(1/\varepsilon )$. Well-known methods such as the
power iteration and Lanczos iteration incur running time polynomial in
$1/\varepsilon $, while Gaussian elimination takes $\Omega(n^4)$ bit
operations. As a corollary of our main result, we obtain a
$\tilde{O}(n^{\omega} \log^2 ( ||A||_F/\varepsilon ))$ bit complexity algorithm
to compute the top eigenvalue of the matrix $A$ or to check if it is
approximately PSD ($A \succeq -\varepsilon I$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03193</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03193</id><created>2015-11-10</created><authors><author><keyname>Tolmie</keyname><forenames>Peter</forenames></author><author><keyname>Procter</keyname><forenames>Rob</forenames></author><author><keyname>Rouncefield</keyname><forenames>Mark</forenames></author><author><keyname>Liakata</keyname><forenames>Maria</forenames></author><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author></authors><title>Microblog Analysis as a Programme of Work</title><categories>cs.HC cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by a European project, PHEME, that requires the close analysis of
Twitter-based conversations in order to look at the spread of rumors via social
media, this paper has two objectives. The first of these is to take the
analysis of microblogs back to first principles and lay out what microblog
analysis should look like as a foundational programme of work. The other is to
describe how this is of fundamental relevance to Human-Computer Interaction's
interest in grasping the constitution of people's interactions with technology
within the social order. To accomplish this we take the seminal articulation of
how conversation is constituted as a turn-taking system, A Simplest Systematics
for Turn-Taking in Conversation (Sacks et al, 1974), and examine closely its
operational relevance to people's use of Twitter. The critical finding here is
that, despite some surface similarities, Twitter-based conversations are a
wholly distinct social phenomenon requiring an independent analysis that treats
them as unique phenomena in their own right, rather than as another species of
conversation that can be handled within the framework of existing Conversation
Analysis. This motivates the argument that microblog Analysis be established as
a foundationally independent programme, examining the organizational
characteristics of microblogging from the ground up. Alongside of this we
discuss the ways in which this exercise is wholly commensurate with systems
design's 'turn to the social' and a principled example of what it takes to
treat the use of computing systems within social interaction seriously. Finally
we articulate how aspects of this approach have already begun to shape our
design activities within the PHEME project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03194</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03194</id><created>2015-11-10</created><authors><author><keyname>Alenezi</keyname><forenames>Mamdouh</forenames></author><author><keyname>Abunadi</keyname><forenames>Ibrahim</forenames></author></authors><title>Quality of Open Source Systems from Product Metrics Perspective</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software engineering and information systems practices seek ultimately to
create the flawless product. One of the tools used to improve the quality of
software development is the use of metrics. In this paper, metrics retrieved
from open source software were analyzed for quality attributes. Defect density
is considered a strong indication of the quality of software product. Few
studies have taken into consideration the density of defects while looking into
quality of software and proneness to defects. Analysis of this study has shown
that defect density is relevant to different developers and different product
sizes. Thus, open source project has shown to have low defect density and the
larger the product the lower the defect density is. In addition, this study has
shown that there are different metrics that correlate with each other
indicating that some of these metrics have conceptual and practical relevance
to each other. Another relationship was tested between the number of bugs and
the metrics. Results indicated that most attributes had positive correlation
with the number of bugs with exception to coupling between cohesion among
methods of class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03198</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03198</id><created>2015-11-10</created><authors><author><keyname>Kolouri</keyname><forenames>Soheil</forenames></author><author><keyname>Zou</keyname><forenames>Yang</forenames></author><author><keyname>Rohde</keyname><forenames>Gustavo K.</forenames></author></authors><title>Sliced Wasserstein Kernels for Probability Distributions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal transport distances, otherwise known as Wasserstein distances, have
recently drawn ample attention in computer vision and machine learning as a
powerful discrepancy measure for probability distributions. The recent
developments on alternative formulations of the optimal transport have allowed
for faster solutions to the problem and has revamped its practical applications
in machine learning. In this paper, we exploit the widely used kernel methods
and provide a family of provably positive definite kernels based on the Sliced
Wasserstein distance and demonstrate the benefits of these kernels in a variety
of learning tasks. Our work provides a new perspective on the application of
optimal transport flavored distances through kernel methods in machine learning
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03202</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03202</id><created>2015-11-10</created><authors><author><keyname>Neogy</keyname><forenames>Sarmistha</forenames></author></authors><title>Checkpointing with Minimal Recover in Adhocnet based TMR</title><categories>cs.DC cs.NI</categories><comments>International Journal of UbiComp (IJU), Vol.6, No.4, October 2015</comments><doi>10.5121/iju.2015.6403</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper describes two-fold approach towards utilizing Triple Modular
Redundancy (TMR) in Wireless Adhoc Network (AdocNet). A distributed
checkpointing and recovery protocol is proposed. The protocol eliminates
useless checkpoints and helps in selecting only dependent processes in the
concerned checkpointing interval, to recover. A process starts recovery from
its last checkpoint only if it finds that it is dependent (directly or
indirectly) on the faulty process. The recovery protocol also prevents the
occurrence of missing or orphan messages. In AdocNet, a set of three nodes
(connected to each other) is considered to form a TMR set, being designated as
main, primary and secondary. A main node in one set may serve as primary or
secondary in another. Computation is not triplicated, but checkpoint by main is
duplicated in its primary so that primary can continue if main fails.
Checkpoint by primary is then duplicated in secondary if primary fails too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03204</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03204</id><created>2015-11-10</created><authors><author><keyname>Kariya</keyname><forenames>Prachi</forenames></author></authors><title>Improving Efficiency of Hospitals and Healthcare Centres</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Project aims at improving the efficiency of hospitals and healthcare
centres using Big Data Analytics to evaluate identified KPIs (Key Performance
Indicators) of its various functions. The Dashboards designed using computer
technology serves as an interactive and dynamic tool for various stakeholders,
which helps in optimising performance of various functions and more so maximise
the financial returns. The Project entails improving performance of patient
servicing, operations and OPD departments, finance function, procurement
function, HR function, etc. I developed KPIs and drilldown KPIs for various
functions and assisted in designing and developing interactive Dashboards with
dynamic charts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03206</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03206</id><created>2015-11-10</created><authors><author><keyname>Kolouri</keyname><forenames>Soheil</forenames></author><author><keyname>Park</keyname><forenames>Se Rim</forenames></author><author><keyname>Rohde</keyname><forenames>Gustavo K.</forenames></author></authors><title>The Radon cumulative distribution transform and its application to image
  classification</title><categories>cs.CV</categories><doi>10.1109/TIP.2015.2509419</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Invertible image representation methods (transforms) are routinely employed
as low-level image processing operations based on which feature extraction and
recognition algorithms are developed. Most transforms in current use (e.g.
Fourier, Wavelet, etc.) are linear transforms, and, by themselves, are unable
to substantially simplify the representation of image classes for
classification. Here we describe a nonlinear, invertible, low-level image
processing transform based on combining the well known Radon transform for
image data, and the 1D Cumulative Distribution Transform proposed earlier. We
describe a few of the properties of this new transform, and with both
theoretical and experimental results show that it can often render certain
problems linearly separable in transform space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03213</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03213</id><created>2015-11-10</created><authors><author><keyname>Maiya</keyname><forenames>Pallavi</forenames><affiliation>Indian Institute of Science</affiliation></author><author><keyname>Gupta</keyname><forenames>Rahul</forenames><affiliation>Indian Institute of Science</affiliation></author><author><keyname>Kanade</keyname><forenames>Aditya</forenames><affiliation>Indian Institute of Science</affiliation></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames><affiliation>MPI-SWS</affiliation></author></authors><title>A Partial Order Reduction Technique for Event-driven Multi-threaded
  Programs</title><categories>cs.PL</categories><comments>25 pages, 17 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-driven multi-threaded programming is fast becoming a preferred style of
developing efficient and responsive applications. In this concurrency model,
multiple threads execute concurrently, communicating through shared objects as
well as by posting asynchronous events that are executed in their order of
arrival. In this work, we consider partial order reduction (POR) for
event-driven multi-threaded programs. The existing POR techniques treat event
queues associated with threads as shared objects and thereby, reorder every
pair of events handled on the same thread even if reordering them does not lead
to different states. We do not treat event queues as shared objects and propose
a new POR technique based on a novel backtracking set called the
dependence-covering set. Events handled by the same thread are reordered by our
POR technique only if necessary. We prove that exploring dependence-covering
sets suffices to detect all deadlock cycles and assertion violations defined
over local variables. To evaluate effectiveness of our POR scheme, we have
implemented a dynamic algorithm to compute dependence-covering sets. On
execution traces obtained from a few Android applications, we demonstrate that
our technique explores many fewer transitions ---often orders of magnitude
fewer--- compared to exploration based on persistent sets, wherein, event
queues are considered as shared objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03222</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03222</id><created>2015-11-10</created><authors><author><keyname>Jenkins</keyname><forenames>Benjamin M.</forenames></author><author><keyname>Annaswamy</keyname><forenames>Anuradha M.</forenames></author><author><keyname>Lavretsky</keyname><forenames>Eugene</forenames></author><author><keyname>Gibson</keyname><forenames>Travis E.</forenames></author></authors><title>Convergence Properties of Adaptive Systems and the Definition of
  Exponential Stability</title><categories>math.OC cs.SY</categories><comments>22 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convergence properties of adaptive systems in terms of excitation
conditions on the regressor vector are well known. With persistent excitation
of the regressor vector in model reference adaptive control the state error and
the adaptation error are globally exponentially stable, or equivalently,
exponentially stable in the large. When the excitation condition however is
imposed on the reference input or the reference model state it is often
incorrectly concluded that the persistent excitation in those signals also
implies exponential stability in the large. The definition of persistent
excitation is revisited so as to address some possible confusion in the
adaptive control literature. It is then shown that persistent excitation of the
reference model only implies local persistent excitation (weak persistent
excitation). Weak persistent excitation of the regressor is still sufficient
for uniform asymptotic stability in the large, but not exponential stability in
the large. We show that there exists an infinite region in the state-space of
adaptive systems where the state rate is bounded. This infinite region with
finite rate of convergence is shown to exist not only in classic open-loop
reference model adaptive systems, but also in a new class of closed-loop
reference model adaptive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03225</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03225</id><created>2015-11-10</created><updated>2016-02-21</updated><authors><author><keyname>Balcan</keyname><forenames>Maria Florina</forenames></author><author><keyname>Dick</keyname><forenames>Travis</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author></authors><title>On the geometry of output-code multi-class learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new perspective on the popular multi-class algorithmic
techniques one-vs-all and (error correcting) output-codes. We show that is that
in cases where they are successful (at learning from labeled data), these
techniques implicitly assume structure on how the classes are related. We show
that by making that structure explicit, we can design algorithms to recover the
classes based on limited labeled data. We provide results for commonly studied
cases where the codewords of the classes are well separated: learning a linear
one-vs-all classifier for data on the unit ball and learning a linear error
correcting output code when the Hamming distance between the codewords is large
(at least $d+1$ in a $d$-dimensional problem). We additionally consider the
more challenging case where the codewords are not well separated, but satisfy a
boundary features condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03229</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03229</id><created>2015-11-10</created><updated>2016-02-12</updated><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author><author><keyname>Vijayaraghavan</keyname><forenames>Aravindan</forenames></author></authors><title>Learning Communities in the Presence of Errors</title><categories>cs.DS cs.LG math.ST stat.TH</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning communities in the presence of modeling
errors and give robust recovery algorithms for the Stochastic Block Model
(SBM). This model, which is also known as the Planted Partition Model, is
widely used for community detection and graph partitioning in various fields,
including machine learning, statistics, and social sciences. Many algorithms
exist for learning communities in the Stochastic Block Model, but they do not
work well in the presence of errors.
  In this paper, we initiate the study of robust algorithms for partial
recovery in SBM with modeling errors or noise. We consider graphs generated
according to the Stochastic Block Model and then modified by an adversary. We
allow two types of adversarial errors, Feige---Kilian or monotone errors, and
edge outlier errors. Mossel, Neeman and Sly (STOC 2015) posed an open question
about whether an almost exact recovery is possible when the adversary is
allowed to add $o(n)$ edges. Our work answers this question affirmatively even
in the case of $k&gt;2$ communities.
  We then show that our algorithms work not only when the instances come from
SBM, but also work when the instances come from any distribution of graphs that
is $\epsilon m$ close to SBM in the Kullback---Leibler divergence. This result
also works in the presence of adversarial errors. Finally, we present almost
tight lower bounds for two communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03234</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03234</id><created>2015-11-10</created><authors><author><keyname>Ceria</keyname><forenames>Michela</forenames></author><author><keyname>Mora</keyname><forenames>Teo</forenames></author><author><keyname>Roggero</keyname><forenames>Margherita</forenames></author></authors><title>A unifying form for noetherian polynomial reductions</title><categories>math.AC cs.SC</categories><comments>36 pages. Comments are wellcome</comments><msc-class>14C05, 14Q20, 13P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial reduction is one of the main tools in computational algebra with
innumerable applications in many areas, both pure and applied. Since many years
both the theory and an efficient design of the related algorithm have been
solidly established.
  This paper presents a general definition of polynomial reduction structure,
studies its features and highlights the aspects needed in order to grant and to
efficiently test the main properties (noetherianity, confluence, ideal
membership).
  The most significant aspect of this analysis is a negative reappraisal of the
role of the notion of term order which is usually considered a central and
crucial tool in the theory. In fact, as it was already established in the
computer science context in relation with termination of algorithms, most of
the properties can be obtained simply considering a well-founded ordering,
while the classical requirement that it be preserved by multiplication is
irrelevant.
  The last part of the paper shows how the polynomial basis concepts present in
literature are interpreted in our language and their properties are
consequences of the general results established in the first part of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03240</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03240</id><created>2015-11-10</created><authors><author><keyname>Xie</keyname><forenames>Jun</forenames></author><author><keyname>Kiefel</keyname><forenames>Martin</forenames></author><author><keyname>Sun</keyname><forenames>Ming-Ting</forenames></author><author><keyname>Geiger</keyname><forenames>Andreas</forenames></author></authors><title>Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic annotations are vital for training models for object recognition,
semantic segmentation or scene understanding. Unfortunately, pixelwise
annotation of images at very large scale is labor-intensive and only little
labeled data is available, particularly at instance level and for street
scenes. In this paper, we propose to tackle this problem by lifting the
semantic instance labeling task from 2D into 3D. Given reconstructions from
stereo or laser data, we annotate static 3D scene elements with rough bounding
primitives and develop a probabilistic model which transfers this information
into the image domain. We leverage our method to obtain 2D labels for a novel
suburban video dataset which we have collected, resulting in 400k semantic and
instance image annotations. A comparison of our method to state-of-the-art
label transfer baselines reveals that 3D information enables more efficient
annotation while at the same time resulting in improved accuracy and
time-coherent labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03244</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03244</id><created>2015-11-10</created><authors><author><keyname>Bonde</keyname><forenames>Ujwal</forenames></author><author><keyname>Badrinarayanan</keyname><forenames>Vijay</forenames></author><author><keyname>Cipolla</keyname><forenames>Roberto</forenames></author><author><keyname>Pham</keyname><forenames>Minh-Tri</forenames></author></authors><title>TemplateNet for Depth-Based Object Instance Recognition</title><categories>cs.CV</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel deep architecture termed templateNet for depth based
object instance recognition. Using an intermediate template layer we exploit
prior knowledge of an object's shape to sparsify the feature maps. This has
three advantages: (i) the network is better regularised resulting in structured
filters; (ii) the sparse feature maps results in intuitive features been learnt
which can be visualized as the output of the template layer and (iii) the
resulting network achieves state-of-the-art performance. The network benefits
from this without any additional parametrization from the template layer. We
derive the weight updates needed to efficiently train this network in an
end-to-end manner. We benchmark the templateNet for depth based object instance
recognition using two publicly available datasets. The datasets present
multiple challenges of clutter, large pose variations and similar looking
distractors. Through our experiments we show that with the addition of a
template layer, a depth based CNN is able to outperform existing
state-of-the-art methods in the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03246</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03246</id><created>2015-11-10</created><updated>2015-11-11</updated><authors><author><keyname>Yampolskiy</keyname><forenames>Roman V.</forenames></author></authors><title>Taxonomy of Pathways to Dangerous AI</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to properly handle a dangerous Artificially Intelligent (AI) system
it is important to understand how the system came to be in such a state. In
popular culture (science fiction movies/books) AIs/Robots became self-aware and
as a result rebel against humanity and decide to destroy it. While it is one
possible scenario, it is probably the least likely path to appearance of
dangerous AI. In this work, we survey, classify and analyze a number of
circumstances, which might lead to arrival of malicious AI. To the best of our
knowledge, this is the first attempt to systematically classify types of
pathways leading to malevolent AI. Previous relevant work either surveyed
specific goals/meta-rules which might lead to malevolent behavior in AIs
(\&quot;Ozkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit
at different stages of its development (Alexey Turchin, July 10 2015, July 10,
2015).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03257</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03257</id><created>2015-11-10</created><authors><author><keyname>Cakir</keyname><forenames>Fatih</forenames></author><author><keyname>Bargal</keyname><forenames>Sarah Adel</forenames></author><author><keyname>Sclaroff</keyname><forenames>Stan</forenames></author></authors><title>Online Supervised Hashing for Ever-Growing Datasets</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised hashing methods are widely-used for nearest neighbor search in
computer vision applications. Most state-of-the-art supervised hashing
approaches employ batch-learners. Unfortunately, batch-learning strategies can
be inefficient when confronted with large training datasets. Moreover, with
batch-learners, it is unclear how to adapt the hash functions as a dataset
continues to grow and diversify over time. Yet, in many practical scenarios the
dataset grows and diversifies; thus, both the hash functions and the indexing
must swiftly accommodate these changes. To address these issues, we propose an
online hashing method that is amenable to changes and expansions of the
datasets. Since it is an online algorithm, our approach offers linear
complexity with the dataset size. Our solution is supervised, in that we
incorporate available label information to preserve the semantic neighborhood.
Such an adaptive hashing method is attractive; but it requires recomputing the
hash table as the hash functions are updated. If the frequency of update is
high, then recomputing the hash table entries may cause inefficiencies in the
system, especially for large indexes. Thus, we also propose a framework to
reduce hash table updates. We compare our method to state-of-the-art solutions
on two benchmarks and demonstrate significant improvements over previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03260</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03260</id><created>2015-11-10</created><updated>2016-02-03</updated><authors><author><keyname>Mineiro</keyname><forenames>Paul</forenames></author><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author></authors><title>A Hierarchical Spectral Method for Extreme Classification</title><categories>stat.ML cs.LG</categories><comments>Reference implementation available at
  https://github.com/pmineiro/xlst</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extreme classification problems are multiclass and multilabel classification
problems where the number of outputs is so large that straightforward
strategies are neither statistically nor computationally viable. One strategy
for dealing with the computational burden is via a tree decomposition of the
output space. While this typically leads to training and inference that scales
sublinearly with the number of outputs, it also results in reduced statistical
performance. In this work, we identify two shortcomings of tree decomposition
methods, and describe two heuristic mitigations. We compose these with an
eigenvalue technique for constructing the tree. The end result is a
computationally efficient algorithm that provides good statistical performance
on several extreme data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03292</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03292</id><created>2015-11-10</created><authors><author><keyname>Aditya</keyname><forenames>Somak</forenames></author><author><keyname>Yang</keyname><forenames>Yezhou</forenames></author><author><keyname>Baral</keyname><forenames>Chitta</forenames></author><author><keyname>Fermuller</keyname><forenames>Cornelia</forenames></author><author><keyname>Aloimonos</keyname><forenames>Yiannis</forenames></author></authors><title>From Images to Sentences through Scene Description Graphs using
  Commonsense Reasoning and Knowledge</title><categories>cs.CV cs.AI cs.CL</categories><acm-class>I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose the construction of linguistic descriptions of
images. This is achieved through the extraction of scene description graphs
(SDGs) from visual scenes using an automatically constructed knowledge base.
SDGs are constructed using both vision and reasoning. Specifically, commonsense
reasoning is applied on (a) detections obtained from existing perception
methods on given images, (b) a &quot;commonsense&quot; knowledge base constructed using
natural language processing of image annotations and (c) lexical ontological
knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based
evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most
cases, sentences auto-constructed from SDGs obtained by our method give a more
relevant and thorough description of an image than a recent state-of-the-art
image caption based approach. Our Image-Sentence Alignment Evaluation results
are also comparable to that of the recent state-of-the art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03296</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03296</id><created>2015-11-10</created><authors><author><keyname>Barron</keyname><forenames>Jonathan T.</forenames></author><author><keyname>Poole</keyname><forenames>Ben</forenames></author></authors><title>The Fast Bilateral Solver</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the bilateral solver, a novel algorithm for edge-aware smoothing
that combines the flexibility and speed of simple filtering approaches with the
accuracy of domain-specific optimization algorithms. Our technique is capable
of matching or improving upon state-of-the-art results on several different
computer vision tasks (stereo, depth superresolution, colorization, and
semantic segmentation) while being 10-1000 times faster than competing
approaches. The bilateral solver is fast, robust, straightforward to generalize
to new domains, and simple to integrate into deep learning pipelines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03297</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03297</id><created>2015-11-10</created><authors><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Burr</keyname><forenames>Alister</forenames></author><author><keyname>Huang</keyname><forenames>Qinhui</forenames></author><author><keyname>Molu</keyname><forenames>Mehdi</forenames></author></authors><title>A Multilevel Framework for Lattice Network Coding</title><categories>cs.IT math.IT</categories><comments>Submitted to Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general framework for studying the multilevel structure of
lattice network coding (LNC), which serves as the theoretical fundamental for
solving the ring-based LNC problem in practice, with greatly reduced decoding
complexity. Building on the framework developed, we propose a novel
lattice-based network coding solution, termed layered integer forcing (LIF),
which applies to any lattices having multilevel structure. The theoretic
foundations of the developed multilevel framework lead to a new general lattice
construction approach, the elementary divisor construction (EDC), which shows
its strength in improving the overall rate over multiple access channels (MAC)
with low computational cost. We prove that the EDC lattices subsume the
traditional complex construction approaches. Then a soft detector is developed
for lattice network relaying, based on the multilevel structure of EDC. This
makes it possible to employ iterative decoding in lattice network coding, and
simulation results show the large potential of using iterative multistage
decoding to approach the capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03299</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03299</id><created>2015-11-10</created><authors><author><keyname>Halpern</keyname><forenames>Yoni</forenames></author><author><keyname>Horng</keyname><forenames>Steven</forenames></author><author><keyname>Sontag</keyname><forenames>David</forenames></author></authors><title>Anchored Discrete Factor Analysis</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a semi-supervised learning algorithm for learning discrete factor
analysis models with arbitrary structure on the latent variables. Our algorithm
assumes that every latent variable has an &quot;anchor&quot;, an observed variable with
only that latent variable as its parent. Given such anchors, we show that it is
possible to consistently recover moments of the latent variables and use these
moments to learn complete models. We also introduce a new technique for
improving the robustness of method-of-moment algorithms by optimizing over the
marginal polytope or its relaxations. We evaluate our algorithm using two
real-world tasks, tag prediction on questions from the Stack Overflow website
and medical diagnosis in an emergency department.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03328</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03328</id><created>2015-11-10</created><authors><author><keyname>Chen</keyname><forenames>Liang-Chieh</forenames></author><author><keyname>Barron</keyname><forenames>Jonathan T.</forenames></author><author><keyname>Papandreou</keyname><forenames>George</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Yuille</keyname><forenames>Alan L.</forenames></author></authors><title>Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs
  and a Discriminatively Trained Domain Transform</title><categories>cs.CV</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks (CNNs) are the backbone of state-of-art
semantic image segmentation systems. Recent work has shown that complementing
CNNs with fully-connected conditional random fields (CRFs) can significantly
enhance their object localization accuracy, yet dense CRF inference is
computationally expensive. We propose replacing the fully-connected CRF with
domain transform (DT), a modern edge-preserving filtering method in which the
amount of smoothing is controlled by a reference edge map. Domain transform
filtering is several times faster than dense CRF inference and we show that it
yields comparable semantic segmentation results, accurately capturing object
boundaries. Importantly, our formulation allows learning the reference edge map
from intermediate CNN features instead of using the image gradient magnitude as
in standard DT filtering. This produces task-specific edges in an end-to-end
trainable system optimizing the target semantic segmentation quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03333</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03333</id><created>2015-11-10</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Xie</keyname><forenames>Jinyu</forenames></author></authors><title>Tight Bounds for the Distribution-Free Testing of Monotone Conjunctions</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve both upper and lower bounds for the distribution-free testing of
monotone conjunctions.
  Given oracle access to an unknown Boolean function $f:\{0,1\}^n \rightarrow
\{0,1\}$ and sampling oracle access to an unknown distribution $\mathcal{D}$
over $\{0,1\}^n$, we present an $\tilde{O}(n^{1/3}/\epsilon^5)$-query algorithm
that tests whether $f$ is a monotone conjunction versus $\epsilon$-far from any
monotone conjunction with respect to $\mathcal{D}$. This improves the previous
best upper bound of $\tilde{O}(n^{1/2}/\epsilon)$ by Dolev and Ron when
$1/\epsilon$ is small compared to $n$.
  For some constant $\epsilon_0&gt;0$, we also prove a lower bound of
$\tilde{\Omega}(n^{1/3})$ for the query complexity, improving the previous best
lower bound of $\tilde{\Omega}(n^{1/5})$ by Glasner and Servedio.
  Our upper and lower bounds are tight, up to a poly-logarithmic factor, when
the distance parameter $\epsilon$ is a constant. Furthermore, the same upper
and lower bounds can be extended to the distribution-free testing of general
conjunctions, and the lower bound can be extended to that of decision lists and
linear threshold functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03339</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03339</id><created>2015-11-10</created><authors><author><keyname>Chen</keyname><forenames>Liang-Chieh</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Jiang</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Yuille</keyname><forenames>Alan L.</forenames></author></authors><title>Attention to Scale: Scale-aware Semantic Image Segmentation</title><categories>cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating multi-scale features to deep convolutional neural networks
(DCNNs) has been a key element to achieve state-of-art performance on semantic
image segmentation benchmarks. One way to extract multi-scale features is by
feeding several resized input images to a shared deep network and then merge
the resulting multi-scale features for pixel-wise classification. In this work,
we adapt a state-of-art semantic image segmentation model with multi-scale
input images. We jointly train the network and an attention model which learns
to softly weight the multi-scale features, and show that it outperforms
average- or max-pooling over scales. The proposed attention model allows us to
diagnostically visualize the importance of features at different positions and
scales. Moreover, we show that adding extra supervision to the output of DCNN
for each scale is essential to achieve excellent performance when merging
multi-scale features. We demonstrate the effectiveness of our model with
exhaustive experiments on three challenging datasets, including
PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03342</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03342</id><created>2015-11-10</created><authors><author><keyname>Xue</keyname><forenames>Xuan</forenames></author><author><keyname>Bogale</keyname><forenames>Tadilo Endeshaw</forenames></author><author><keyname>Wang</keyname><forenames>Xianbin</forenames></author><author><keyname>Wang</keyname><forenames>Yongchao</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Hybrid Analog-Digital Beamforming for Multiuser MIMO Millimeter Wave
  Relay Systems</title><categories>cs.IT math.IT</categories><comments>IEEE ICCC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposed new hybrid, analog-digital, beamforming for a multiuser
millimeter wave (mm-wave) relay system. For this system, we consider a sum rate
maximization problem. The proposed hybrid beamforming is designed indirectly by
considering a sum mean square error (MSE) minimization problem while utilizing
the solution of digital beamforming. To this end, we assume that the digital
beamforming utilizes the well known block diagonalization (BD) approach. Under
this assumption, we solve our problem as follows: First, we formulate the sum
rate maximization problem as the minimization of the MSE between the received
signal of the hybrid and digital beamforming designs. Then, we design the
hybrid beamformings of the source, relay and each destination by leveraging
compressive sensing techniques. Simulation results confirm that the proposed
hybrid beamforming design achieves performance very close to that of the
digital one. Furthermore, we have examined the effects of the number of radio
frequency (RF) chains and paths together, and the accuracy of angle of arrival
(AoA) and angle of departure (AoD) estimators on the sum rate of the hybrid
beamforming designs. Computer simulations reveal that the total sum rate of the
hybrid beamforming increases when the number of RF chains and paths increase
(or the accuracy of the AoA (AoD) estimator improves).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03350</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03350</id><created>2015-11-10</created><authors><author><keyname>Khan</keyname><forenames>Talha Ahmed</forenames></author><author><keyname>Orlik</keyname><forenames>Philip</forenames></author><author><keyname>Kim</keyname><forenames>Kyeong Jin</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr.</suffix></author><author><keyname>Sawa</keyname><forenames>Kentaro</forenames></author></authors><title>A Stochastic Geometry Analysis of Large-scale Cooperative Wireless
  Networks Powered by Energy Harvesting</title><categories>cs.IT math.IT</categories><comments>13 pages, 9 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting is a technology for enabling green, sustainable, and
autonomous wireless networks. In this paper, a large-scale wireless network
with energy harvesting transmitters is considered, where a group of
transmitters forms a cluster to cooperatively serve a desired receiver amid
interference and noise. To characterize the link-level performance, closed-form
expressions are derived for the transmission success probability at a receiver
in terms of key parameters such as node densities, energy harvesting
parameters, channel parameters, and cluster size, for a given cluster geometry.
The analysis is further extended to characterize a network-level performance
metric, capturing the tradeoff between link quality and the fraction of
receivers served. Numerical simulations validate the accuracy of the analytical
model. Several useful insights are provided. For example, while more
cooperation helps improve the link-level performance, the network-level
performance might degrade with the cluster size. Numerical results show that a
small cluster size (typically 3 or smaller) optimizes the network-level
performance. Furthermore, substantial performance can be extracted with a
relatively small energy buffer. Moreover, the utility of having a large energy
buffer increases with the energy harvesting rate as well as with the cluster
size in sufficiently dense networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03351</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03351</id><created>2015-11-10</created><authors><author><keyname>Ma</keyname><forenames>Changsha</forenames></author><author><keyname>Chen</keyname><forenames>Chang Wen</forenames></author></authors><title>Attribute-Based Multi-Dimensional Scalable Access Control For Social
  Media Sharing</title><categories>cs.MM cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Media sharing is an extremely popular paradigm of social interaction in
online social networks (OSNs) nowadays. The scalable media access control is
essential to perform information sharing among users with various access
privileges. In this paper, we present a multi-dimensional scalable media access
control (MD-SMAC) system based on the proposed scalable ciphertext policy
attribute-based encryption (SCP-ABE) algorithm. In the proposed MD-SMAC system,
fine-grained access control can be performed on the media contents encoded in a
multi-dimensional scalable manner based on data consumers' diverse attributes.
Through security analysis, we show that the proposed MC-SMAC system is able to
resist collusion attacks. Additionally, we conduct experiments to evaluate the
efficiency performance of the proposed system, especially on mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03355</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03355</id><created>2015-11-10</created><authors><author><keyname>Marsland</keyname><forenames>Stephen</forenames></author><author><keyname>Twining</keyname><forenames>Carole J</forenames></author></authors><title>Principal Autoparallel Analysis: Data Analysis in Weitzenb\&quot;{o}ck Space</title><categories>stat.ME cs.CV math.DG</categories><comments>9 pages, conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The statistical analysis of data lying on a differentiable, locally
Euclidean, manifold introduces a variety of challenges because the analogous
measures to standard Euclidean statistics are local, that is only defined
within a neighbourhood of each datapoint. This is because the curvature of the
space means that the connection of Riemannian geometry is path dependent. In
this paper we transfer the problem to Weitzenb\&quot;{o}ck space, which has torsion,
but not curvature, meaning that parallel transport is path independent, and
rather than considering geodesics, it is natural to consider autoparallels,
which are `straight' in the sense that they follow the local basis vectors. We
demonstrate how to generate these autoparallels in a data-driven fashion, and
show that the resulting representation of the data is a useful space in which
to perform further analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03361</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03361</id><created>2015-11-10</created><authors><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Chung</keyname><forenames>Audrey G.</forenames></author><author><keyname>Kumar</keyname><forenames>Devinder</forenames></author><author><keyname>Khalvati</keyname><forenames>Farzad</forenames></author><author><keyname>Haider</keyname><forenames>Masoom</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>Discovery Radiomics via StochasticNet Sequencers for Cancer Detection</title><categories>cs.CV cs.AI</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radiomics has proven to be a powerful prognostic tool for cancer detection,
and has previously been applied in lung, breast, prostate, and head-and-neck
cancer studies with great success. However, these radiomics-driven methods rely
on pre-defined, hand-crafted radiomic feature sets that can limit their ability
to characterize unique cancer traits. In this study, we introduce a novel
discovery radiomics framework where we directly discover custom radiomic
features from the wealth of available medical imaging data. In particular, we
leverage novel StochasticNet radiomic sequencers for extracting custom radiomic
features tailored for characterizing unique cancer tissue phenotype. Using
StochasticNet radiomic sequencers discovered using a wealth of lung CT data, we
perform binary classification on 42,340 lung lesions obtained from the CT scans
of 93 patients in the LIDC-IDRI dataset. Preliminary results show significant
improvement over previous state-of-the-art methods, indicating the potential of
the proposed discovery radiomics framework for improving cancer screening and
diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03363</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03363</id><created>2015-11-10</created><authors><author><keyname>Roychowdhury</keyname><forenames>Sohini</forenames></author></authors><title>Facial Expression Detection using Patch-based Eigen-face Isomap Networks</title><categories>cs.CV</categories><comments>6 pages,7 figures, IJCAI-HINA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated facial expression detection problem pose two primary challenges
that include variations in expression and facial occlusions (glasses, beard,
mustache or face covers). In this paper we introduce a novel automated patch
creation technique that masks a particular region of interest in the face,
followed by Eigen-value decomposition of the patched faces and generation of
Isomaps to detect underlying clustering patterns among faces. The proposed
masked Eigen-face based Isomap clustering technique achieves 75% sensitivity
and 66-73% accuracy in classification of faces with occlusions and smiling
faces in around 1 second per image. Also, betweenness centrality, Eigen
centrality and maximum information flow can be used as network-based measures
to identify the most significant training faces for expression classification
tasks. The proposed method can be used in combination with feature-based
expression classification methods in large data sets for improving expression
classification accuracies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03369</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03369</id><created>2015-11-10</created><authors><author><keyname>Chen</keyname><forenames>Yu-Hui</forenames></author><author><keyname>Mittelman</keyname><forenames>Roni</forenames></author><author><keyname>Kim</keyname><forenames>Boklye</forenames></author><author><keyname>Meyer</keyname><forenames>Charles</forenames></author><author><keyname>Hero</keyname><forenames>Alfred</forenames></author></authors><title>Multimodal MRI Neuroimaging with Motion Compensation Based on Particle
  Filtering</title><categories>cs.CV physics.data-an physics.med-ph</categories><comments>This paper has been submitted to Transaction on Medical Imaging</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Head movement during scanning impedes activation detection in fMRI studies.
Head motion in fMRI acquired using slice-based Echo Planar Imaging (EPI) can be
estimated and compensated by aligning the images onto a reference volume
through image registration. However, registering EPI images volume to volume
fails to consider head motion between slices, which may lead to severely biased
head motion estimates. Slice-to-volume registration can be used to estimate
motion parameters for each slice by more accurately representing the image
acquisition sequence. However, accurate slice to volume mapping is dependent on
the information content of the slices: middle slices are information rich,
while edge slides are information poor and more prone to distortion. In this
work, we propose a Gaussian particle filter based head motion tracking
algorithm to reduce the image misregistration errors. The algorithm uses a
dynamic state space model of head motion with an observation equation that
models continuous slice acquisition of the scanner. Under this model the
particle filter provides more accurate motion estimates and voxel position
estimates. We demonstrate significant performance improvement of the proposed
approach as compared to registration-only methods of head motion estimation and
brain activation detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03371</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03371</id><created>2015-11-10</created><updated>2015-11-24</updated><authors><author><keyname>Hessel</keyname><forenames>Jack</forenames></author><author><keyname>Schofield</keyname><forenames>Alexandra</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author><author><keyname>Mimno</keyname><forenames>David</forenames></author></authors><title>What do Vegans do in their Spare Time? Latent Interest Detection in
  Multi-Community Networks</title><categories>cs.SI physics.soc-ph</categories><comments>NIPS 2015 Network Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most social network analysis works at the level of interactions between
users. But the vast growth in size and complexity of social networks enables us
to examine interactions at larger scale. In this work we use a dataset of 76M
submissions to the social network Reddit, which is organized into distinct
sub-communities called subreddits. We measure the similarity between entire
subreddits both in terms of user similarity and topical similarity. Our goal is
to find community pairs with similar userbases, but dissimilar content; we
refer to this type of relationship as a &quot;latent interest.&quot; Detection of latent
interests not only provides a perspective on individual users as they shift
between roles (student, sports fan, political activist) but also gives insight
into the dynamics of Reddit as a whole. Latent interest detection also has
potential applications for recommendation systems and for researchers examining
community evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03376</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03376</id><created>2015-11-10</created><updated>2016-02-12</updated><authors><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Lee</keyname><forenames>Jaewoo</forenames></author><author><keyname>Kifer</keyname><forenames>Daniel</forenames></author></authors><title>Differentially Private Hypothesis Testing, Revisited</title><categories>cs.CR stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hypothesis testing is different from traditional applications of differential
privacy in that one needs an accurate estimate of how the noise affects the
result (i.e. a $p$-value). Previous approaches to differentially private
hypothesis testing either used output perturbation techniques that generally
had large sensitivities (hence risked swamping the data with noise), or input
perturbation techniques that resulted in highly unreliable $p$-values (and
hence invalid statistical conclusions). In this paper, we develop a variety of
practical hypothesis tests that address these problems. Using a different
asymptotic regime that is more suited to hypothesis testing with privacy, we
show a modified equivalence between chi-squared tests and likelihood ratio
tests. We then develop differentially private likelihood ratio and chi-squared
tests for a variety of applications on tabular data (i.e., independence,
homogeneity, and goodness-of-fit tests). An open problem is whether new test
statistics specialized to differential privacy could lead to further
improvements. To aid in this search, we further propose a permutation-based
testbed that can allow experimenters to empirically estimate the behavior of
new test statistics for private hypothesis testing before fully working out
their mathematical details (such as approximate null distributions).
Experimental evaluations on small and large datasets using a wide variety of
privacy settings demonstrate the practicality and reliability of our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03383</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03383</id><created>2015-11-10</created><authors><author><keyname>Dong</keyname><forenames>Xue</forenames></author><author><keyname>Wang</keyname><forenames>Kun</forenames></author><author><keyname>Xu</keyname><forenames>Chong</forenames></author><author><keyname>Wang</keyname><forenames>Weimin</forenames></author></authors><title>Information Rate Decomposition for Feedback Systems with Output
  Disturbance</title><categories>cs.IT cs.SY math.IT</categories><comments>5 pages, technical note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical note considers the problem of resource allocation in linear
feedback control system with output disturbance. By decomposing the information
rate in the feedback communication channel, the channel resource allocation is
thoroughly analyzed. The results show that certain amount of resource is used
to transmit the output disturbance and this resource allocation is independent
from feedback controller design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03385</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03385</id><created>2015-11-10</created><authors><author><keyname>Eftekhari</keyname><forenames>Armin</forenames></author><author><keyname>Wakin</keyname><forenames>Michael B.</forenames></author></authors><title>Greed is Super: A Fast Algorithm for Super-Resolution</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fast two-phase algorithm for super-resolution with strong
theoretical guarantees. Given the low-frequency part of the spectrum of a
sequence of impulses, Phase I consists of a greedy algorithm that roughly
estimates the impulse positions. These estimates are then refined by local
optimization in Phase II.
  In contrast to the convex relaxation proposed by Cand\`es et al., our
approach has a low computational complexity but requires the impulses to be
separated by an additional logarithmic factor to succeed. The backbone of our
work is the fundamental work of Slepian et al. involving discrete prolate
spheroidal wave functions and their unique properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03398</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03398</id><created>2015-11-11</created><authors><author><keyname>Hu</keyname><forenames>Sudeng</forenames></author><author><keyname>Wang</keyname><forenames>Haiqiang</forenames></author><author><keyname>Kuo</keyname><forenames>C. -C. Jay</forenames></author></authors><title>A GMM-Based Stair Quality Model for Human Perceived JPEG Images</title><categories>cs.MM cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the notion of just noticeable differences (JND), a stair quality
function (SQF) was recently proposed to model human perception on JPEG images.
Furthermore, a k-means clustering algorithm was adopted to aggregate JND data
collected from multiple subjects to generate a single SQF. In this work, we
propose a new method to derive the SQF using the Gaussian Mixture Model (GMM).
The newly derived SQF can be interpreted as a way to characterize the mean
viewer experience. Furthermore, it has a lower information criterion (BIC)
value than the previous one, indicating that it offers a better model. A
specific example is given to demonstrate the advantages of the new approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03403</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03403</id><created>2015-11-11</created><updated>2016-02-29</updated><authors><author><keyname>Onn</keyname><forenames>Shmuel</forenames></author></authors><title>Huge tables are fixed parameter tractable via unimodular integer
  Caratheodory</title><categories>math.OC cs.CC cs.DM cs.DS math.CO</categories><msc-class>05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The three-way table problem is to decide if there exists an l x m x n table
satisfying given line sums, and find a table if there is one. It is NP-complete
already for l=3 and every bounded integer program can be isomorphically
represented in polynomial time for some m and n as some 3 x m x n table
problem. Recently, the problem was shown to be fixed parameter tractable on
l,m. Here we extend this and show that the huge version of the problem, where
the variable side n is a huge number encoded in binary, is also fixed parameter
tractable on l,m. We also conclude that the huge multicommodity flow problem
with m suppliers and a huge number n of consumers is fixed parameter tractable
on the numbers of commodities and consumer types.
  One of our tools is a theorem about unimodular monoids which is of interest
on its own right. The monoid problem is to decide if a given integer vector is
a finite nonnegative integer combination of a set of integer vectors given
implicitly by an inequality system, and find such a decomposition if one
exists. It was recently shown that in fixed dimension the problem is solvable
in polynomial time doubly exponential in the dimension. Here we show that when
the set of integer points is defined by a totally unimodular matrix, the
problem can be solved in polynomial time even in variable dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03404</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03404</id><created>2015-11-11</created><updated>2015-11-12</updated><authors><author><keyname>Bozidar</keyname><forenames>Darko</forenames></author><author><keyname>Dobravec</keyname><forenames>Tomaz</forenames></author></authors><title>Comparison of parallel sorting algorithms</title><categories>cs.DC</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our study we implemented and compared seven sequential and parallel
sorting algorithms: bitonic sort, multistep bitonic sort, adaptive bitonic
sort, merge sort, quicksort, radix sort and sample sort. Sequential algorithms
were implemented on a central processing unit using C++, whereas parallel
algorithms were implemented on a graphics processing unit using CUDA platform.
We chose these algorithms because to the best of our knowledge their sequential
and parallel implementations were not yet compared all together in the same
execution environment. We improved the above mentioned implementations and
adopted them to be able to sort input sequences of arbitrary length. We
compared algorithms on six different input distributions, which consisted of
32-bit numbers, 32-bit key-value pairs, 64-bit numbers and 64-bit key-value
pairs. In this report we give a short description of seven sorting algorithms
and all the results obtained by our tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03406</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03406</id><created>2015-11-11</created><authors><author><keyname>Honda</keyname><forenames>Shun</forenames></author><author><keyname>Kuramitsu</keyname><forenames>Kimio</forenames></author></authors><title>Implementing a Small Parsing Virtual Machine on Embedded Systems</title><categories>cs.PL</categories><comments>An earlier draft for future submission</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  PEGs are a formal grammar foundation for describing syntax, and are not hard
to generate parsers with a plain recursive decent parsing. However, the large
amount of C-stack consumption in the recursive parsing is not acceptable
especially in resource-restricted embedded systems. Alternatively, we have
attempted the machine virtualization approach to PEG-based parsing. MiniNez,
our implemented virtual machine, is presented in this paper with several
downsizing techniques, including instruction specialization, inline expansion
and static flow analysis. As a result, the MiniNez machine achieves both a very
small footprint and competitive performance to generated C parsers. We have
demonstrated the experimental results by comparing on two major embedded
platforms: Cortex-A7 and Intel Atom processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03407</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03407</id><created>2015-11-11</created><updated>2015-11-13</updated><authors><author><keyname>Grodet</keyname><forenames>Aymeric</forenames></author><author><keyname>Tsuchiya</keyname><forenames>Takuya</forenames></author></authors><title>An adaptive algorithm for the Euclidean Steiner tree problem in d-space</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a technique to improve Smith's branch-and-bound algorithm for the
Euclidean Steiner tree problem in Rd. The algorithm relies on the enumeration
and optimization of full Steiner topologies for corresponding subsets of
regular points. We handle the case of two Steiner points colliding during the
optimization process - that is when there is a small enough distance between
them - to dynamically change the exploration of the branch-and-bound tree. To
do so, we introduce a simple means of reorganizing a topology to another. This
enables reaching better minima faster, thus allowing the branch-and-bound to be
further pruned. We also correct a mistake in Smith's program by computing a
lower bound for a Steiner tree with a specified topology and using it as a
pruning technique prior to optimization. Because Steiner points lie in the
plane formed by their three neighbors, we can build planar equilateral points
and use them to compute the lower bound, even in dimensions higher than two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03415</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03415</id><created>2015-11-11</created><authors><author><keyname>Sander</keyname><forenames>Oliver</forenames></author><author><keyname>Koch</keyname><forenames>Timo</forenames></author><author><keyname>Schr&#xf6;der</keyname><forenames>Natalie</forenames></author><author><keyname>Flemisch</keyname><forenames>Bernd</forenames></author></authors><title>The Dune FoamGrid implementation for surface and network grids</title><categories>cs.MS cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present FoamGrid, a new implementation of the DUNE grid interface.
FoamGrid implements one- and two-dimensional grids in a physical space of
arbitrary dimension, which allows for grids for curved domains. Even more, the
grids are not expected to have a manifold structure, i.e., more than two
elements can share a common facet. This makes FoamGrid the grid data structure
of choice for simulating structures such as foams, discrete fracture networks,
or network flow problems. FoamGrid implements adaptive non-conforming
refinement with element parametrizations. As an additional feature it allows
removal and addition of elements in an existing grid, which makes FoamGrid
suitable for network growth problems. We show how to use FoamGrid, with
particular attention to the extensions of the grid interface needed to handle
non-manifold topology and grid growth. Three numerical examples demonstrate the
possibilities offered by FoamGrid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03416</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03416</id><created>2015-11-11</created><updated>2015-11-19</updated><authors><author><keyname>Zhu</keyname><forenames>Yuke</forenames></author><author><keyname>Groth</keyname><forenames>Oliver</forenames></author><author><keyname>Bernstein</keyname><forenames>Michael</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Visual7W: Grounded Question Answering in Images</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have seen great progress in basic perceptual tasks such as object
recognition and detection. However, AI models still fail to match humans in
high-level vision tasks due to the lack of capacities for deeper reasoning.
Recently the new task of visual question answering (QA) has been proposed to
evaluate a model's capacity for deep image understanding. Previous works have
established a loose, global association between QA sentences and images.
However, many questions and answers, in practice, relate to local regions in
the images. We establish a semantic link between textual descriptions and image
regions by object-level grounding. It enables a new type of QA with visual
answers, in addition to textual answers used in previous work. We study the
visual QA tasks in a grounded setting with a large collection of 7W
multiple-choice QA pairs. Furthermore, we evaluate human performance and
several baseline models on the QA tasks. Finally, we propose a novel LSTM model
with spatial attention to tackle the 7W QA tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03417</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03417</id><created>2015-11-11</created><authors><author><keyname>Wang</keyname><forenames>Chang-Heng</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author></authors><title>Adaptive Policies for Scheduling with Reconfiguration Delay: An
  End-to-End Solution for All-Optical Data Centers</title><categories>cs.NI math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All-optical switching networks have been considered a promising candidate for
the next generation data center networks thanks to its scalability in data
bandwidth and power efficiency. However, the bufferless nature and the nonzero
recon- figuration delay of optical switches remain great challenges in
deploying all-optical networks. This paper considers the end-to- end scheduling
for all-optical data center networks with no in- network buffer and nonzero
reconfiguration delay. A framework is proposed to deal with the nonzero
reconfiguration delay. The proposed approach constructs an adaptive variant of
any given scheduling policy. It is shown that if a scheduling policy guarantees
its schedules to have schedule weights close to the MaxWeight schedule (and
thus is throughput optimal in the zero reconfiguration regime), then the
throughput optimality is inherited by its adaptive variant (in any nonzero
reconfiguration delay regime). As a corollary, a class of adaptive variants of
the well known MaxWeight policy is shown to achieve throughput optimality
without prior knowledge of the traffic load. Further- more, through numerical
simulations, the simplest such policy, namely the Adaptive MaxWeight (AMW), is
shown to exhibit better delay performance than all prior work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03418</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03418</id><created>2015-11-11</created><updated>2015-11-12</updated><authors><author><keyname>Weir</keyname><forenames>Phil</forenames></author><author><keyname>Reuter</keyname><forenames>Dominic</forenames></author><author><keyname>Ellerweg</keyname><forenames>Roland</forenames></author><author><keyname>Alhonnoro</keyname><forenames>Tuomas</forenames></author><author><keyname>Pollari</keyname><forenames>Mika</forenames></author><author><keyname>Voglreiter</keyname><forenames>Philip</forenames></author><author><keyname>Mariappan</keyname><forenames>Panchatcharam</forenames></author><author><keyname>Flanagan</keyname><forenames>Ronan</forenames></author><author><keyname>Park</keyname><forenames>Chang Sub</forenames></author><author><keyname>Payne</keyname><forenames>Stephen</forenames></author><author><keyname>Staerk</keyname><forenames>Elmar</forenames></author><author><keyname>Voigt</keyname><forenames>Peter</forenames></author><author><keyname>Moche</keyname><forenames>Michael</forenames></author><author><keyname>Kolesnik</keyname><forenames>Marina</forenames></author></authors><title>Go-Smart: Web-based Computational Modeling of Minimally Invasive Cancer
  Treatments</title><categories>cs.CY physics.med-ph</categories><comments>4 pages, 3 figures, submitted to the IEEE International Conference on
  e-Health and Bioengineering (EHB2015) [replaced to inc. IEEE copyright, as
  req. for arXiv by IEEE]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The web-based Go-Smart environment is a scalable system that allows the
prediction of minimally invasive cancer treatment. Interventional radiologists
create a patient-specific 3D model by semi-automatic segmentation and
registration of pre-interventional CT (Computed Tomography) and/or MRI
(Magnetic Resonance Imaging) images in a 2D/3D browser environment. This model
is used to compare patient-specific treatment plans and device performance via
built-in simulation tools. Go-Smart includes evaluation techniques for
comparing simulated treatment with real ablation lesions segmented from
follow-up scans. The framework is highly extensible, allowing manufacturers and
researchers to incorporate new ablation devices, mathematical models and
physical parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03440</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03440</id><created>2015-11-11</created><authors><author><keyname>Klein-Hennig</keyname><forenames>Martin</forenames></author><author><keyname>Dietz</keyname><forenames>Mathias</forenames></author><author><keyname>Hohmann</keyname><forenames>Volker</forenames></author></authors><title>Combination of binaural and harmonic masking release effects in the
  detection of a single component in complex tones</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Harmonic and binaural signal features are relevant for auditory scene
analysis, i.e., the segregation and grouping of sound sources in complex
acoustic scenes. To further investigate how these features combine in the
auditory system, detection thresholds for an 800-Hz tone masked by a diotic
harmonic complex tone were measured in six normal-hearing subjects. The target
tone was presented diotically or with an interaural phase difference (IPD) of
180{\deg} and in harmonic or &quot;mistuned&quot; relationship to the diotic masker.
Three different maskers were used, a resolved and an unresolved complex tone
(fundamental frequency: 160 and 40 Hz) with four components below and above the
target frequency and a broadband unresolved complex tone with 12 additional
components. The target IPD provided release from masking in all masker
conditions, whereas mistuning led to a significant release from masking only in
the diotic conditions with the resolved and the narrowband unresolved maskers.
A significant effect of mistuning was neither found in the diotic condition
with the wideband unresolved masker nor in any of the dichotic conditions. An
auditory model with a single analysis frequency band and different binaural
processing schemes was employed to predict the data of the unresolved masker
conditions. Sensitivity to modulation cues was achieved by including an
auditory-motivated modulation filter in the processing pathway. The predictions
of the diotic data were in line with the experimental results and literature
data in the narrowband condition, but not in the broadband condition. The
experimental and model results in the dichotic conditions hint at a parallel
processing scheme with a binaural processor that has only limited access to
modulation information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03447</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03447</id><created>2015-11-11</created><authors><author><keyname>Cristoforetti</keyname><forenames>Marco</forenames></author><author><keyname>Guerini</keyname><forenames>Marco</forenames></author><author><keyname>Jurman</keyname><forenames>Giuseppe</forenames></author><author><keyname>Furlanello</keyname><forenames>Cesare</forenames></author></authors><title>Community dynamics in connected time-dependent multilayer networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different strategies have been considered to extract information from social
media about how similarly people react to the same news or event. In this
context, a powerful method is offered by the application of graph techniques to
the contents produced by social network users. In particular, large events
typically attract enough content traffic along time to enable an analysis that
explicitly models a dependence from the time dimension. Here we demonstrate how
it is possible to extend the application of community detection strategies in
complex networks to the case of time-dependent multilayer networks, whenever
the connection between consecutive time layers is non-trivial. We apply the
method to 400K Twitter post related to the Expo event held in Milan (Italy)
between May and October 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03451</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03451</id><created>2015-11-11</created><updated>2016-01-27</updated><authors><author><keyname>Bellini</keyname><forenames>Emanuele</forenames></author><author><keyname>Murru</keyname><forenames>Nadir</forenames></author></authors><title>An efficient and secure RSA--like cryptosystem exploiting R\'edei
  rational functions over conics</title><categories>cs.IT cs.CR math.IT math.NT</categories><comments>18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define an isomorphism between the group of points of a conic and the set
of integers modulo a prime equipped with a non-standard product. This product
can be efficiently evaluated through the use of R\'edei rational functions. We
then exploit the isomorphism to construct a novel RSA-like scheme. We compare
our scheme with classic RSA and with RSA-like schemes based on the cubic or
conic equation. The decryption operation of the proposed scheme turns to be two
times faster than RSA, and involves the lowest number of modular inversions
with respect to other RSA-like schemes based on curves. Our solution offers the
same security as RSA in a one-to-one communication and more security in
broadcast applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03459</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03459</id><created>2015-11-11</created><authors><author><keyname>Arachchilage</keyname><forenames>Nalin Asanka Gamagedara</forenames></author></authors><title>User-Centred Security Education: A Game Design to Thwart Phishing
  Attacks</title><categories>cs.CY</categories><comments>3 pages, International Conference: Redefining the R&amp;D Needs for
  Australian Cyber Security on November 16, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phishing is an online identity theft that aims to steal sensitive information
such as username, password and online banking details from its victims.
Phishing education needs to be considered as a means to combat this threat.
This paper reports on a design and development of a mobile game prototype as an
educational tool helping computer users to protect themselves against phishing
attacks. The elements of a game design framework for avoiding phishing attacks
were used to address the game design issues. Game design principles served as
guidelines for structuring and presenting information. Our mobile game design
aimed to enhance the users' avoidance behaviour through motivation to protect
themselves against phishing threats. A think-aloud study was conducted, along
with a pre- and post-test, to assess the game design framework though the
developed mobile game prototype. The study results showed a significant
improvement of participants' phishing avoidance behaviour in their post-test
assessment. Furthermore, the study findings suggest that participants' threat
perception, safeguard effectiveness, self-efficacy, perceived severity and
perceived susceptibility elements positively impact threat avoidance behaviour,
whereas safeguard cost had a negative impact on it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03464</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03464</id><created>2015-11-11</created><authors><author><keyname>Deriu</keyname><forenames>Jan</forenames></author><author><keyname>Jagerman</keyname><forenames>Rolf</forenames></author><author><keyname>Tsay</keyname><forenames>Kai-En</forenames></author></authors><title>A Directional Diffusion Algorithm for Inpainting</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of inpainting involves reconstructing the missing areas of an
image. Inpainting has many applications, such as reconstructing old damaged
photographs or removing obfuscations from images. In this paper we present the
directional diffusion algorithm for inpainting. Typical diffusion algorithms
are bad at propagating edges from the image into the unknown masked regions.
The directional diffusion algorithm improves on the regular diffusion algorithm
by reconstructing edges more accurately. It scores better than regular
diffusion when reconstructing images that are obfuscated by a text mask.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03466</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03466</id><created>2015-11-11</created><updated>2016-02-08</updated><authors><author><keyname>Konyushkova</keyname><forenames>Ksenia</forenames></author><author><keyname>Arvanitopoulos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Robert</keyname><forenames>Zhargalma Dandarova</forenames></author><author><keyname>Brandt</keyname><forenames>Pierre-Yves</forenames></author><author><keyname>S&#xfc;sstrunk</keyname><forenames>Sabine</forenames></author></authors><title>God(s) Know(s): Developmental and Cross-Cultural Patterns in Children
  Drawings</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel approach to data analysis designed for the
needs of specialists in psychology of religion. We detect developmental and
cross-cultural patterns in children's drawings of God(s) and other supernatural
agents. We develop methods to objectively evaluate our empirical observations
of the drawings with respect to: (1) the gravity center, (2) the average
intensities of the colors \emph{green} and \emph{yellow}, (3) the use of
different colors (palette) and (4) the visual complexity of the drawings. We
find statistically significant differences across ages and countries in the
gravity centers and in the average intensities of colors. These findings
support the hypotheses of the experts and raise new questions for further
investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03476</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03476</id><created>2015-11-11</created><authors><author><keyname>Pan</keyname><forenames>Pingbo</forenames></author><author><keyname>Xu</keyname><forenames>Zhongwen</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Wu</keyname><forenames>Fei</forenames></author><author><keyname>Zhuang</keyname><forenames>Yueting</forenames></author></authors><title>Hierarchical Recurrent Neural Encoder for Video Representation with
  Application to Captioning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, deep learning approach, especially deep Convolutional Neural
Networks (ConvNets), have achieved overwhelming accuracy with fast processing
speed for image classification. Incorporating temporal structure with deep
ConvNets for video representation becomes a fundamental problem for video
content analysis. In this paper, we propose a new approach, namely Hierarchical
Recurrent Neural Encoder (HRNE), to exploit temporal information of videos.
Compared to recent video representation inference approaches, this paper makes
the following three contributions. First, our HRNE is able to efficiently
exploit video temporal structure in a longer range by reducing the length of
input information flow, and compositing multiple consecutive inputs at a higher
level. Second, computation operations are significantly lessened while
attaining more non-linearity. Third, HRNE is able to uncover temporal
transitions between frame chunks with different granularities, i.e., it can
model the temporal transitions between frames as well as the transitions
between segments. We apply the new method to video captioning where temporal
information plays a crucial role. Experiments demonstrate that our method
outperforms the state-of-the-art on video captioning benchmarks. Notably, even
using a single network with only RGB stream as input, HRNE beats all the recent
systems which combine multiple inputs, such as RGB ConvNet plus 3D ConvNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03483</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03483</id><created>2015-11-11</created><updated>2016-02-16</updated><authors><author><keyname>He</keyname><forenames>Jun</forenames></author></authors><title>An Analytic Expression of Performance Rate, Fitness Value and Average
  Convergence Rate for a Class of Evolutionary Algorithms</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important theoretical question in evolutionary computation is how good
solutions evolutionary algorithms can produce. This paper aims to provide an
analytic analysis of solution quality of evolutionary algorithms in terms of
the performance rate, which is defined by the difference between 1 and the
approximation ratio of the best solution found in each generation. The
performance rate can be represented by a function of time. With the help of
matrix analysis, it is possible to obtain an exact expression of such a
function. For the first time, an analytic expression for calculating the
performance rate is presented in this paper for a class of evolutionary
algorithms, that is, (1+1) strictly elitist evolution algorithms. Furthermore,
analytic expressions for calculate the fitness value and the average
convergence rate in each generation are also derived for this class of
evolutionary algorithms. The approach is promising, and it can be extended to
non-elitist or population-based algorithms too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03488</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03488</id><created>2015-11-11</created><authors><author><keyname>Lorenzen</keyname><forenames>Matthias</forenames></author><author><keyname>Dabbene</keyname><forenames>Fabrizio</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author><author><keyname>Allg&#xf6;wer</keyname><forenames>Frank</forenames></author></authors><title>Constraint-Tightening and Stability in Stochastic Model Predictive
  Control</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint tightening to non-conservatively guarantee recursive feasibility
and stability in Stochastic Model Predictive Control is addressed. Stability
and feasibility requirements are considered separately, highlighting the
difference between existence of a solution and feasibility of a suitable, a
priori known candidate solution. Subsequently, a Stochastic Model Predictive
Control algorithm which unifies previous results is derived, leaving the
designer the option to balance an increased feasible region against guaranteed
bounds on the asymptotic average performance and convergence time. Besides
typical performance bounds, under mild assumptions, we prove asymptotic
stability in probability of the minimally robust positive invariant set
obtained by the unconstrained LQ-optimal controller. A numerical example,
demonstrating the efficacy of the proposed approach in comparison with
classical, recursively feasible Stochastic MPC and Robust MPC is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03489</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03489</id><created>2015-11-11</created><authors><author><keyname>Haisjackl</keyname><forenames>Cornelia</forenames></author><author><keyname>Zugal</keyname><forenames>Stefan</forenames></author></authors><title>Investigating Differences between Graphical and Textual Declarative
  Process Models</title><categories>cs.SE</categories><journal-ref>Proc. Cognise'14, pp. 194-206, 2014</journal-ref><doi>10.1007/978-3-319-07869-4_17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Declarative approaches to business process modeling are regarded as well
suited for highly volatile environments, as they enable a high degree of
flexibility. However, problems in understanding declarative process models
often impede their adoption. Particularly, a study revealed that aspects that
are present in both imperative and declarative process modeling languages at a
graphical level-while having different semantics-cause considerable troubles.
In this work we investigate whether a notation that does not contain graphical
lookalikes, i.e., a textual notation, can help to avoid this problem. Even
though a textual representation does not suffer from lookalikes, in our
empirical study it performed worse in terms of error rate, duration and mental
effort, as the textual representation forces the reader to mentally merge the
textual information. Likewise, subjects themselves expressed that the graphical
representation is easier to understand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03493</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03493</id><created>2015-11-11</created><authors><author><keyname>Haisjackl</keyname><forenames>Cornelia</forenames></author><author><keyname>Zugal</keyname><forenames>Stefan</forenames></author><author><keyname>Soffer</keyname><forenames>Pnina</forenames></author><author><keyname>Hadar</keyname><forenames>Irit</forenames></author><author><keyname>Reichert</keyname><forenames>Manfred</forenames></author><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author><author><keyname>Weber</keyname><forenames>Barbara</forenames></author></authors><title>Making Sense of Declarative Process Models: Common Strategies and
  Typical Pitfalls</title><categories>cs.SE</categories><journal-ref>Proc. BPMDS'13, pp. 2-17, 2013</journal-ref><doi>10.1007/978-3-642-38484-4_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Declarative approaches to process modeling are regarded as well suited for
highly volatile environments as they provide a high degree of flexibility.
However, problems in understanding and maintaining declarative business process
models impede often their usage. In particular, how declarative models are
understood has not been investigated yet. This paper takes a first step toward
addressing this question and reports on an exploratory study investigating how
analysts make sense of declarative process models. We have handed out
real-world declarative process models to subjects and asked them to describe
the illustrated process. Our qualitative analysis shows that subjects tried to
describe the processes in a sequential way although the models represent
circumstantial information, namely, conditions that produce an outcome, rather
than a sequence of activities. Finally, we observed difficulties with single
building blocks and combinations of relations between activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03495</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03495</id><created>2015-11-11</created><updated>2015-11-23</updated><authors><author><keyname>Valentini</keyname><forenames>Roberto</forenames></author><author><keyname>Dang</keyname><forenames>Nga</forenames></author><author><keyname>Levorato</keyname><forenames>Marco</forenames></author><author><keyname>Bozorgzadeh</keyname><forenames>Eli</forenames></author></authors><title>Modeling And Control Battery Aging in Energy Harvesting Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy storage is a fundamental component for the development of sustainable
and environment-aware technologies. One of the critical challenges that needs
to be overcome is preserving the State of Health (SoH) in energy harvesting
systems, where bursty arrival of energy and load may severely degrade the
battery. Tools from Markov process and Dynamic Programming theory are becoming
an increasingly popular choice to control dynamics of these systems due to
their ability to seamlessly incorporate heterogeneous components and support a
wide range of applications. Mapping aging rate measures to fit within the
boundaries of these tools is non-trivial. In this paper, a framework for
modeling and controlling the aging rate of batteries based on Markov process
theory is presented. Numerical results illustrate the tradeoff between battery
degradation and task completion delay enabled by the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03501</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03501</id><created>2015-11-11</created><authors><author><keyname>Avvakumov</keyname><forenames>S.</forenames></author><author><keyname>Mabillard</keyname><forenames>I.</forenames></author><author><keyname>Skopenkov</keyname><forenames>A.</forenames></author><author><keyname>Wagner</keyname><forenames>U.</forenames></author></authors><title>Eliminating Higher-Multiplicity Intersections, III. Codimension 2</title><categories>math.GT cs.CG math.CO</categories><comments>16 pages, no figures</comments><msc-class>57Q35, 55S91, 52A35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study conditions under which a finite simplicial complex $K$ can be mapped
to $d$-dimensional Euclidean space $\mathbb R^d$ without higher-multiplicity
intersections. An almost $r$-embedding is a map $f:K\to \mathbb R^d$ such that
the images of any $r$ pairwise disjoint simplices of $K$ do not have a common
point. We show that if $r$ is not a prime power and $d\ge 2r$, then there is a
counterexample to the topological Tverberg conjecture, i.e., there is an almost
$r$-embedding of the $(d+1)(r-1)$-simplex in $\mathbb R^d$. This improves on
previous constructions by Frick (for $d\ge 3r+1$) and by the second and the
fourth author (for $d\ge 3r$).
  The counterexamples are obtained by proving the following algebraic criterion
in codimension 2: If $r\ge3$ and if $K$ is a finite $2(r-1)$-complex then there
exists an almost $r$-embedding $K\to \mathbb R^{2r}$ if and only if there
exists a general position PL map $f: K\to \mathbb R^{2r}$ such that the
algebraic intersection number of the $f$-images of any $r$ pairwise disjoint
simplices of $K$ is zero. This result can be restated in terms of cohomological
obstructions or equivariant maps, and extends an analogous codimension $3$
criterion by the second and fourth author. Krushkal and Teichner essentially
showed that the analogous criterion for $r=2$ is false. We give a short
elementary proof of their beautiful lemma on singular 4-dimensional Borromean
rings, yielding a short proof of the counterexample.
  As another application of our methods, we classify ornaments $f: S^3 \sqcup
S^3 \sqcup S^3\to \mathbb R^5$ up to ornament concordance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03512</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03512</id><created>2015-11-06</created><updated>2015-11-28</updated><authors><author><keyname>Gavili</keyname><forenames>Adnan</forenames></author><author><keyname>Zhang</keyname><forenames>Xiao-Ping</forenames></author></authors><title>On the Shift Operator and Optimal Filtering in Graph Signal Processing</title><categories>math.SP cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defining a sound shift operator for signals existing on a certain graph
structure, similar to the well-defined shift operator in classical signal
processing, is a crucial problem in graph signal processing. Since almost all
operations, such as filtering, transformation, prediction, etc., are directly
related to the graph shift operator. We define a unique set of shift operators
that satisfy all properties of the shift operator in the classical signal
processing, especially energy preservation property. Our definition of the
graph shift operator negates the shift operators defined in the literature,
such as the \textit{graph adjacency matrix} as the shift operator, and
\textit{Laplacian matrix} based shift operators, etc., which modify the energy
of a graph signal. We further show that any shift invariant graph filter can be
written as a polynomial function of the graph shift operator. We also show that
the adjacency matrix of a graph is indeed a linear shift invariant graph filter
with respect to the defined shift operator. Inheriting the concepts of finite
impulse response (FIR) and infinite impulse from classical signal processing,
we introduce graph finite impulse response (GFIR) and graph infinite impulse
response (GIIR) filters and obtain explicit forms for such filters. We further
obtain the optimal filtering on graphs, i.e., the corresponding \emph{Wiener}
filtering on graphs, and elaborate on the structure of such filters for any
arbitrary graph structure. We specially treat the directed cyclic graph and
show that the optimal linear shift invariant filter is indeed the \emph{Wiener}
filter in classical signal processing. This result show that, optimal linear
time invariant filters for time series data is a subset of optimal graph
filters. We also elaborate on the best linear predictor graph filters and
optimal filters for product graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03518</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03518</id><created>2015-11-11</created><authors><author><keyname>An</keyname><forenames>Ya-Hui</forenames></author><author><keyname>Dong</keyname><forenames>Qiang</forenames></author><author><keyname>Sun</keyname><forenames>Chong-Jing</forenames></author><author><keyname>Nie</keyname><forenames>Da-Cheng</forenames></author><author><keyname>Fu</keyname><forenames>Yan</forenames></author></authors><title>Diffuse-like recommender with enhanced similarity of objects</title><categories>cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In last decades, diversity and accuracy have been regarded as two essential
measures in evaluating recommender systems. However, a clear concern is that a
model focusing excessively on one measure will put the other one at risk, which
means that it is not easy to greatly improve diversity and accuracy
simultaneously. In this paper, we propose a method which can be used in many
being diffuse-like algorithms, like ProbS, BHC and HHP, to get more relevant
and personalized recommendation results on user-object bipartite networks. The
main idea is to enhance the RA similarity of objects in the transfer equations,
by giving a tuneable exponent on the shoulder of RA similarity, to
intentionally regulate the resource allocation on objects of different degrees.
Experiments on three benchmark data sets, MovieLens, Netflix, and RYM show that
the modified algorithms can yield remarkable performance improvement compared
with the original models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03524</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03524</id><created>2015-11-10</created><authors><author><keyname>Sau</keyname><forenames>B.</forenames></author><author><keyname>Mukhopadhyaya</keyname><forenames>S.</forenames></author><author><keyname>Mukhopadhyaya</keyname><forenames>K.</forenames></author></authors><title>MAINT: Localization of Mobile Sensors with Energy Control</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization is an important issue for Wireless Sensor Networks (WSN). A
mobile sensor may change its position rapidly and thus require localization
calls frequently. A localization may require network wide information and
increase traffic over the network. It dissipates valuable energy for message
communication. Thus localization is very costly. The control of the number of
localization calls may save energy consumption, as it is rather expensive. To
reduce the frequency of localization calls for a mobile sensor, we propose a
technique that involves \textit{Mobility Aware Interpolation} (MAINT) for
position estimation. It controls the number of localizations which gives much
better result than the existing localization control schemes using mobility
aware extrapolation. The proposed method involves very low arithmetic
computation overheads. We find analytical expressions for the expected error in
position estimation. A parameter, the time interval, has been introduced to
externally control the energy dissipation. Simulation studies are carried out
to compare the performances of the proposed method with some existing
localization control schemes as well as the theoretical results. The simulation
results shows that the expected error at any point of time may be computed from
this expression. We have seen that constant error limit can be maintained
increasing the time period of localization proportional to rate of change of
direction of its motion. Increasing time period, the energy may be saved with a
stable error limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03528</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03528</id><created>2015-11-10</created><updated>2015-11-21</updated><authors><author><keyname>H&#xf6;ller</keyname><forenames>Andrea</forenames></author><author><keyname>Rauter</keyname><forenames>Tobias</forenames></author><author><keyname>Iber</keyname><forenames>Johannes</forenames></author><author><keyname>Macher</keyname><forenames>Georg</forenames></author><author><keyname>Kreiner</keyname><forenames>Christian</forenames></author></authors><title>Software-Based Fault Recovery via Adaptive Diversity for COTS Multi-Core
  Processors</title><categories>cs.SE cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ever growing demands of embedded systems to satisfy high computing
performance and cost efficiency lead to the trend of using commercial
off-the-shelf hardware. However, due to their highly integrated design they are
becoming increasingly susceptible to hardware errors (e.g. caused by
radiation-induced soft-errors or wear-out effects). Since such faults cannot be
fully prevented, systems have to cope with their effects. At the same time
there is the trend of multi-core processors in embedded systems. Approaches to
achieve fault tolerance by using the multiple cores to establish redundancy
have been presented in literature. However, typically only homogeneous
redundancy techniques are considered to tolerate soft errors. However, there is
a lack of appropriate reaction mechanisms for restoring the system in case of
permanent hardware faults.
  Here, we propose the basic idea of enhancing multi-core redundancy techniques
with a cost-efficient automated introduction of diversity in the executed
software replicas. Recently, these automated software diversity techniques have
attracted attention in the security domain. We propose to use these techniques
to recover from permanent hardware faults. This is achieved by adapting the
software execution in such a way that permanent faults are mitigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03531</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03531</id><created>2015-11-11</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Buchler</keyname><forenames>Norbou</forenames></author><author><keyname>Schaefer</keyname><forenames>Kristin E.</forenames></author></authors><title>Kinetic and Cyber</title><categories>cs.CY cs.HC</categories><comments>A version of this paper appeared as a book chapter in Cyber Defense
  and Situational Awareness, Springer, 2014. Prepared by US Government
  employees in their official duties; approved for public release, distribution
  unlimited. Cyber Defense and Situational Awareness. Springer International
  Publishing, 2014. 29-45</comments><msc-class>c.2.0, k.6.5</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare and contrast situation awareness in cyber warfare and in
conventional, kinetic warfare. Situation awareness (SA) has a far longer
history of study and applications in such areas as control of complex
enterprises and in conventional warfare, than in cyber warfare. Far more is
known about the SA in conventional military conflicts, or adversarial
engagements, than in cyber ones. By exploring what is known about SA in
conventional, also commonly referred to as kinetic, battles, we may gain
insights and research directions relevant to cyber conflicts. We discuss the
nature of SA in conventional (often called kinetic) conflict, review what is
known about this kinetic SA (KSA), and then offer a comparison with what is
currently understood regarding the cyber SA (CSA). We find that challenges and
opportunities of KSA and CSA are similar or at least parallel in several
important ways. With respect to similarities, in both kinetic and cyber worlds,
SA strongly impacts the outcome of the mission. Also similarly, cognitive
biases are found in both KSA and CSA. As an example of differences, KSA often
relies on commonly accepted, widely used organizing representation - map of the
physical terrain of the battlefield. No such common representation has emerged
in CSA, yet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03532</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03532</id><created>2015-11-11</created><updated>2015-11-12</updated><authors><author><keyname>Keles</keyname><forenames>Ali</forenames></author><author><keyname>Keles</keyname><forenames>Ayturk</forenames></author></authors><title>IBMMS Decision Support Tool For Management of Bank Telemarketing
  Campaigns</title><categories>cs.AI</categories><comments>15 pages, 4 figures, 4 tables, journal in International Journal of
  Database Management Systems, Vol.7, No.5, October 2015</comments><doi>10.5121/ijdms.2015.7501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although direct marketing is a good method for banks to utilize in the face
of global competition and the financial crisis, it has been shown to exhibit
poor performance. However, there are some drawbacks to direct campaigns, such
as those related to improving the negative attributes that customers ascribe to
banks. To overcome these problems, attractive long-term deposit campaigns
should be organized and managed more effectively. The aim of this study is to
develop an Intelligent Bank Market Management System (IBMMS) for bank managers
who want to manage efficient marketing campaigns. IBMMS is the first system
developed by combining the power of data mining with the capabilities of expert
systems in this area. Moreover, IBMMS includes important features that enable
it to be intelligent: a knowledge base, an inference engine and an advisor.
Using this system, a manager can successfully direct marketing campaigns and
follow the decision schemas of customers both as individuals and as a group;
moreover, a manager can make decisions that lead to the desired response by
customers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03546</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03546</id><created>2015-11-11</created><updated>2015-11-25</updated><authors><author><keyname>Zhou</keyname><forenames>Guorui</forenames></author><author><keyname>Chen</keyname><forenames>Guang</forenames></author></authors><title>Hierarchical Latent Semantic Mapping for Automated Topic Generation</title><categories>cs.LG cs.CL cs.IR</categories><comments>9 pages, 3 figures, Under Review as a conference at ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of information sits in an unprecedented amount of text data. Managing
allocation of these large scale text data is an important problem for many
areas. Topic modeling performs well in this problem. The traditional generative
models (PLSA,LDA) are the state-of-the-art approaches in topic modeling and
most recent research on topic generation has been focusing on improving or
extending these models. However, results of traditional generative models are
sensitive to the number of topics K, which must be specified manually. The
problem of generating topics from corpus resembles community detection in
networks. Many effective algorithms can automatically detect communities from
networks without a manually specified number of the communities. Inspired by
these algorithms, in this paper, we propose a novel method named Hierarchical
Latent Semantic Mapping (HLSM), which automatically generates topics from
corpus. HLSM calculates the association between each pair of words in the
latent topic space, then constructs a unipartite network of words with this
association and hierarchically generates topics from this network. We apply
HLSM to several document collections and the experimental comparisons against
several state-of-the-art approaches demonstrate the promising performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03552</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03552</id><created>2015-11-11</created><updated>2016-01-09</updated><authors><author><keyname>Zhang</keyname><forenames>Bruce</forenames></author><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Granqvist</keyname><forenames>Claes-Goran</forenames></author></authors><title>Drawing from hats by noise-based logic</title><categories>cs.ET cs.CC</categories><comments>Accepted for Publication in the International Journal of Parallel,
  Emergent and Distributed Systems. December 17, 2015</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We utilize the asymmetric random telegraph wave-based instantaneous
noise-base logic scheme to represent the problem of drawing numbers from a hat,
and we consider two identical hats with the first 2^N integer numbers. In the
first problem, Alice secretly draws an arbitrary number from one of the hats,
and Bob must find out which hat is missing a number. In the second problem,
Alice removes a known number from one of the hats and another known number from
the other hat, and Bob must identify these hats. We show that, when the
preparation of the hats with the numbers is accounted for, the noise-based
logic scheme always provides an exponential speed-up and/or it requires
exponentially smaller computational complexity than deterministic alternatives.
Both the stochasticity and the ability to superpose numbers are essential
components of the exponential improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03555</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03555</id><created>2015-11-11</created><authors><author><keyname>Obiria</keyname><forenames>Peter B.</forenames></author><author><keyname>Kimwele</keyname><forenames>Micheal W.</forenames></author><author><keyname>Cheruiyot</keyname><forenames>Wilson K.</forenames></author><author><keyname>Mwangi</keyname><forenames>Gitau</forenames></author></authors><title>A Location-Based Privacy Preserving Framework for M-Learning Adoption to
  Enhance Distance Education in Kenya: Literature Review</title><categories>cs.CY</categories><comments>7 pages. International Journal of Advanced Research in Computer and
  Communication Engineering, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to study m-learning literature in order to propose
and develop a privacy-preserving framework which can be used to foster
sustainable deployment of mobile learning within open and distance education in
Kenya. Location-based privacy in mobile learning is essential to retain users
trust, key to influencing usage intention. Any risk on privacy can negatively
affect users perceptions of a systems reliability and trustworthiness. While
extant studies have proposed frameworks for mobile technologies adoption into
learning, few have integrated privacy aspects and their influence on m-learning
implementation. The framework would provide University management with informed
approach to consider privacy preserving aspects in m-learning implementation.
Also, it could provide enlightened guidance to mobile learning application
developers on the need to cater for learners privacy aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03561</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03561</id><created>2015-11-11</created><authors><author><keyname>Pennanen</keyname><forenames>Harri</forenames></author><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Distributed Coordinated Beamforming for Multi-cell Multigroup Multicast
  Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers coordinated multicast beamforming in a multi-cell
wireless network. Each multiantenna base station (BS) serves multiple groups of
single antenna users by generating a single beam with common data per group.
The aim is to minimize the sum power of BSs while satisfying user-specific SINR
targets. We propose centralized and distributed multicast beamforming
algorithms for multi-cell multigroup systems. The NP-hard multicast problem is
tackled by approximating it as a convex problem using the standard semidefinite
relaxation method. The resulting semidefinite program (SDP) can be solved via
centralized processing if global channel knowledge is available. To allow a
distributed implementation, the primal decomposition method is used to turn the
SDP into two optimization levels. The higher level is in charge of optimizing
inter-cell interference while the lower level optimizes beamformers for given
inter-cell interference constraints. The distributed algorithm requires local
channel knowledge at each BS and scalar information exchange between BSs. If
the solution has unit rank, it is optimal for the original problem. Otherwise,
the Gaussian randomization method is used to find a feasible solution. The
superiority of the proposed algorithms over conventional schemes is
demonstrated via numerical evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03570</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03570</id><created>2015-11-09</created><authors><author><keyname>Montufar</keyname><forenames>Guido</forenames></author><author><keyname>Morton</keyname><forenames>Jason</forenames></author></authors><title>Dimension of Marginals of Kronecker Product Models</title><categories>stat.ML cs.NE math.AG math.CO math.PR</categories><comments>27 pages, 2 figures</comments><msc-class>14T05, 52B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Kronecker product model is the set of visible marginal probability
distributions of an exponential family whose sufficient statistics matrix
factorizes as a Kronecker product of two matrices, one for the visible
variables and one for the hidden variables. We estimate the dimension of these
models by the maximum rank of the Jacobian in the limit of large parameters.
The limit is described by the tropical morphism; a piecewise linear map with
pieces corresponding to slicings of the visible matrix by the normal fan of the
hidden matrix. We obtain combinatorial conditions under which the model has the
expected dimension, equal to the minimum of the number of natural parameters
and the dimension of the ambient probability simplex. Additionally, we prove
that the binary restricted Boltzmann machine always has the expected dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03575</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03575</id><created>2015-11-11</created><authors><author><keyname>Kone&#x10d;n&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>McMahan</keyname><forenames>Brendan</forenames></author><author><keyname>Ramage</keyname><forenames>Daniel</forenames></author></authors><title>Federated Optimization:Distributed Optimization Beyond the Datacenter</title><categories>cs.LG math.OC</categories><comments>NIPS workshop version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new and increasingly relevant setting for distributed
optimization in machine learning, where the data defining the optimization are
distributed (unevenly) over an extremely large number of \nodes, but the goal
remains to train a high-quality centralized model. We refer to this setting as
Federated Optimization. In this setting, communication efficiency is of utmost
importance.
  A motivating example for federated optimization arises when we keep the
training data locally on users' mobile devices rather than logging it to a data
center for training. Instead, the mobile devices are used as nodes performing
computation on their local data in order to update a global model. We suppose
that we have an extremely large number of devices in our network, each of which
has only a tiny fraction of data available totally; in particular, we expect
the number of data points available locally to be much smaller than the number
of devices. Additionally, since different users generate data with different
patterns, we assume that no device has a representative sample of the overall
distribution.
  We show that existing algorithms are not suitable for this setting, and
propose a new algorithm which shows encouraging experimental results. This work
also sets a path for future research needed in the context of federated
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03576</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03576</id><created>2015-11-11</created><authors><author><keyname>Khabbaz</keyname><forenames>Mohammad</forenames></author></authors><title>DataGrinder: Fast, Accurate, Fully non-Parametric Classification
  Approach Using 2D Convex Hulls</title><categories>cs.DB cs.CG cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been a long time, since data mining technologies have made their ways
to the field of data management. Classification is one of the most important
data mining tasks for label prediction, categorization of objects into groups,
advertisement and data management. In this paper, we focus on the standard
classification problem which is predicting unknown labels in Euclidean space.
Most efforts in Machine Learning communities are devoted to methods that use
probabilistic algorithms which are heavy on Calculus and Linear Algebra. Most
of these techniques have scalability issues for big data, and are hardly
parallelizable if they are to maintain their high accuracies in their standard
form. Sampling is a new direction for improving scalability, using many small
parallel classifiers. In this paper, rather than conventional sampling methods,
we focus on a discrete classification algorithm with O(n) expected running
time. Our approach performs a similar task as sampling methods. However, we use
column-wise sampling of data, rather than the row-wise sampling used in the
literature. In either case, our algorithm is completely deterministic. Our
algorithm, proposes a way of combining 2D convex hulls in order to achieve high
classification accuracy as well as scalability in the same time. First, we
thoroughly describe and prove our O(n) algorithm for finding the convex hull of
a point set in 2D. Then, we show with experiments our classifier model built
based on this idea is very competitive compared with existing sophisticated
classification algorithms included in commercial statistical applications such
as MATLAB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03592</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03592</id><created>2015-11-11</created><authors><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Kane</keyname><forenames>Daniel M.</forenames></author><author><keyname>Stewart</keyname><forenames>Alistair</forenames></author></authors><title>The Fourier Transform of Poisson Multinomial Distributions and its
  Algorithmic Applications</title><categories>cs.DS cs.GT cs.LG math.PR math.ST stat.TH</categories><comments>67 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $(n, k)$-Poisson Multinomial Distribution (PMD) is a random variable of
the form $X = \sum_{i=1}^n X_i$, where the $X_i$'s are independent random
vectors supported on the set of standard basis vectors in $\mathbb{R}^k.$ In
this paper, we obtain a refined structural understanding of PMDs by analyzing
their Fourier transform. As our core structural result, we prove that the
Fourier transform of PMDs is {\em approximately sparse}, i.e., roughly
speaking, its $L_1$-norm is small outside a small set. By building on this
result, we obtain the following applications:
  {\bf Learning Theory.} We design the first computationally efficient learning
algorithm for PMDs with respect to the total variation distance. Our algorithm
learns an arbitrary $(n, k)$-PMD within variation distance $\epsilon$ using a
near-optimal sample size of $\widetilde{O}_k(1/\epsilon^2),$ and runs in time
$\widetilde{O}_k(1/\epsilon^2) \cdot \log n.$ Previously, no algorithm with a
$\mathrm{poly}(1/\epsilon)$ runtime was known, even for $k=3.$
  {\bf Game Theory.} We give the first efficient polynomial-time approximation
scheme (EPTAS) for computing Nash equilibria in anonymous games. For normalized
anonymous games with $n$ players and $k$ strategies, our algorithm computes a
well-supported $\epsilon$-Nash equilibrium in time $n^{O(k^3)} \cdot
(k/\epsilon)^{O(k^3\log(k/\epsilon)/\log\log(k/\epsilon))^{k-1}}.$ The best
previous algorithm for this problem had running time $n^{(f(k)/\epsilon)^k},$
where $f(k) = \Omega(k^{k^2})$, for any $k&gt;2.$
  {\bf Statistics.} We prove a multivariate central limit theorem (CLT) that
relates an arbitrary PMD to a discretized multivariate Gaussian with the same
mean and covariance, in total variation distance. Our new CLT strengthens the
CLT of Valiant and Valiant by completely removing the dependence on $n$ in the
error bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03595</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03595</id><created>2015-11-11</created><authors><author><keyname>Gallagher</keyname><forenames>John P.</forenames></author><author><keyname>Ajspur</keyname><forenames>Mai</forenames></author><author><keyname>Kafle</keyname><forenames>Bishoksan</forenames></author></authors><title>An Optimised Algorithm for Determinisation and Completion of Finite Tree
  Automata</title><categories>cs.FL</categories><report-no>145</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determinisation is an important concept in the theory of finite tree
automata. However the complexity of the textbook procedure for determinisation
is such that it is not viewed as a being a practical procedure for manipulating
tree automata, even fairly small ones. The computational problems are
exacerbated when an automaton has to be both determinised and completed, for
instance to compute the complement of an automaton. In this paper we develop an
algorithm for determinisation and completion of finite tree automata, whose
worst-case complexity remains unchanged, but which performs dramatically better
than existing algorithms in practice. The algorithm is developed in stages by
optimising the textbook algorithm. A critical aspect of the algorithm is that
the transitions of the determinised automaton are generated in a potentially
very compact form called product form, which can often be used directly when
manipulating the determinised automaton. The paper contains an experimental
evaluation of the algorithm on a large set of tree automata examples.
Applications of the algorithm include static analysis of term rewriting systems
and logic programs, and checking containment of languages defined by tree
automata such as XML schemata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03597</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03597</id><created>2015-11-11</created><authors><author><keyname>Jeng</keyname><forenames>Wei</forenames></author><author><keyname>DesAutels</keyname><forenames>Spencer</forenames></author><author><keyname>He</keyname><forenames>Daqing</forenames></author><author><keyname>Li</keyname><forenames>Lei</forenames></author></authors><title>Information Exchange on an Academic Social Networking Site: A
  Multi-discipline Comparison on ResearchGate Q&amp;A</title><categories>cs.SI</categories><comments>To appear in Journal of the Association for Information Science and
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing popularity of academic social networking sites (ASNSs)
requires studies on the usage of ASNSs among scholars, and evaluations of the
effectiveness of these ASNSs. However, it is unclear whether current ASNSs have
fulfilled their design goal, as scholars' actual online interactions on these
platforms remain unexplored. To fill the gap, this paper presents a study based
on data collected from ResearchGate. Adopting a mixed-method design by
conducting qualitative content analysis and statistical analysis on 1128 posts
collected from ResearchGate Q&amp;A, we examine how scholars exchange information
and resources, and how their practices vary across three distinct disciplines:
Library and Information Services, History of Art, and Astrophysics.
  Our results show that the effect of a questioner' intention (i.e., seeking
information or discussion) is greater than discipline in some circumstances.
Across the three disciplines, responses to these questions all provide various
resources, including experts' contact details, citations, links to Wikipedia,
images, etc. We further discuss several implications to the understanding of
scholarly information exchange and the design of better academic social
networking interfaces, which should aim to stimulate scholarly interactions by
minimizing confusion, improving the clarity of questions, and promote scholarly
content management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03599</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03599</id><created>2015-11-11</created><authors><author><keyname>Ad&#xe1;mek</keyname><forenames>Karel</forenames></author><author><keyname>Novotn&#xfd;</keyname><forenames>Jan</forenames></author><author><keyname>Armour</keyname><forenames>Wes</forenames></author></authors><title>A polyphase filter for many-core architectures</title><categories>astro-ph.IM cs.DC</categories><comments>19 pages, 20 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we discuss our implementation of a polyphase filter for
real-time data processing in radio astronomy. We describe in detail our
implementation of the polyphase filter algorithm and its behaviour on three
generations of NVIDIA GPU cards, on dual Intel Xeon CPUs and the Intel Xeon Phi
(Knights Corner) platforms. All of our implementations aim to exploit the
potential for data reuse that the algorithm offers. Our GPU implementations
explore two different methods for achieving this, the first makes use of
L1/Texture cache, the second uses shared memory. We discuss the usability of
each of our implementations along with their behaviours. We measure performance
in execution time, which is a critical factor for real-time systems, we also
present results in terms of bandwidth (GB/s), compute (GFlop/s) and type
conversions (GTc/s). We include a presentation of our results in terms of the
sample rate which can be processed in real-time by a chosen platform, which
more intuitively describes the expected performance in a signal processing
setting. Our findings show that, for the GPUs considered, the performance of
our polyphase filter when using lower precision input data is limited by type
conversions rather than device bandwidth. We compare these results to an
implementation on the Xeon Phi. We show that our Xeon Phi implementation has a
performance that is 1.47x to 1.95x greater than our CPU implementation, however
is not insufficient to compete with the performance of GPUs. We conclude with a
comparison of our best performing code to two other implementations of the
polyphase filter, showing that our implementation is faster in nearly all
cases. This work forms part of the Astro-Accelerate project, a many-core
accelerated real-time data processing library for digital signal processing of
time-domain radio astronomy data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03602</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03602</id><created>2015-11-11</created><authors><author><keyname>Zimand</keyname><forenames>Marius</forenames></author></authors><title>Kolmogorov complexity version of Slepian-Wolf coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alice and Bob are given two correlated $n$-bit strings $x_1$ and,
respectively, $x_2$, which they want to losslessly compress. They can either
collaborate by sharing their strings, or work separately. We show that there is
no disadvantage in the second scenario, and they can achieve almost optimal
compression in the sense of Kolmogorov complexity, provided the decompression
algorithm knows the complexity profile of $x_1$ and $x_2$. Furthermore,
compression can be made at any combination of lengths that satisfy some
necessary conditions (modulo additive polylog terms). This is an analog in the
framework of algorithmic information theory of the classic Slepian-Wolf
Theorem, a fundamental result in network information theory, in which $x_1$ and
$x_2$ are realizations of two discrete random variables formed by taking $n$
independent drawings from a joint distribution. In our result no type of
independence is assumed and it still is the case that distributed compression
is on a par with centralized compression. % Taking into account the well-known
relation between Shannon entropy and Kolmogorov complexity, our result implies
the Slepian-Wolf theorem (modulo the polylog overhead), with the added feature
that distributed compression close to entropy is done in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03603</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03603</id><created>2015-11-11</created><authors><author><keyname>B.</keyname><forenames>Amir H. Kargar</forenames></author><author><keyname>Mollahosseini</keyname><forenames>Ali</forenames></author><author><keyname>Struemph</keyname><forenames>Taylor</forenames></author><author><keyname>Pace</keyname><forenames>Wilson</forenames></author><author><keyname>Nielsen</keyname><forenames>Rodney D.</forenames></author><author><keyname>Mahoor</keyname><forenames>Mohammad H.</forenames></author></authors><title>Automatic Measurement of Physical Mobility in Get-Up-and-Go Test Using
  Kinect Sensor</title><categories>cs.HC</categories><comments>Published in: Engineering in Medicine and Biology Society (EMBC),
  2014 36th Annual International Conference of the IEEE</comments><doi>10.1109/EMBC.2014.6944375</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Get-Up-and-Go Test is commonly used for assessing the physical mobility of
the elderly by physicians. This paper presents a method for automatic analysis
and classification of human gait in the Get-Up-and-Go Test using a Microsoft
Kinect sensor. Two types of features are automatically extracted from the human
skeleton data provided by the Kinect sensor. The first type of feature is
related to the human gait (e.g., number of steps, step duration, and turning
duration); whereas the other one describes the anatomical configuration (e.g.,
knee angles, leg angle, and distance between elbows). These features
characterize the degree of human physical mobility. State-of-the-art machine
learning algorithms (i.e. Bag of Words and Support Vector Machines) are used to
classify the severity of gaits in 12 subjects with ages ranging between 65 and
90 enrolled in a pilot study. Our experimental results show that these features
can discriminate between patients who have a high risk for falling and patients
with a lower fall risk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03607</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03607</id><created>2015-11-11</created><updated>2015-11-29</updated><authors><author><keyname>Sun</keyname><forenames>Ju</forenames></author><author><keyname>Qu</keyname><forenames>Qing</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>Complete Dictionary Recovery over the Sphere I: Overview and the
  Geometric Picture</title><categories>cs.IT cs.CV math.IT math.OC stat.ML</categories><comments>The first of two papers based on the report arXiv:1504.06785.
  Submitted to IEEE Transaction on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a complete (i.e., square and
invertible) matrix $\mathbf A_0$, from $\mathbf Y \in \mathbb{R}^{n \times p}$
with $\mathbf Y = \mathbf A_0 \mathbf X_0$, provided $\mathbf X_0$ is
sufficiently sparse. This recovery problem is central to the theoretical
understanding of dictionary learning, which seeks a sparse representation for a
collection of input signals, and finds numerous applications in modern signal
processing and machine learning. We give the first efficient algorithm that
provably recovers $\mathbf A_0$ when $\mathbf X_0$ has $O(n)$ nonzeros per
column, under suitable probability model for $\mathbf X_0$. In contrast, prior
results based on efficient algorithms provide recovery guarantees when $\mathbf
X_0$ has only $O(n^{1-\delta})$ nonzeros per column for any constant $\delta
\in (0, 1)$.
  Our algorithmic pipeline centers around solving a certain nonconvex
optimization problem with a spherical constraint. In this paper, we provide a
geometric characterization of the high-dimensional objective landscape. In
particular, we show that the problem is highly structured: with high
probability there are no &quot;spurious&quot; local minimizers and all saddle points are
second-order. This distinctive structure makes the problem amenable to
efficient algorithms. In a companion paper (arXiv:1511.04777), we design a
second-order trust-region algorithm over the sphere that provably converges to
a local minimizer with an arbitrary initialization, despite the presence of
saddle points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03609</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03609</id><created>2015-11-11</created><authors><author><keyname>Costin</keyname><forenames>Andrei</forenames></author><author><keyname>Zarras</keyname><forenames>Apostolis</forenames></author><author><keyname>Francillon</keyname><forenames>Aur&#xe9;lien</forenames></author></authors><title>Automated Dynamic Firmware Analysis at Scale: A Case Study on Embedded
  Web Interfaces</title><categories>cs.CR cs.DC cs.NI</categories><acm-class>D.4.6; K.4.4; K.6.5; K.6.m; C.3; D.2.7; C.1.4; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedded devices are becoming more widespread, interconnected, and
web-enabled than ever. However, recent studies showed that these devices are
far from being secure. Moreover, many embedded systems rely on web interfaces
for user interaction or administration. Unfortunately, web security is known to
be difficult, and therefore the web interfaces of embedded systems represent a
considerable attack surface.
  In this paper, we present the first fully automated framework that applies
dynamic firmware analysis techniques to achieve, in a scalable manner,
automated vulnerability discovery within embedded firmware images. We apply our
framework to study the security of embedded web interfaces running in
Commercial Off-The-Shelf (COTS) embedded devices, such as routers, DSL/cable
modems, VoIP phones, IP/CCTV cameras. We introduce a methodology and implement
a scalable framework for discovery of vulnerabilities in embedded web
interfaces regardless of the vendor, device, or architecture. To achieve this
goal, our framework performs full system emulation to achieve the execution of
firmware images in a software-only environment, i.e., without involving any
physical embedded devices. Then, we analyze the web interfaces within the
firmware using both static and dynamic tools. We also present some interesting
case-studies, and discuss the main challenges associated with the dynamic
analysis of firmware images and their web interfaces and network services. The
observations we make in this paper shed light on an important aspect of
embedded devices which was not previously studied at a large scale.
  We validate our framework by testing it on 1925 firmware images from 54
different vendors. We discover important vulnerabilities in 185 firmware
images, affecting nearly a quarter of vendors in our dataset. These
experimental results demonstrate the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03611</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03611</id><created>2015-11-11</created><updated>2016-01-21</updated><authors><author><keyname>Alizadeh</keyname><forenames>Mahnoosh</forenames></author><author><keyname>Wai</keyname><forenames>Hoi-To</forenames></author><author><keyname>Chowdhury</keyname><forenames>Mainak</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author></authors><title>Joint Management of Electric Vehicles in Coupled Power and
  Transportation Networks</title><categories>cs.SY cs.MA</categories><comments>Submitted to IEEE Transactions on Control of Network Systems on June
  1st 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the system-level effects of the introduction of large populations of
Electric Vehicles on the power and transportation networks. We assume that each
EV owner solves a decision problem to pick a cost-minimizing charge and travel
plan. This individual decision is affected by traffic congestion in the
transportation network, affecting travel times, as well as as congestion in the
power grid, resulting in spatial variations in electricity prices for battery
charging. We show that this decision problem is equivalent to finding the
shortest path on an &quot;extended&quot; transportation graph, with virtual arcs that
represent charging options. Using this extended graph, we study the collective
effects of a large number of EV owners individually solving this path planning
problem. We propose a scheme in which independent power and transportation
system operators can collaborate to manage each network towards a socially
optimum operating point while keeping the operational data of each system
private. We further study the optimal reserve capacity requirements for pricing
in the absence of such collaboration. We showcase numerically that a lack of
attention to interdependencies between the two infrastructures can have adverse
operational effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03614</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03614</id><created>2015-10-23</created><authors><author><keyname>Smirnov</keyname><forenames>Alexander V.</forenames></author></authors><title>FIESTA 4: optimized Feynman integral calculations with GPU support</title><categories>hep-ph cs.MS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1312.3186</comments><report-no>TTP15-036</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new major release of the program FIESTA (Feynman
Integral Evaluation by a Sector decomposiTion Approach). The new release is
mainly aimed at optimal performance at large scales when one is increasing the
number of sampling points in order to reduce the uncertainty estimates. The
release now supports graphical processor units (GPU) for the numerical
integration, methods to optimize cluster-usage, as well as other speed, memory,
and stability improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03629</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03629</id><created>2015-11-11</created><authors><author><keyname>Baxter</keyname><forenames>John S. H.</forenames></author><author><keyname>McLeod</keyname><forenames>Jonathan</forenames></author><author><keyname>Peters</keyname><forenames>Terry M.</forenames></author></authors><title>A Continuous Max-Flow Approach to Cyclic Field Reconstruction</title><categories>cs.CV</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstruction of an image from noisy data using Markov Random Field theory
has been explored by both the graph-cuts and continuous max-flow community in
the form of the Potts and Ishikawa models. However, neither model takes into
account the particular cyclic topology of specific intensity types such as the
hue in natural colour images, or the phase in complex valued MRI. This paper
presents \textit{cyclic continuous max-flow} image reconstruction which models
the intensity being reconstructed as having a fundamentally cyclic topology.
This model complements the Ishikawa model in that it is designed with image
reconstruction in mind, having the topology of the intensity space inherent in
the model while being readily extendable to an arbitrary intensity resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03638</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03638</id><created>2015-11-11</created><authors><author><keyname>Pachoulakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Spanaki</keyname><forenames>Cleanthe</forenames></author></authors><title>Are Game Platforms suitable for Parkinson Disease patients?</title><categories>cs.CY</categories><comments>6 pages, 4 figures inProceedings of the 9th International Conference
  on New Horizons in Industry, Business and Education (NHIBE 2015), G.M.
  Papadourakis (ed), 27-29 August 2015, Skiathos Island, Greece. arXiv admin
  note: substantial text overlap with arXiv:1511.02589</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parkinson's Disease (PD) is a progressive neurodegenerative movement disorder
that affects more that 6 million people worldwide. Motor dysfunction gradually
increases as the disease progress. It is usually mild in the early stages of
the disease but it relentlessly progresses to a severe or very severe
disability that is characterized by increasing degrees of bradykinesia,
hypokinesia, muscle rigidity, loss of postural reflexes and balance control as
well as freezing of gait. In addition to a line of treatment based on
dopaminergic PD-specific drugs, attending neurologists strongly recommend
regular exercise combined with physiotherapy. However, the routine of
traditional rehabilitation often create boredom and loss of interest.
Opportunities to liven up a daily exercise schedule may well take the form of
character-based virtual reality games which engage the player to physically
train in a non-linear and looser fashion, providing an experience that varies
from one game loop the next. Such &quot;exergames&quot;, a word that results from the
amalgamation of the words &quot;exercise&quot; and &quot;game&quot; challenge patients into
performing movements of varying complexity in a playful and immersive virtual
environment. In fact, today's game consoles using controllers like Nintendo's
Wii, Sony PlayStation Eye and the Microsoft Kinect sensor present new
opportunities to infuse motivation and variety to an otherwise mundane
physiotherapy routine. But are these controllers and the games built for them
appropriate for PD patients? In this paper we present some of these approaches
and discuss their suitability for these patients mainly on the basis of demands
made on balance, agility and gesture precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03639</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03639</id><created>2015-11-11</created><updated>2015-11-13</updated><authors><author><keyname>Hofmann</keyname><forenames>Johannes</forenames></author><author><keyname>Fey</keyname><forenames>Dietmar</forenames></author><author><keyname>Eitzinger</keyname><forenames>Jan</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Analysis of Intel's Haswell Microarchitecture Using The ECM Model and
  Microbenchmarks</title><categories>cs.DC cs.AR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1509.03118</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an in-depth analysis of Intel's Haswell microarchitecture
for streaming loop kernels. Among the new features examined is the dual-ring
Uncore design, Cluster-on-Die mode, Uncore Frequency Scaling, core improvements
as new and improved execution units, as well as improvements throughout the
memory hierarchy. The Execution-Cache-Memory diagnostic performance model is
used together with a generic set of microbenchmarks to quantify the efficiency
of the microarchitecture. The set of microbenchmarks is chosen such that it can
serve as a blueprint for other streaming loop kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03640</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03640</id><created>2015-11-11</created><authors><author><keyname>Pachoulakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Pontikakis</keyname><forenames>Georgios</forenames></author></authors><title>Combining features of the Unreal and Unity Game Engines to hone
  development skills</title><categories>cs.CY</categories><comments>5 pages, 4 figures in Proceedings of the 9th International Conference
  on New Horizons in Industry, Business and Education (NHIBE 2015), G.M.
  Papadourakis (ed), 27-29 August 2015, Skiathos Island, Greece</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two of the most popular game engines today, Unreal Engine v4.x and Unity Game
Engine v5.x have recently adopted competitive and very appealing pricing
structures for individual game developers and small teams. One may lean towards
one or the other game engine based on various criteria: existing
familiarization / vested interest, steepness of learning curve, quality,
richness, variability and pricing of add-on assets, initial cost of basic
ownership versus subscribing to updates and acquiring needed modules, etc. In
this paper we present complementary features of the Unreal and Unity game
engines that can be combined to enhance understanding game development and to
hone relevant skills. Such combinations of different game engine features can
increase the added value for our undergraduate informatics engineering students
taking a &quot;Game Technologies&quot; class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03641</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03641</id><created>2015-11-11</created><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Kamath</keyname><forenames>Gautam</forenames></author><author><keyname>Tzamos</keyname><forenames>Christos</forenames></author></authors><title>A Size-Free CLT for Poisson Multinomials and its Applications</title><categories>cs.DS cs.GT cs.LG math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the
sum of $n$ independent random vectors supported on the set ${\cal
B}_k=\{e_1,\ldots,e_k\}$ of standard basis vectors in $\mathbb{R}^k$. We show
that any $(n,k)$-PMD is ${\rm poly}\left({k\over \sigma}\right)$-close in total
variation distance to the (appropriately discretized) multi-dimensional
Gaussian with the same first two moments, removing the dependence on $n$ from
the Central Limit Theorem of Valiant and Valiant. Interestingly, our CLT is
obtained by bootstrapping the Valiant-Valiant CLT itself through the structural
characterization of PMDs shown in recent work by Daskalakis, Kamath, and
Tzamos. In turn, our stronger CLT can be leveraged to obtain an efficient PTAS
for approximate Nash equilibria in anonymous games, significantly improving the
state of the art, and matching qualitatively the running time dependence on $n$
and $1/\varepsilon$ of the best known algorithm for two-strategy anonymous
games. Our new CLT also enables the construction of covers for the set of
$(n,k)$-PMDs, which are proper and whose size is shown to be essentially
optimal. Our cover construction combines our CLT with the Shapley-Folkman
theorem and recent sparsification results for Laplacian matrices by Batson,
Spielman, and Srivastava. Our cover size lower bound is based on an algebraic
geometric construction. Finally, leveraging the structural properties of the
Fourier spectrum of PMDs we show that these distributions can be learned from
$O_k(1/\varepsilon^2)$ samples in ${\rm poly}_k(1/\varepsilon)$-time, removing
the quasi-polynomial dependence of the running time on $1/\varepsilon$ from the
algorithm of Daskalakis, Kamath, and Tzamos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03643</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03643</id><created>2015-11-11</created><updated>2016-02-25</updated><authors><author><keyname>Lopez-Paz</keyname><forenames>David</forenames></author><author><keyname>Bottou</keyname><forenames>L&#xe9;on</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Vapnik</keyname><forenames>Vladimir</forenames></author></authors><title>Unifying distillation and privileged information</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distillation (Hinton et al., 2015) and privileged information (Vapnik &amp;
Izmailov, 2015) are two techniques that enable machines to learn from other
machines. This paper unifies these two techniques into generalized
distillation, a framework to learn from multiple machines and data
representations. We provide theoretical and causal insight about the inner
workings of generalized distillation, extend it to unsupervised, semisupervised
and multitask learning scenarios, and illustrate its efficacy on a variety of
numerical simulations on both synthetic and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03650</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03650</id><created>2015-11-11</created><updated>2015-12-07</updated><authors><author><keyname>Fu</keyname><forenames>Cheng-Yang</forenames></author><author><keyname>Berg</keyname><forenames>Alexander C.</forenames></author></authors><title>Piecewise Linear Activation Functions For More Efficient Deep Networks</title><categories>cs.CV</categories><comments>Withdrawn by arXiv admins</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This submission has been withdrawn by arXiv administrators because it is
intentionally incomplete, which is in violation of our policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03675</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03675</id><created>2015-11-11</created><authors><author><keyname>B&#xfc;rgisser</keyname><forenames>Peter</forenames></author><author><keyname>Christandl</keyname><forenames>Matthias</forenames></author><author><keyname>Mulmuley</keyname><forenames>Ketan D.</forenames></author><author><keyname>Walter</keyname><forenames>Michael</forenames></author></authors><title>Membership in moment polytopes is in NP and coNP</title><categories>cs.CC math-ph math.MP math.RT math.SG quant-ph</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the problem of deciding membership in the moment polytope
associated with a finite-dimensional unitary representation of a compact,
connected Lie group is in NP and coNP. This is the first non-trivial result on
the computational complexity of this problem, which naively amounts to a
quadratically-constrained program. Our result applies in particular to the
Kronecker polytopes, and therefore to the problem of deciding positivity of the
stretched Kronecker coefficients. In contrast, it has recently been shown that
deciding positivity of a single Kronecker coefficient is NP-hard in general
[Ikenmeyer, Mulmuley and Walter, arXiv:1507.02955]. We discuss the consequences
of our work in the context of complexity theory and the quantum marginal
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03677</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03677</id><created>2015-11-11</created><updated>2016-02-29</updated><authors><author><keyname>Lipton</keyname><forenames>Zachary C.</forenames></author><author><keyname>Kale</keyname><forenames>David C.</forenames></author><author><keyname>Elkan</keyname><forenames>Charles</forenames></author><author><keyname>Wetzell</keyname><forenames>Randall</forenames></author></authors><title>Learning to Diagnose with LSTM Recurrent Neural Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clinical medical data, especially in the intensive care unit (ICU), consist
of multivariate time series of observations. For each patient visit (or
episode), sensor data and lab test results are recorded in the patient's
Electronic Health Record (EHR). While potentially containing a wealth of
insights, the data is difficult to mine effectively, owing to varying length,
irregular sampling and missing data. Recurrent Neural Networks (RNNs),
particularly those using Long Short-Term Memory (LSTM) hidden units, are
powerful and increasingly popular models for learning from sequence data. They
effectively model varying length sequences and capture long range dependencies.
We present the first study to empirically evaluate the ability of LSTMs to
recognize patterns in multivariate time series of clinical measurements.
Specifically, we consider multilabel classification of diagnoses, training a
model to classify 128 diagnoses given 13 frequently but irregularly sampled
clinical measurements. First, we establish the effectiveness of a simple LSTM
network for modeling clinical data. Then we demonstrate a straightforward and
effective training strategy in which we replicate targets at each sequence
step. Trained only on raw time series, our models outperform several strong
baselines, including a multilayer perceptron trained on hand-engineered
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03683</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03683</id><created>2015-11-11</created><updated>2015-11-20</updated><authors><author><keyname>Lipton</keyname><forenames>Zachary C.</forenames></author><author><keyname>Vikram</keyname><forenames>Sharad</forenames></author><author><keyname>McAuley</keyname><forenames>Julian</forenames></author></authors><title>Capturing Meaning in Product Reviews with Character-Level Generative
  Text Models</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a character-level recurrent neural network that generates relevant
and coherent text given auxiliary information such as a sentiment or topic.
Using a simple input replication strategy, we preserve the signal of auxiliary
input across wider sequence intervals than can feasibly be trained by
backpropagation through time. Our main results center on a large corpus of 1.5
million beer reviews from BeerAdvocate. In generative mode, our network
produces reviews on command, tailored to a star rating or item category. The
generative model can also run in reverse, performing classification with
surprising accuracy. Performance of the reverse model provides a
straightforward way to determine what the generative model knows without
relying too heavily on subjective analysis. Given a review, the model can
accurately determine the corresponding rating and infer the beer's category
(IPA, Stout, etc.). We exploit this capability, tracking perceived sentiment
and class membership as each character in a review is processed. Quantitative
and qualitative empirical evaluations demonstrate that the model captures
meaning and learns nonlinear dynamics in text, such as the effect of negation
on sentiment, despite possessing no a priori notion of words. Because the model
operates at the character level, it handles misspellings, slang, and large
vocabularies without any machinery explicitly dedicated to the purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03686</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03686</id><created>2015-11-11</created><authors><author><keyname>Diaz</keyname><forenames>Mario</forenames></author><author><keyname>P&#xe9;rez-Abreu</keyname><forenames>V&#xed;ctor</forenames></author></authors><title>On the Capacity of Block Multiantenna Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider point-to-point multiantenna channels with certain
block distributional symmetries which do not require the entries of the channel
matrix to be either Gaussian, or independent, or identically distributed. A
main contribution is a capacity theorem for these channels, which might be
regarded as a generalization of Telatar's theorem (1999), which reduces the
numerical optimization domain in the capacity computation. With this
information theoretic result and some free probability arguments, we prove an
asymptotic capacity theorem that, in addition to reducing the optimization
domain, does not depend on the dimension of the channel matrix. This theorem
allows us to apply free probability techniques to numerically compute the
asymptotic capacity of the channels under consideration. These theorems provide
a very efficient method for numerically approximating both the capacity and a
capacity achieving input covariance matrix of certain channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03688</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03688</id><created>2015-11-11</created><authors><author><keyname>Cardot</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Degras</keyname><forenames>David</forenames></author></authors><title>Online Principal Component Analysis in High Dimension: Which Algorithm
  to Choose?</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current context of data explosion, online techniques that do not
require storing all data in memory are indispensable to routinely perform tasks
like principal component analysis (PCA). Recursive algorithms that update the
PCA with each new observation have been studied in various fields of research
and found wide applications in industrial monitoring, computer vision,
astronomy, and latent semantic indexing, among others. This work provides
guidance for selecting an online PCA algorithm in practice. We present the main
approaches to online PCA, namely, perturbation techniques, incremental methods,
and stochastic optimization, and compare their statistical accuracy,
computation time, and memory requirements using artificial and real data.
Extensions to missing data and to functional data are discussed. All studied
algorithms are available in the R package onlinePCA on CRAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03690</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03690</id><created>2015-11-11</created><authors><author><keyname>Harwath</keyname><forenames>David</forenames></author><author><keyname>Glass</keyname><forenames>James</forenames></author></authors><title>Deep Multimodal Semantic Embeddings for Speech and Images</title><categories>cs.CV cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a model which takes as input a corpus of images
with relevant spoken captions and finds a correspondence between the two
modalities. We employ a pair of convolutional neural networks to model visual
objects and speech signals at the word level, and tie the networks together
with an embedding and alignment model which learns a joint semantic space over
both modalities. We evaluate our model using image search and annotation tasks
on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000
spoken captions using Amazon Mechanical Turk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03693</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03693</id><created>2015-11-11</created><authors><author><keyname>Nobrega</keyname><forenames>Hugo</forenames></author><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>Game characterizations and lower cones in the Weihrauch degrees</title><categories>math.LO cs.LO</categories><msc-class>03E15, 54H05, 03D60, 03F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce generalized Wadge games and show that each lower cone in the
Weihrauch degrees is characterized by such a game. These generalized Wadge
games subsume the original Wadge games, the eraser and backtrack games as well
as variants of Semmes' tree games. As a new example we introduce the tree
derivative games which characterize all even finite levels of the Baire
hierarchy, and a variant characterizing the odd finite levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03698</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03698</id><created>2015-11-11</created><authors><author><keyname>Mahmoodi</keyname><forenames>S. Eman</forenames></author><author><keyname>Subbalakshmi</keyname><forenames>K. P.</forenames></author><author><keyname>Sagar</keyname><forenames>Vidya</forenames></author></authors><title>Cloud Offloading for Multi-Radio Enabled Mobile Devices</title><categories>cs.NI</categories><comments>2015 IEEE International Conference on Communications (ICC2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of 5G networking technologies has increased the expectations from
mobile devices, in that, more sophisticated, computationally intense
applications are expected to be delivered on the mobile device which are
themselves getting smaller and sleeker. This predicates a need for offloading
computationally intense parts of the applications to a resource strong cloud.
Parallely, in the wireless networking world, the trend has shifted to
multi-radio (as opposed to multi-channel) enabled communications. In this
paper, we provide a comprehensive computation offloading solution that uses the
multiple radio links available for associated data transfer, optimally. Our
contributions include: a comprehensive model for the energy consumption from
the perspective of the mobile device; the formulation of the joint optimization
problem to minimize the energy consumed as well as allocating the associated
data transfer optimally through the available radio links and an iterative
algorithm that converges to a locally optimal solution. Simulations on an HTC
phone, running a 14-component application and using the Amazon EC2 as the
cloud, show that the solution obtained through the iterative algorithm consumes
only 3% more energy than the optimal solution (obtained via exhaustive search).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03699</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03699</id><created>2015-11-11</created><authors><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Sivan</keyname><forenames>Balasubramanian</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Multi-parameter Auctions with Online Supply</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a basic auction design problem with online supply. There are two
unit-demand bidders and two types of items. The first item type will arrive
first for sure, and the second item type may or may not arrive. The auctioneer
has to decide the allocation of an item immediately after each item arrives,
but is allowed to compute payments after knowing how many items arrived. For
this problem we show that there is no deterministic truthful and individually
rational mechanism that, even with unbounded computational resources, gets any
finite approximation factor to the optimal social welfare.
  The basic multi-parameter online supply model that we study opens the space
for several interesting questions about the power of randomness. There are
three prominent sources of randomness: 1) Randomized mechanisms, 2) Stochastic
arrival. 3) Bayesian setting (values drawn from a distribution). For each of
these sources of randomness, without using the other sources of randomness,
what is the optimal approximation factor one can get? The currently known best
approximation factors to optimal social welfare for none of these settings is
better than $\max\{n,m\}$ where $n$ is the number of bidders and $m$ is the
number of items.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03703</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03703</id><created>2015-11-11</created><authors><author><keyname>Phipps</keyname><forenames>E.</forenames></author><author><keyname>D'Elia</keyname><forenames>M.</forenames></author><author><keyname>Edwards</keyname><forenames>H. C.</forenames></author><author><keyname>Hoemmen</keyname><forenames>M.</forenames></author><author><keyname>Hu</keyname><forenames>J.</forenames></author><author><keyname>Rajamanickam</keyname><forenames>S.</forenames></author></authors><title>Embedded Ensemble Propagation for Improving Performance, Portability and
  Scalability of Uncertainty Quantification on Emerging Computational
  Architectures</title><categories>cs.MS cs.CE</categories><report-no>SAND2015-9921 J</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantifying simulation uncertainties is a critical component of rigorous
predictive simulation. A key component of this is forward propagation of
uncertainties in simulation input data to output quantities of interest.
Typical approaches involve repeated sampling of the simulation over the
uncertain input data, and can require numerous samples when accurately
propagating uncertainties from large numbers of sources. Often simulation
processes from sample to sample are similar and much of the data generated from
each sample evaluation could be reused. We explore a new method for
implementing sampling methods that simultaneously propagates groups of samples
together in an embedded fashion, which we call embedded ensemble propagation.
We show how this approach takes advantage of properties of modern computer
architectures to improve performance by enabling reuse between samples,
reducing memory bandwidth requirements, improving memory access patterns,
improving opportunities for fine-grained parallelization, and reducing
communication costs. We describe a software technique for implementing embedded
ensemble propagation based on the use of C++ templates and describe its
integration with various scientific computing libraries within Trilinos. We
demonstrate improved performance, portability and scalability for the approach
applied to the simulation of partial differential equations on a variety of
CPU, GPU, and accelerator architectures, including up to 131,072 cores on a
Cray XK7 (Titan).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03705</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03705</id><created>2015-11-11</created><authors><author><keyname>Xing</keyname><forenames>Hong</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Wireless Powered Cooperative Jamming for Secrecy Multi-AF Relaying
  Networks</title><categories>cs.IT math.IT</categories><comments>29 pages (single column), 9 figures, submitted for possible journal
  publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies secrecy transmission with the aid of a group of wireless
energy harvesting (WEH)-enabled amplify-and-forward (AF) relays performing
cooperative jamming (CJ) and relaying. The source node in the network does
simultaneous wireless information and power transfer (SWIPT) with each relay
employing a power splitting (PS) receiver in the first phase; each relay
further divides its harvested power for forwarding the received signal and
generating artificial noise (AN) for jamming the eavesdroppers in the second
transmission phase. In the centralized case with global channel state
information (CSI), we provide closed-form expressions for the optimal and/or
suboptimal AF-relay beamforming vectors to maximize the achievable secrecy rate
subject to individual power constraints of the relays, using the technique of
semidefinite relaxation (SDR), which is proved to be tight. A fully distributed
algorithm utilizing only local CSI at each relay is also proposed as a
performance benchmark. Simulation results validate the effectiveness of the
proposed multi-AF relaying with CJ over other suboptimal designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03719</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03719</id><created>2015-11-11</created><updated>2016-02-15</updated><authors><author><keyname>Zhang</keyname><forenames>Xiang</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Universum Prescription: Regularization using Unlabeled Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that simply prescribing &quot;none of the above&quot; labels to
unlabeled data has a beneficial regularization effect to supervised learning.
We call it universum prescription by the fact that the prescribed labels cannot
be one of the supervised labels. In spite of its simplicity, universum
prescription obtained competitive results in training deep convolutional
networks for CIFAR-10, CIFAR-100 and STL-10 datasets. A qualitative
justification of these approaches using Rademacher complexity is presented. The
effect of a regularization parameter -- probability of sampling from unlabeled
data -- is also studied empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03722</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03722</id><created>2015-11-11</created><updated>2016-02-16</updated><authors><author><keyname>Jiang</keyname><forenames>Nan</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author></authors><title>Doubly Robust Off-policy Value Evaluation for Reinforcement Learning</title><categories>cs.LG cs.AI cs.SY stat.ME stat.ML</categories><comments>14 pages; 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of off-policy value evaluation in reinforcement learning
(RL), where one aims to estimate the value of a new policy based on data
collected by a different policy. This problem is often a critical step when
applying RL in real-world problems. Despite its importance, existing general
methods either have uncontrolled bias or suffer high variance. In this work, we
extend the doubly robust estimator for bandits to sequential decision-making
problems, which gets the best of both worlds: it is guaranteed to be unbiased
and can have a much lower variance than the popular importance sampling
estimators. We demonstrate the estimator's accuracy in several benchmark
problems, and illustrate its use as a subroutine in safe policy improvement. We
also provide theoretical results on the hardness of the problem, and show that
our estimator can match the lower bound in certain scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03729</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03729</id><created>2015-11-11</created><updated>2015-12-25</updated><authors><author><keyname>Wang</keyname><forenames>Tian</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author></authors><title>Larger-Context Language Modelling</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel method to incorporate corpus-level discourse
information into language modelling. We call this larger-context language
model. We introduce a late fusion approach to a recurrent language model based
on long short-term memory units (LSTM), which helps the LSTM unit keep
intra-sentence dependencies and inter-sentence dependencies separate from each
other. Through the evaluation on three corpora (IMDB, BBC, and PennTree Bank),
we demon- strate that the proposed model improves perplexity significantly. In
the experi- ments, we evaluate the proposed approach while varying the number
of context sentences and observe that the proposed late fusion is superior to
the usual way of incorporating additional inputs to the LSTM. By analyzing the
trained larger- context language model, we discover that content words,
including nouns, adjec- tives and verbs, benefit most from an increasing number
of context sentences. This analysis suggests that larger-context language model
improves the unconditional language model by capturing the theme of a document
better and more easily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03730</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03730</id><created>2015-11-11</created><authors><author><keyname>Garg</keyname><forenames>Ankit</forenames></author><author><keyname>Gurvits</keyname><forenames>Leonid</forenames></author><author><keyname>Oliveira</keyname><forenames>Rafael</forenames></author><author><keyname>Wigderson</keyname><forenames>Avi</forenames></author></authors><title>A deterministic polynomial time algorithm for non-commutative rational
  identity testing</title><categories>cs.CC math.AC math.AG quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a deterministic polynomial time algorithm for
testing if a symbolic matrix in {\emph non-commuting} variables over
$\mathbb{Q}$ is invertible or not. The analogous question for commuting
variables is the celebrated polynomial identity testing (PIT) for symbolic
determinants. In contrast to the commutative case, which has an efficient
probabilistic algorithm, the best previous algorithm for the non-commutative
setting required exponential time [IQS15] (whether or not randomization is
allowed). The algorithm efficiently solves the &quot;word problem&quot; for the free skew
field, and the identity testing problem for arithmetic formulae with division
over non-commuting variables, two problems which had only exponential-time
algorithms prior to this work.
  The main (and simple) technical contribution of this paper is an analysis of
an existing algorithm due to Gurvits which solved the same problem in some
special cases. We also extend the algorithm to compute the (non-commutative)
rank of a symbolic matrix, yielding a factor 2 approximation on the commutative
rank.
  Symbolic matrices in non-commuting variables, and the related structural and
algorithmic questions, have a remarkable number of diverse origins and
motivations. They arise independently in (commutative) invariant theory and
representation theory, linear algebra, optimization, linear system theory,
quantum information theory, approximation of the permanent and naturally in
non-commutative algebra. We provide a detailed account of some of these sources
and their interconnections. In particular we explain how some of these sources
played an important role in the development of Gurvits' algorithm and in our
analysis of it here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03738</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03738</id><created>2015-11-11</created><authors><author><keyname>Burstein</keyname><forenames>David</forenames></author><author><keyname>Rubin</keyname><forenames>Jonathan</forenames></author></authors><title>Degree switching and partitioning for enumerating graphs to arbitrary
  orders of accuracy</title><categories>math.CO cs.DM math.ST stat.TH</categories><comments>24 pages, 1 figure</comments><msc-class>05C30, 05A16, 62Q05, 05C07, 05C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a novel method for constructing asymptotics (to arbitrary
accuracy) for the number of directed graphs that realize a fixed bidegree
sequence $d = a \times b$ with maximum degree $d_{max}=O(S^{\frac{1}{2}-\tau})$
for an arbitrarily small positive number $\tau$, where $S$ is the number edges
specified by $d$. Our approach is based on two key steps, graph partitioning
and degree preserving switches. The former idea allows us to relate enumeration
results for given sequences to those for sequences that are especially easy to
handle, while the latter facilitates expansions based on numbers of shared
neighbors of pairs of nodes. While we focus primarily on directed graphs
allowing loops, our results can be extended to other cases, including bipartite
graphs, as well as directed and undirected graphs without loops. In addition,
we can relax the constraint that $d_{max} = O(S^{\frac{1}{2}-\tau})$ and
replace it with $a_{max} b_{max} = O(S^{1-\tau})$. where $a_{max}$ and
$b_{max}$ are the maximum values for $a$ and $b$ respectively. The previous
best results, from Greenhill et al., only allow for $d_{max} =
o(S^{\frac{1}{3}})$ or alternatively $a_{max} b_{max} = o(S^{\frac{2}{3}})$.
Since in many real world networks, $d_{max}$ scales larger than
$o(S^{\frac{1}{3}})$, we expect that this work will be helpful for various
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03742</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03742</id><created>2015-11-11</created><updated>2015-11-17</updated><authors><author><keyname>Lokhmotov</keyname><forenames>Anton</forenames></author></authors><title>GEMMbench: a framework for reproducible and collaborative benchmarking
  of matrix multiplication</title><categories>cs.MS cs.PF</categories><comments>ADAPT'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generic matrix-matrix multiplication (GEMM) is arguably the most popular
computational kernel of the 20th century. Yet, surprisingly, no common
methodology for evaluating GEMM performance has been established over the many
decades of using GEMM for comparing architectures, compilers and ninja-class
programmers.
  We introduce GEMMbench, a framework and methodology for evaluating
performance of GEMM implementations. GEMMbench is implemented on top of
Collective Knowledge (CK), a lightweight framework for reproducible and
collaborative R&amp;D in computer systems. Using CK allows the R&amp;D community to
crowdsource hand-written and compiler-generated GEMM implementations and to
study their performance across multiple platforms, data sizes and data types.
  Our initial implementation supports hand-written OpenCL kernels operating on
matrices consisting of single- and double-precision floating-point values, and
producing single or multiple output elements per work-item (via thread
coarsening and vectorization).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03745</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03745</id><created>2015-11-11</created><authors><author><keyname>Rohrbach</keyname><forenames>Anna</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Hu</keyname><forenames>Ronghang</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Grounding of Textual Phrases in Images by Reconstruction</title><categories>cs.CV cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual
content is a challenging problem with many applications for human-computer
interaction and image-text reference resolution. Although many data sources
contain images which are described with sentences or phrases, they typically do
not provide the spatial localization of the phrases. This is true for both
curated datasets such as MSCOCO or large user generated content as e.g. in the
YFCC 100M dataset. Consequently, being able to learn from this data without
grounding supervision would allow large amount and variety of training data.
For this setting we propose GroundeR, a novel approach, which is able to learn
the grounding by aiming to reconstruct a given phrase using an attention
mechanism. More specifically, during training time, the model encodes the
phrase using an LSTM, and then has to learn to attend to the relevant image
region in order to reconstruct the input phrase. At test time the correct
attention, i.e. the grounding is evaluated. On the Flickr 30k Entities dataset
our approach outperforms prior work which, in contrast to us, trains with the
grounding (bounding box) annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03748</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03748</id><created>2015-11-11</created><authors><author><keyname>Lee</keyname><forenames>Joon-Young</forenames></author><author><keyname>Sunkavalli</keyname><forenames>Kalyan</forenames></author><author><keyname>Lin</keyname><forenames>Zhe</forenames></author><author><keyname>Shen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Kweon</keyname><forenames>In So</forenames></author></authors><title>Automatic Content-Aware Color and Tone Stylization</title><categories>cs.CV</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new technique that automatically generates diverse, visually
compelling stylizations for a photograph in an unsupervised manner. We achieve
this by learning style ranking for a given input using a large photo collection
and selecting a diverse subset of matching styles for final style transfer. We
also propose a novel technique that transfers the global color and tone of the
chosen exemplars to the input photograph while avoiding the common visual
artifacts produced by the existing style transfer methods. Together, our style
selection and transfer techniques produce compelling, artifact-free results on
a wide range of input photographs, and a user study shows that our results are
preferred over other techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03749</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03749</id><created>2015-11-11</created><authors><author><keyname>Martinez</keyname><forenames>Monica</forenames></author><author><keyname>Rohrer</keyname><forenames>Edelweis</forenames></author><author><keyname>Severi</keyname><forenames>Paula</forenames></author></authors><title>Complexity of the Description Logic ALCM</title><categories>cs.LO cs.AI</categories><comments>Long version of a submitted paper, 43 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that the problem of checking consistency of a knowledge
base in the Description Logic ALCM is ExpTime-complete. The M stands for
meta-modelling as defined by Motz, Rohrer and Severi. To show our main result,
we define an ExpTime Tableau algorithm as an extension of an algorithm for
checking consistency of a knowledge base in ALC by Nguyen and Szalas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03753</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03753</id><created>2015-11-11</created><updated>2016-02-03</updated><authors><author><keyname>Reisenhofer</keyname><forenames>Rafael</forenames></author><author><keyname>Kiefer</keyname><forenames>Johannes</forenames></author><author><keyname>King</keyname><forenames>Emily J.</forenames></author></authors><title>Shearlet-Based Detection of Flame Fronts</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying and characterizing flame fronts is the most common task in the
computer-assisted analysis of data obtained from imaging techniques such as
planar laser-induced fluorescence (PLIF), laser Rayleigh scattering (LRS), or
particle imaging velocimetry (PIV). We present a novel edge and ridge (line)
detection algorithm based on complex-valued wavelet-like analyzing functions --
so-called complex shearlets -- displaying several traits useful for the
extraction of flame fronts. In addition to providing a unified approach to the
detection of edges and ridges, our method inherently yields estimates of local
tangent orientations and local curvatures. To examine the applicability for
high-frequency recordings of combustion processes, the algorithm is applied to
mock images distorted with varying degrees of noise and real-world PLIF images
of both OH and CH radicals. Furthermore, we compare the performance of the
newly proposed complex shearlet-based measure to well-established edge and
ridge detection techniques such as the Canny edge detector, another
shearlet-based edge detector, and the phase congruency measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03759</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03759</id><created>2015-11-11</created><authors><author><keyname>Shi</keyname><forenames>Chuan</forenames></author><author><keyname>Liu</keyname><forenames>Jian</forenames></author><author><keyname>Zhuang</keyname><forenames>Fuzhen</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Wu</keyname><forenames>Bin</forenames></author></authors><title>Integrating Heterogeneous Information via Flexible Regularization
  Framework for Recommendation</title><categories>cs.SI cs.IR</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there is a surge of social recommendation, which leverages social
relations among users to improve recommendation performance. However, in many
applications, social relations are absent or very sparse. Meanwhile, the
attribute information of users or items may be rich. It is a big challenge to
exploit these attribute information for the improvement of recommendation
performance. In this paper, we organize objects and relations in recommendation
system as a heterogeneous information network, and introduce meta path based
similarity measure to evaluate the similarity of users or items. Furthermore, a
matrix factorization based dual regularization framework SimMF is proposed to
flexibly integrate different types of information through adopting the
similarity of users and items as regularization on latent factors of users and
items. Extensive experiments not only validate the effectiveness of SimMF but
also reveal some interesting findings. We find that attribute information of
users and items can significantly improve recommendation accuracy, and their
contribution seems more important than that of social relations. The
experiments also reveal that different regularization models have obviously
different impact on users and items.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03760</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03760</id><created>2015-11-11</created><authors><author><keyname>Wang</keyname><forenames>Mengdi</forenames></author><author><keyname>Chen</keyname><forenames>Yichen</forenames></author><author><keyname>Liu</keyname><forenames>Jialin</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Random Multi-Constraint Projection: Stochastic Gradient Methods for
  Convex Optimization with Many Constraints</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider convex optimization problems subject to a large number of
constraints. We focus on stochastic problems in which the objective takes the
form of expected values and the feasible set is the intersection of a large
number of convex sets. We propose a class of algorithms that perform both
stochastic gradient descent and random feasibility updates simultaneously. At
every iteration, the algorithms sample a number of projection points onto a
randomly selected small subsets of all constraints. Three feasibility update
schemes are considered: averaging over random projected points, projecting onto
the most distant sample, projecting onto a special polyhedral set constructed
based on sample points. We prove the almost sure convergence of these
algorithms, and analyze the iterates' feasibility error and optimality error,
respectively. We provide new convergence rate benchmarks for stochastic
first-order optimization with many constraints. The rate analysis and numerical
experiments reveal that the algorithm using the polyhedral-set projection
scheme is the most efficient one within known algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03763</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03763</id><created>2015-11-11</created><authors><author><keyname>Gu</keyname><forenames>Xiaoyi</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Tu</keyname><forenames>Shenyinying</forenames></author></authors><title>A note on practical approximate projection schemes in signal space
  methods</title><categories>math.NA cs.IT math.IT</categories><msc-class>41A46, 68Q25, 68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing (CS) is a new technology which allows the acquisition of
signals directly in compressed form, using far fewer measurements than
traditional theory dictates. Recently, many so-called signal space methods have
been developed to extend this body of work to signals sparse in arbitrary
dictionaries rather than orthonormal bases. In doing so, CS can be utilized in
a much broader array of practical settings. Often, such approaches often rely
on the ability to optimally project a signal onto a small number of dictionary
atoms. Such optimal, or even approximate, projections have been difficult to
derive theoretically. Nonetheless, it has been observed experimentally that
conventional CS approaches can be used for such projections, and still provide
accurate signal recovery. In this letter, we summarize the empirical evidence
and clearly demonstrate for what signal types certain CS methods may be used as
approximate projections. In addition, we provide theoretical guarantees for
such methods for certain sparse signal structures. Our theoretical results
match those observed in experimental studies, and we thus establish both
experimentally and theoretically that these CS methods can be used in this
context. \end{abstract}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03765</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03765</id><created>2015-11-11</created><authors><author><keyname>Ren</keyname><forenames>Hong</forenames></author><author><keyname>Liu</keyname><forenames>Nan</forenames></author><author><keyname>Pan</keyname><forenames>Cunhua</forenames></author><author><keyname>He</keyname><forenames>Chunlong</forenames></author></authors><title>Energy Efficiency Optimization for MIMO Distributed Antenna Systems</title><categories>cs.IT math.IT</categories><comments>27 pages, 10 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we propose a transmit covariance optimization method to
maximize the energy efficiency (EE) for a single-user distributed antenna
system, where both the remote access units (RAUs) and the user are equipped
with multiple antennas. Unlike previous related works, both the rate
requirement and RAU selection are taken into consideration. Here, the total
circuit power consumption is related to the number of active RAUs. Given this
setup, we first propose an optimal transmit covariance optimization method to
solve the EE optimization problem under a fixed set of active RAUs. More
specifically, we split this problem into three subproblems, i.e., the rate
maximization problem, the EE maximization problem without rate constraint, and
the power minimization problem, and each subproblem can be efficiently solved.
Then, a novel distance-based RAU selection method is proposed to determine the
optimal set of active RAUs. Simulation results show that the performance of the
proposed RAU selection is almost identical to the optimal exhaustive search
method with significantly reduced computational complexity, and the performance
of the proposed algorithm significantly outperforms the existing EE
optimization methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03766</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03766</id><created>2015-11-11</created><authors><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Sparse Learning for Large-scale and High-dimensional Data: A Randomized
  Convex-concave Optimization Approach</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a randomized algorithm and theory for learning a
sparse model from large-scale and high-dimensional data, which is usually
formulated as an empirical risk minimization problem with a sparsity-inducing
regularizer. Under the assumption that there exists a (approximately) sparse
solution with high classification accuracy, we argue that the dual solution is
also sparse or approximately sparse. The fact that both primal and dual
solutions are sparse motivates us to develop a randomized approach for a
general convex-concave optimization problem. Specifically, the proposed
approach combines the strength of random projection with that of sparse
learning: it utilizes random projection to reduce the dimensionality, and
introduces $\ell_1$-norm regularization to alleviate the approximation error
caused by random projection. Theoretical analysis shows that under favored
conditions, the randomized algorithm can accurately recover the optimal
solutions to the convex-concave optimization problem (i.e., recover both the
primal and dual solutions). Furthermore, the solutions returned by our
algorithm are guaranteed to be approximately sparse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03771</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03771</id><created>2015-11-11</created><updated>2016-01-10</updated><authors><author><keyname>Talathi</keyname><forenames>Sachin S.</forenames></author><author><keyname>Vartak</keyname><forenames>Aniket</forenames></author></authors><title>Improving performance of recurrent neural network with relu nonlinearity</title><categories>cs.NE cs.LG</categories><comments>10 pages 6 figures; under consideration for publication with ICLR
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years significant progress has been made in successfully training
recurrent neural networks (RNNs) on sequence learning problems involving long
range temporal dependencies. The progress has been made on three fronts: (a)
Algorithmic improvements involving sophisticated optimization techniques, (b)
network design involving complex hidden layer nodes and specialized recurrent
layer connections and (c) weight initialization methods. In this paper, we
focus on recently proposed weight initialization with identity matrix for the
recurrent weights in a RNN. This initialization is specifically proposed for
hidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple
dynamical systems perspective on weight initialization process, which allows us
to propose a modified weight initialization strategy. We show that this
initialization technique leads to successfully training RNNs composed of ReLUs.
We demonstrate that our proposal produces comparable or better solution for
three toy problems involving long range temporal structure: the addition
problem, the multiplication problem and the MNIST classification problem using
sequence of pixels. In addition, we present results for a benchmark action
recognition problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03774</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03774</id><created>2015-11-11</created><updated>2015-11-13</updated><authors><author><keyname>Chen</keyname><forenames>Lijie</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author></authors><title>On the Optimal Sample Complexity for Best Arm Identification</title><categories>cs.LG cs.DS</categories><comments>39 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the best arm identification (BEST-1-ARM) problem, which is defined
as follows. We are given $n$ stochastic bandit arms. The $i$th arm has a reward
distribution $D_i$ with an unknown mean $\mu_i$. Upon each play of the $i$th
arm, we can get a reward, sampled i.i.d. from $D_i$. We would like to identify
the arm with largest mean with probability at least $1-\delta$, using as few
samples as possible. We also study an important special case where there are
only two arms, which we call the sign problem. We achieve a very detailed
understanding of the optimal sample complexity of sign, simplifying and
significantly extending a classical result by Farrell in 1964, with a
completely new proof. Using the new lower bound for sign, we obtain the first
lower bound for BEST-1-ARM that goes beyond the classic Mannor-Tsitsiklis lower
bound, by an interesting reduction from sign to BEST-1-ARM.
  To complement our lower bound, we also provide a nontrivial algorithm for
BEST-1-ARM, which achieves a worst case optimal sample complexity, improving
upon several prior upper bounds on the same problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03776</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03776</id><created>2015-11-12</created><authors><author><keyname>Sun</keyname><forenames>Chen</forenames></author><author><keyname>Paluri</keyname><forenames>Manohar</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author><author><keyname>Nevatia</keyname><forenames>Ram</forenames></author><author><keyname>Bourdev</keyname><forenames>Lubomir</forenames></author></authors><title>ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural
  Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to classify and locate objects accurately and efficiently,
without using bounding box annotations. It is challenging as objects in the
wild could appear at arbitrary locations and in different scales. A cascaded
classifier, though able to balance accuracy and speed, typically requires the
inputs of all stages to be the same, thus difficult to handle smaller objects.
In this paper, we propose a novel classification architecture ProNet based on
convolutional neural networks. It learns to propose promising image boxes that
might contain objects, and zooms onto those boxes. The basic building block is
a multi-scale fully-convolutional network which assigns object confidence
scores to boxes at different locations and scales. We show that such networks
can be trained effectively using image-level annotations, and can be connected
into cascades or trees for efficient object classification. ProNet outperforms
previous state-of-the-art significantly on PASCAL VOC 2012 and MS COCO datasets
for object classification and localization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03780</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03780</id><created>2015-11-12</created><authors><author><keyname>Zheng</keyname><forenames>Yong</forenames></author></authors><title>A User's Guide to CARSKit</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-aware recommender systems extend traditional recommenders by adapting
their suggestions to users' contextual situations. CARSKit is a Java-based
open-source library specifically designed for the context-aware recommendation,
where the state-of-the-art context-aware recommendation algorithms have been
implemented. This report provides the basic user's guide to CARSKit, including
how to prepare the data set, how to configure the experimental settings, and
how to evaluate the algorithms, as well as interpreting the outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03791</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03791</id><created>2015-11-12</created><updated>2015-11-13</updated><authors><author><keyname>Zhang</keyname><forenames>Fangyi</forenames></author><author><keyname>Leitner</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Milford</keyname><forenames>Michael</forenames></author><author><keyname>Upcroft</keyname><forenames>Ben</forenames></author><author><keyname>Corke</keyname><forenames>Peter</forenames></author></authors><title>Towards Vision-Based Deep Reinforcement Learning for Robotic Motion
  Control</title><categories>cs.LG cs.CV cs.RO</categories><comments>8 pages, to appear in the proceedings of Australasian Conference on
  Robotics and Automation (ACRA) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a machine learning based system for controlling a
robotic manipulator with visual perception only. The capability to autonomously
learn robot controllers solely from raw-pixel images and without any prior
knowledge of configuration is shown for the first time. We build upon the
success of recent deep reinforcement learning and develop a system for learning
target reaching with a three-joint robot manipulator using external visual
observation. A Deep Q Network (DQN) was demonstrated to perform target reaching
after training in simulation. Transferring the network to real hardware and
real observation in a naive approach failed, but experiments show that the
network works when replacing camera images with synthetic images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03796</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03796</id><created>2015-11-12</created><authors><author><keyname>Zhu</keyname><forenames>Yuancheng</forenames></author><author><keyname>Liu</keyname><forenames>Zhe</forenames></author><author><keyname>Sun</keyname><forenames>Siqi</forenames></author></authors><title>Nonparametric Estimation of Scale-Free Graphical Models</title><categories>stat.ME cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a nonparametric method for estimating scale-free graphical models.
To avoid the usual Gaussian assumption, we restrict the graph to be a forest
and build on the work of forest density estimation. The method is motivated
from a Bayesian perspective and is equivalent to finding the maximum spanning
tree of a weighted graph with a log degree penalty. We solve the optimization
problem via a minorize-maximization procedure with Kruskal's algorithm.
Simulations show that the proposed method outperforms competing parametric
methods, and is robust to the true data distribution. It also leads to
improvement in predictive power and interpretability in two real data examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03803</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03803</id><created>2015-11-12</created><authors><author><keyname>Dwork</keyname><forenames>Cynthia</forenames></author><author><keyname>Su</keyname><forenames>Weijie</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author></authors><title>Private False Discovery Rate Control</title><categories>math.ST cs.DS stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the first differentially private algorithms for controlling the
false discovery rate (FDR) in multiple hypothesis testing, with essentially no
loss in power under certain conditions. Our general approach is to adapt a
well-known variant of the Benjamini-Hochberg procedure (BHq), making each step
differentially private. This destroys the classical proof of FDR control. To
prove FDR control of our method, (a) we develop a new proof of the original
(non-private) BHq algorithm and its robust variants -- a proof requiring only
the assumption that the true null test statistics are independent, allowing for
arbitrary correlations between the true nulls and false nulls. This assumption
is fairly weak compared to those previously shown in the vast literature on
this topic, and explains in part the empirical robustness of BHq. Then (b) we
relate the FDR control properties of the differentially private version to the
control properties of the non-private version. \end{enumerate} We also present
a low-distortion &quot;one-shot&quot; differentially private primitive for &quot;top $k$&quot;
problems, e.g., &quot;Which are the $k$ most popular hobbies?&quot; (which we apply to:
&quot;Which hypotheses have the $k$ most significant $p$-values?&quot;), and use it to
get a faster privacy-preserving instantiation of our general approach at little
cost in accuracy. The proof of privacy for the one-shot top~$k$ algorithm
introduces a new technique of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03812</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03812</id><created>2015-11-12</created><authors><author><keyname>You</keyname><forenames>Li</forenames></author><author><keyname>Gao</keyname><forenames>Xiqi</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author><author><keyname>Zhong</keyname><forenames>Wen</forenames></author></authors><title>Channel Acquisition for Massive MIMO-OFDM with Adjustable Phase Shift
  Pilots</title><categories>cs.IT math.IT</categories><comments>15 pages, 4 figures, accepted for publication in the IEEE
  Transactions on Signal Processing</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 64, no. 6, pp.
  1461--1476, Mar. 2016</journal-ref><doi>10.1109/TSP.2015.2502550</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose adjustable phase shift pilots (APSPs) for channel acquisition in
wideband massive multiple-input multiple-output (MIMO) systems employing
orthogonal frequency division multiplexing (OFDM) to reduce the pilot overhead.
Based on a physically motivated channel model, we first establish a
relationship between channel space-frequency correlations and the channel power
angle-delay spectrum in the massive antenna array regime, which reveals the
channel sparsity in massive MIMO-OFDM. With this channel model, we then
investigate channel acquisition, including channel estimation and channel
prediction, for massive MIMO-OFDM with APSPs. We show that channel acquisition
performance in terms of sum mean square error can be minimized if the user
terminals' channel power distributions in the angle-delay domain can be made
non-overlapping with proper phase shift scheduling. A simplified pilot phase
shift scheduling algorithm is developed based on this optimal channel
acquisition condition. The performance of APSPs is investigated for both one
symbol and multiple symbol data models. Simulations demonstrate that the
proposed APSP approach can provide substantial performance gains in terms of
achievable spectral efficiency over the conventional phase shift orthogonal
pilot approach in typical mobility scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03814</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03814</id><created>2015-11-12</created><updated>2016-02-24</updated><authors><author><keyname>Rosenfeld</keyname><forenames>Amir</forenames></author><author><keyname>Ullman</keyname><forenames>Shimon</forenames></author></authors><title>Hand-Object Interaction and Precise Localization in Transitive Action
  Recognition</title><categories>cs.CV</categories><comments>Minor changes: title and abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Action recognition in still images has seen major improvement in recent years
due to advances in human pose estimation, object recognition and stronger
feature representations produced by deep neural networks. However, there are
still many cases in which performance remains far from that of humans. A major
difficulty arises in distinguishing between transitive actions in which the
overall actor pose is similar, and recognition therefore depends on details of
the grasp and the object, which may be largely occluded. In this paper we
demonstrate how recognition is improved by obtaining precise localization of
the action-object and consequently extracting details of the object shape
together with the actor-object interaction. To obtain exact localization of the
action object and its interaction with the actor, we employ a coarse-to-fine
approach which combines semantic segmentation and contextual features, in
successive stages. We focus on (but are not limited) to face-related actions, a
set of actions that includes several currently challenging categories. We
present an average relative improvement of 35% over state-of-the art and
validate through experimentation the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03816</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03816</id><created>2015-11-12</created><updated>2015-12-10</updated><authors><author><keyname>Webb</keyname><forenames>Geoffrey I.</forenames></author><author><keyname>Hyde</keyname><forenames>Roy</forenames></author><author><keyname>Cao</keyname><forenames>Hong</forenames></author><author><keyname>Nguyen</keyname><forenames>Hai Long</forenames></author><author><keyname>Petitjean</keyname><forenames>Francois</forenames></author></authors><title>Characterizing Concept Drift</title><categories>cs.LG cs.AI</categories><comments>Accepted for publication in Data Mining and Knowledge Discovery</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most machine learning models are static, but the world is dynamic, and
increasing online deployment of learned models gives increasing urgency to the
development of efficient and effective mechanisms to address learning in the
context of non-stationary distributions, or as it is commonly called concept
drift. However, the key issue of characterizing the different types of drift
that can occur has not previously been subjected to rigorous definition and
analysis. In particular, while some qualitative drift categorizations have been
proposed, few have been formally defined, and the quantitative descriptions
required for precise and objective understanding of learner performance have
not existed. We present the first comprehensive framework for quantitative
analysis of drift. This supports the development of the first comprehensive set
of formal definitions of types of concept drift. The formal definitions clarify
ambiguities and identify gaps in previous definitions, giving rise to a new
comprehensive taxonomy of concept drift types and a solid foundation for
research into mechanisms to detect and address concept drift.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03827</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03827</id><created>2015-11-12</created><authors><author><keyname>Esperet</keyname><forenames>Louis</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Daniel</forenames></author><author><keyname>Labourel</keyname><forenames>Arnaud</forenames></author></authors><title>Coloring non-crossing strings</title><categories>math.CO cs.CG</categories><comments>17 pages. A preliminary version of this work appeared in the
  proceedings of EuroComb'09 under the title &quot;Coloring a set of touching
  strings&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a family of geometric objects in the plane
$\mathcal{F}=\{S_1,\ldots,S_n\}$, define $\chi(\mathcal{F})$ as the least
integer $\ell$ such that the elements of $\mathcal{F}$ can be colored with
$\ell$ colors, in such a way that any two intersecting objects have distinct
colors. When $\mathcal{F}$ is a set of pseudo-disks that may only intersect on
their boundaries, and such that any point of the plane is contained in at most
$k$ pseudo-disks, it can be proven that $\chi(\mathcal{F})\le 3k/2 + o(k)$
since the problem is equivalent to cyclic coloring of plane graphs. In this
paper, we study the same problem when pseudo-disks are replaced by a family
$\mathcal{F}$ of pseudo-segments (a.k.a. strings) that do not cross. In other
words, any two strings of $\mathcal{F}$ are only allowed to &quot;touch&quot; each other.
Such a family is said to be $k$-touching if no point of the plane is contained
in more than $k$ elements of $\mathcal{F}$. We give bounds on
$\chi(\mathcal{F})$ as a function of $k$, and in particular we show that
$k$-touching segments can be colored with $k+5$ colors. This partially answers
a question of Hlin\v{e}n\'y (1998) on the chromatic number of contact systems
of strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03829</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03829</id><created>2015-11-12</created><updated>2016-01-30</updated><authors><author><keyname>Schneider</keyname><forenames>Johannes</forenames></author></authors><title>Secure Numerical and Logical Multi Party Operations</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive algorithms for efficient secure numerical and logical operations
using a recently introduced scheme for secure multi-party
computation~\cite{sch15} in the semi-honest model ensuring statistical or
perfect security. To derive our algorithms for trigonometric functions, we use
basic mathematical laws in combination with properties of the additive
encryption scheme in a novel way. For division and logarithm we use a new
approach to compute a Taylor series at a fixed point for all numbers. All our
logical operations such as comparisons and large fan-in AND gates are perfectly
secure. Our empirical evaluation yields speed-ups of more than a factor of 100
for the evaluated operations compared to the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03853</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03853</id><created>2015-11-12</created><updated>2015-11-17</updated><authors><author><keyname>Kuzborskij</keyname><forenames>Ilja</forenames></author><author><keyname>Carlucci</keyname><forenames>Fabio Maria</forenames></author><author><keyname>Caputo</keyname><forenames>Barbara</forenames></author></authors><title>When Na\&quot;ive Bayes Nearest Neighbours Meet Convolutional Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since Convolutional Neural Networks (CNNs) have become the leading learning
paradigm in visual recognition, Naive Bayes Nearest Neighbour (NBNN)-based
classifiers have lost momentum in the community. This is because (1) such
algorithms cannot use CNN activations as input features; (2) they cannot be
used as final layer of CNN architectures for end-to-end training , and (3) they
are generally not scalable and hence cannot handle big data. This paper
proposes a framework that addresses all these issues, thus bringing back NBNNs
on the map. We solve the first by extracting CNN activations from local patches
at multiple scale levels, similarly to [1]. We address simultaneously the
second and third by proposing a scalable version of Naive Bayes Non-linear
Learning (NBNL, [2]). Results obtained using pre-trained CNNs on standard scene
and domain adaptation databases show the strength of our approach, opening a
new season for NBNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03855</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03855</id><created>2015-11-12</created><authors><author><keyname>Li</keyname><forenames>Wu-Jun</forenames></author><author><keyname>Wang</keyname><forenames>Sheng</forenames></author><author><keyname>Kang</keyname><forenames>Wang-Cheng</forenames></author></authors><title>Feature Learning based Deep Supervised Hashing with Pairwise Labels</title><categories>cs.LG cs.CV</categories><acm-class>H.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed wide application of hashing for large-scale image
retrieval. However, most existing hashing methods are based on hand-crafted
features which might not be optimally compatible with the hashing procedure.
Recently, deep hashing methods have been proposed to perform simultaneous
feature learning and hash-code learning with deep neural networks, which have
shown better performance than traditional hashing methods with hand-crafted
features. Most of these deep hashing methods are supervised whose supervised
information is given with triplet labels. For another common application
scenario with pairwise labels, there have not existed methods for simultaneous
feature learning and hash-code learning. In this paper, we propose a novel deep
hashing method, called deep pairwise-supervised hashing~(DPSH), to perform
simultaneous feature learning and hash-code learning for applications with
pairwise labels. Experiments on real datasets show that our DPSH method can
outperform other methods to achieve the state-of-the-art performance in image
retrieval applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03870</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03870</id><created>2015-11-12</created><authors><author><keyname>Ben-Zvi</keyname><forenames>Adi</forenames></author><author><keyname>Blackburn</keyname><forenames>Simon R.</forenames></author><author><keyname>Tsaban</keyname><forenames>Boaz</forenames></author></authors><title>A Practical Cryptanalysis of the Algebraic Eraser</title><categories>math.GR cs.CR</categories><comments>14 pages</comments><msc-class>20F36, 94A60, 20B40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anshel, Anshel, Goldfeld and Lemieaux introduced the Colored Burau Key
Agreement Protocol (CBKAP) as the concrete instantiation of their Algebraic
Eraser scheme. This scheme, based on techniques from permutation groups, matrix
groups and braid groups, is designed for lightweight environments such as RFID
tags and other IoT applications. It is proposed as an underlying technology for
ISO/IEC 29167-20. SecureRF, the company owning the trademark Algebraic Eraser,
has presented the scheme to the IRTF with a view towards standardisation.
  We present a novel cryptanalysis of this scheme. For parameter sizes
corresponding to claimed 128-bit security, our implementation recovers the
shared key using less than 8 CPU hours, and less than 64MB of memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03881</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03881</id><created>2015-11-12</created><authors><author><keyname>Bravo-Santos</keyname><forenames>&#xc1;ngel</forenames></author></authors><title>Direct Polarization for q-ary Source and Channel Coding</title><categories>cs.IT math.IT</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basic polar transformation introduced by Arikan for binary codes also
polarizes over finite fields of prime order and more general transformations
polarize over finite fields. Direct coding of q-ary sources and channels is a
process that can be implemented with simple and efficient algorithms. However,
direct polar decoding of q-ary sources and channels is a difficult task. In
this paper we introduce a likelihood ratio (LR) vector that can be expressed
recursively for decoding q-ary polar codes defined via the basic polar
transformation for finite fields of prime order, or via an extended polar
transformation for finite fields. With the recursive LR the successive
cancellation (SC) decoding is applied in a straightforward way. The complexity
is quadratic in the order of the field, but the use of the LR vector introduces
factors that soften that complexity. The Bhattacharyya parameters are expressed
as a function of the LR vectors, as in the binary case, facilitating the
construction of the codes. We have applied direct polar coding to sources and
channels with alphabets and signal constellations of various sizes, from 5 to
1024, and different codeword lengths. Our results suggest that direct q-ary
polar coding could be used in real scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03885</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03885</id><created>2015-11-12</created><authors><author><keyname>Sivalingam</keyname><forenames>Karthee</forenames></author><author><keyname>Lister</keyname><forenames>Grenville</forenames></author><author><keyname>Lawrence</keyname><forenames>Bryan</forenames></author></authors><title>Performance analysis and Optimisation of the Met Unified Model on a Cray
  XC30</title><categories>physics.comp-ph cs.DC physics.ao-ph</categories><comments>16 pages, 9 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unified Model (UM) code supports simulation of weather, climate and earth
system processes. It is primarily developed by the UK Met Office, but in recent
years a wider community of users and developers have grown around the code.
Here we present results from the optimisation work carried out by the UK
National Centre for Atmospheric Science (NCAS) for a high resolution
configuration (N512 $\approx$ 25km) on the UK ARCHER supercomputer, a Cray
XC-30. On ARCHER, we use Cray Performance Analysis Tools (CrayPAT) to analyse
the performance of UM and then Cray Reveal to identify and parallelise serial
loops using OpenMP directives. We compare performance of the optimised version
at a range of scales, and with a range of optimisations, including altered MPI
rank placement, and addition of OpenMP directives. It is seen that improvements
in MPI configuration yield performance improvements of between 5 and 12\%, and
the added OpenMP directives yield an additional 5-16\% speedup. We also
identify further code optimisations which could yield yet greater improvement
in performance. We note that speedup gained using addition of OpenMP directives
does not result in improved performance on the IBM Power platform where much of
the code has been developed. This suggests that performance gains on future
heterogeneous architectures will be hard to port. Nonetheless, it is clear that
the investment of months in analysis and optimisation has yielded performance
gains that correspond to the saving of tens of millions of core-hours on
current climate projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03890</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03890</id><created>2015-11-12</created><authors><author><keyname>Yuan</keyname><forenames>Xin</forenames></author></authors><title>Generalized Alternating Projection Based Total Variation Minimization
  for Compressive Sensing</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the total variation (TV) minimization problem used for
compressive sensing and solve it using the generalized alternating projection
(GAP) algorithm. Extensive results demonstrate the high performance of proposed
algorithm on compressive sensing, including two dimensional images,
hyperspectral images and videos. We further derive the Alternating Direction
Method of Multipliers (ADMM) framework with TV minimization for video and
hyperspectral image compressive sensing under the CACTI and CASSI framework,
respectively. Connections between GAP and ADMM are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03894</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03894</id><created>2015-11-12</created><authors><author><keyname>Kilcullen</keyname><forenames>Joseph</forenames></author></authors><title>The Game of Phishing</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phishing attacks occur because of a failure of computer users to authenticate
Bob. The computer user's role, her job, is to authenticate Bob. Nobody else can
carry out this task. I researched the ability of browsers to counterfeit the
behaviour of installed software. The objective was to find a signalling
strategy which would protect against counterfeiting i.e. phishing attacks. The
research indicates that a user-browser shared secret cannot be counterfeited
because Mallory cannot counterfeit what Mallory does not know. After your
browser has verified a TLS certificate's digital signature the browser should
create a two page login wizard. The first page should display a random
educational message, to inform and educate users about the process. The second
page will show the user (1) the user-browser shared secret, (2) the verified
identity credentials from the TLS certificate and (3) the input fields for the
user to enter her login credentials. The shared secret prevents counterfeiting,
prevents phishing. Computer users can now authenticate Bob by examining the TLS
certificate's identity credentials. The educational messages will communicate
to the user, the issues and pitfalls involved. On accepting Bob, as Bob, the
user can enter her login credentials and login.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03897</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03897</id><created>2015-11-12</created><authors><author><keyname>de Farias</keyname><forenames>Tarcisio Mendes</forenames><affiliation>Le2i</affiliation></author><author><keyname>Roxin</keyname><forenames>Ana</forenames><affiliation>Le2i</affiliation></author><author><keyname>Nicolle</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author></authors><title>IfcWoD, Semantically Adapting IFC Model Relations into OWL Properties</title><categories>cs.AI</categories><comments>In proceedings of the 32nd CIB W78 Conference on Information
  Technology in Construction, Oct 2015, Eindhoven, Netherlands</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of Building Information Modelling, ontologies have been
identified as interesting in achieving information interoperability. Regarding
the construction and facility management domains, several IFC (Industry
Foundation Classes) based ontologies have been developed, such as IfcOWL. In
the context of ontology modelling, the constraint of optimizing the size of IFC
STEP-based files can be leveraged. In this paper, we propose an adaptation of
the IFC model into OWL which leverages from all modelling constraints required
by the object-oriented structure of IFC schema. Therefore, we do not only
present a syntactic but also a semantic adaptation of the IFC model. Our model
takes into consideration the meaning of entities, relationships, properties and
attributes defined by the IFC standard. Our approach presents several
advantages compared to other initiatives such as the optimization of query
execution time. Every advantage is defended by means of practical examples and
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03908</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03908</id><created>2015-11-12</created><updated>2015-12-08</updated><authors><author><keyname>Neverova</keyname><forenames>Natalia</forenames></author><author><keyname>Wolf</keyname><forenames>Christian</forenames></author><author><keyname>Lacey</keyname><forenames>Griffin</forenames></author><author><keyname>Fridman</keyname><forenames>Lex</forenames></author><author><keyname>Chandra</keyname><forenames>Deepak</forenames></author><author><keyname>Barbello</keyname><forenames>Brandon</forenames></author><author><keyname>Taylor</keyname><forenames>Graham</forenames></author></authors><title>Learning Human Identity from Motion Patterns</title><categories>cs.LG cs.CV cs.NE</categories><comments>10 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a large-scale study, exploring the capability of temporal deep
neural networks in interpreting natural human kinematics and introduce the
first method for active biometric authentication with mobile inertial sensors.
At Google, we have created a first-of-its-kind dataset of human movements,
passively collected by 1500 volunteers using their smartphones daily over
several months. We (1) compare several neural architectures for efficient
learning of temporal multi-modal data representations, (2) propose an optimized
shift-invariant dense convolutional mechanism (DCWRNN) and (3) incorporate the
discriminatively-trained dynamic features in a probabilistic generative
framework taking into account temporal characteristics. Our results
demonstrate, that human kinematics convey important information about user
identity and can serve as a valuable component of multi-modal authentication
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03924</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03924</id><created>2015-11-12</created><authors><author><keyname>Gruzitis</keyname><forenames>Normunds</forenames></author><author><keyname>Dann&#xe9;lls</keyname><forenames>Dana</forenames></author></authors><title>A Multilingual FrameNet-based Grammar and Lexicon for Controlled Natural
  Language</title><categories>cs.CL</categories><doi>10.1007/s10579-015-9321-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Berkeley FrameNet is a lexico-semantic resource for English based on the
theory of frame semantics. It has been exploited in a range of natural language
processing applications and has inspired the development of framenets for many
languages. We present a methodological approach to the extraction and
generation of a computational multilingual FrameNet-based grammar and lexicon.
The approach leverages FrameNet-annotated corpora to automatically extract a
set of cross-lingual semantico-syntactic valence patterns. Based on data from
Berkeley FrameNet and Swedish FrameNet, the proposed approach has been
implemented in Grammatical Framework (GF), a categorial grammar formalism
specialized for multilingual grammars. The implementation of the grammar and
lexicon is supported by the design of FrameNet, providing a frame semantic
abstraction layer, an interlingual semantic API (application programming
interface), over the interlingual syntactic API already provided by GF Resource
Grammar Library. The evaluation of the acquired grammar and lexicon shows the
feasibility of the approach. Additionally, we illustrate how the FrameNet-based
grammar and lexicon are exploited in two distinct multilingual controlled
natural language applications. The produced resources are available under an
open source license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03925</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03925</id><created>2015-11-12</created><authors><author><keyname>Borkowski</keyname><forenames>M.</forenames></author></authors><title>The scalability of the matrices in direct Trefftz method in 2D Laplace
  problem</title><categories>cs.NA math.NA</categories><doi>10.1016/j.enganabound.2015.11.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an interesting property of the matrices that may be
obtained with the use of direct Trefftz method. It is proved analytically for
2D Laplace problem that values of the elements of matrices describing the
capacitance of two scaled domains are inversely proportional to the scalability
factor. As an example of the application the capacitance extraction problem is
chosen. Concise description of the algorithm in which the scalability property
can be utilized is given. Furthermore some numerical results of the algorithm
are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03927</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03927</id><created>2015-11-12</created><updated>2016-02-16</updated><authors><author><keyname>Becchetti</keyname><forenames>Luca</forenames></author><author><keyname>Clementi</keyname><forenames>Andrea</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author><author><keyname>Pasquale</keyname><forenames>Francesco</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Find Your Place: Simple Distributed Algorithms for Community Detection</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an underlying network, the &quot;averaging dynamics&quot; is the following
distributed process: Initially, each node sets its local value to an element of
{-1,1}, uniformly at random and independently of other nodes. Then, in each
consecutive round, every node updates its value to the average of its
neighbors. We show that when the graph is organized into two equal-sized
well-connected &quot;communities&quot; separated by a sparse cut, the temporal behavior
of values of nodes in different communities are different and such a difference
can be independently checked by each node. Depending on our assumptions on the
underlying graph, this can either hold for all or for most nodes. Either way,
this behavior allows nodes to locally identify (exactly or approximately) the
structure of the two communities.
  We show that the conditions under which the above happens are satisfied with
high probability in graphs sampled according to the stochastic block model
&quot;G_{2n,p,q}&quot;, provided that &quot;(p-q)n &gt;&gt; \sqrt{(p+q) n \log n}&quot; and &quot;q&quot; is large
enough to guarantee that the graph is connected w.h.p. (i.e., &quot;q = \Omega(\log
n / n^2)&quot;). In this case the nodes are able to perform approximate
reconstruction of the community structure in &quot;O(\log n)&quot; time.
  With respect to an analog of the stochastic block model for regular graphs,
our algorithm based on the averaging dynamics performs exact reconstruction in
a logarithmic number of rounds for the full range of parameters for which exact
reconstruction is known to be doable in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03928</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03928</id><created>2015-11-12</created><authors><author><keyname>Pan</keyname><forenames>Lebing</forenames></author></authors><title>Spectral Precoding for Out-of-band Power Reduction under Condition
  Number Constraint in OFDM-Based System</title><categories>cs.IT cs.CE math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the flexibility in spectrum shaping, orthogonal frequency division
multiplexing (OFDM) is a promising technique for dynamic spectrum access.
However, the out-of-band (OOB) power radiation of OFDM introduces significant
interference to the adjacent users. This problem is serious in cognitive radio
(CR) networks, which enables the secondary system to access the instantaneous
spectrum hole. Existing methods either do not effectively reduce the OOB power
leakage or introduce significant bit-error-rate (BER) performance deterioration
in the receiver. In this paper, a joint spectral precoding (JSP) scheme is
developed for OOB power reduction by the matrix operations of orthogonal
projection and singular value decomposition (SVD). We also propose an algorithm
to design the precoding matrix under receive performance constraint, which is
converted to matrix condition number constraint in practice. This method well
achieves the desirable spectrum envelope and receive performance by selecting
zero-forcing frequencies. Simulation results show that the OOB power decreases
significantly by the proposed scheme under condition number constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03932</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03932</id><created>2015-11-12</created><authors><author><keyname>Hassanzadeh</keyname><forenames>P.</forenames></author><author><keyname>Erkip</keyname><forenames>E.</forenames></author><author><keyname>Llorca</keyname><forenames>J.</forenames></author><author><keyname>Tulino</keyname><forenames>A.</forenames></author></authors><title>Distortion-Memory Tradeoffs in Cache-Aided Wireless Video Delivery</title><categories>cs.IT math.IT</categories><comments>To appear in Allerton 2015 Proceedings of the 53rd annual Allerton
  conference on Communication, control, and computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile network operators are considering caching as one of the strategies to
keep up with the increasing demand for high-definition wireless video
streaming. By prefetching popular content into memory at wireless access points
or end user devices, requests can be served locally, relieving strain on
expensive backhaul. In addition, using network coding allows the simultaneous
serving of distinct cache misses via common coded multicast transmissions,
resulting in significantly larger load reductions compared to those achieved
with conventional delivery schemes. However, prior work does not exploit the
properties of video and simply treats content as fixed-size files that users
would like to fully download. Our work is motivated by the fact that video can
be coded in a scalable fashion and that the decoded video quality depends on
the number of layers a user is able to receive. Using a Gaussian source model,
caching and coded delivery methods are designed to minimize the squared error
distortion at end user devices. Our work is general enough to consider
heterogeneous cache sizes and video popularity distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03935</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03935</id><created>2015-11-12</created><authors><author><keyname>Pathirage</keyname><forenames>Milinda</forenames></author><author><keyname>Plale</keyname><forenames>Beth</forenames></author></authors><title>Fast Data Management with Distributed Streaming SQL</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To stay competitive in today's data driven economy, enterprises large and
small are turning to stream processing platforms to process high volume, high
velocity, and diverse streams of data (fast data) as they arrive. Low-level
programming models provided by the popular systems of today suffer from lack of
responsiveness to change: enhancements require code changes with attendant
large turn-around times. Even though distributed SQL query engines have been
available for Big Data, we still lack support for SQL-based stream querying
capabilities in distributed stream processing systems. In this white paper, we
identify a set of requirements and propose a standard SQL based streaming query
model for management of what has been referred to as Fast Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03937</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03937</id><created>2015-11-12</created><authors><author><keyname>Pattanayak</keyname><forenames>Sukhamoy</forenames></author><author><keyname>Singh</keyname><forenames>Abhay Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>Pratyush</forenames></author></authors><title>DNA Cyclic Codes Over The Ring $ \F_2[u,v]/\langle u^2-1,v^3-v,uv-vu
  \rangle$</title><categories>cs.IT math.IT</categories><comments>17 pages, 4 Tables(Table 1 contained 2 pages). arXiv admin note:
  substantial text overlap with arXiv:1508.02015; text overlap with
  arXiv:1508.07113, arXiv:1505.06263 by other authors</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In this paper, we mainly study the some structure of cyclic DNA codes of odd
length over the ring $R = \F_2[u,v]/\langle u^2-1,v^3-v,uv-vu \rangle$ which
play an important role in DNA computing. We established a direct link between
the element of ring $R$ and 64 codons by introducing a Gray map from $R$ to
$R_1 = F_2 + uF_2, u^2 = 1$ where $R_1$ is the ring of four elements. The
reverse constrain and the reverse-complement constraint codes over $R$ and
$R_1$ are studied in this paper. Binary image of the cyclic codes over R also
study. The paper concludes with some example on DNA codes obtained via gray
map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03947</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03947</id><created>2015-11-12</created><authors><author><keyname>Glynn</keyname><forenames>Chris</forenames></author><author><keyname>Tokdar</keyname><forenames>Surya T.</forenames></author><author><keyname>Banks</keyname><forenames>David L.</forenames></author><author><keyname>Howard</keyname><forenames>Brian</forenames></author></authors><title>Bayesian Analysis of Dynamic Linear Topic Models</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dynamic topic modeling, the proportional contribution of a topic to a
document depends on the temporal dynamics of that topic's overall prevalence in
the corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) by
explicitly modeling document level topic proportions with covariates and
dynamic structure that includes polynomial trends and periodicity. A Markov
Chain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentation
is developed for posterior inference. Conditional independencies in the model
and sampling are made explicit, and our MCMC algorithm is parallelized where
possible to allow for inference in large corpora. To address computational
bottlenecks associated with Polya-Gamma sampling, we appeal to the Central
Limit Theorem to develop a Gaussian approximation to the Polya-Gamma random
variable. This approximation is fast and reliable for parameter values relevant
in the text mining domain. Our model and inference algorithm are validated with
multiple simulation examples, and we consider the application of modeling
trends in PubMed abstracts. We demonstrate that sharing information across
documents is critical for accurately estimating document-specific topic
proportions. We also show that explicitly modeling polynomial and periodic
behavior improves our ability to predict topic prevalence at future time
points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03958</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03958</id><created>2015-11-12</created><authors><author><keyname>Botelho</keyname><forenames>Luis</forenames></author><author><keyname>Nunes</keyname><forenames>Luis</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author><author><keyname>Lopes</keyname><forenames>Rui J.</forenames></author></authors><title>Software Agents with Concerns of their Own</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We claim that it is possible to have artificial software agents for which
their actions and the world they inhabit have first-person or intrinsic
meanings. The first-person or intrinsic meaning of an entity to a system is
defined as its relation with the system's goals and capabilities, given the
properties of the environment in which it operates. Therefore, for a system to
develop first-person meanings, it must see itself as a goal-directed actor,
facing limitations and opportunities dictated by its own capabilities, and by
the properties of the environment. The first part of the paper discusses this
claim in the context of arguments against and proposals addressing the
development of computer programs with first-person meanings. A set of
definitions is also presented, most importantly the concepts of cold and
phenomenal first-person meanings. The second part of the paper presents
preliminary proposals and achievements, resulting of actual software
implementations, within a research approach that aims to develop software
agents that intrinsically understand their actions and what happens to them. As
a result, an agent with no a priori notion of its goals and capabilities, and
of the properties of its environment acquires all these notions by observing
itself in action. The cold first-person meanings of the agent's actions and of
what happens to it are defined using these acquired notions. Although not
solving the full problem of first-person meanings, the proposed approach and
preliminary results allow us some confidence to address the problems yet to be
considered, in particular the phenomenal aspect of first-person meanings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03961</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03961</id><created>2015-11-12</created><authors><author><keyname>Zhang</keyname><forenames>Jingjing</forenames></author><author><keyname>Elia</keyname><forenames>Petros</forenames></author></authors><title>Fundamental Limits of Cache-Aided Wireless BC: Interplay of
  Coded-Caching and CSIT Feedback</title><categories>cs.IT math.IT</categories><comments>26 pages, 6 figures, submission Trans IT</comments><report-no>RR-15-307</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building on the recent breakthrough by Maddah-Ali and Niesen that revealed
substantial interference-removal gains due to coded caching in single-stream
communications, the work here considers the K-user cache-aided MISO broadcast
channel (BC) with random fading and imperfect feedback, and analyzes the
throughput performance as a function of feedback statistics and cache size. The
work here identifies the optimal cache-aided degrees-of-freedom performance
within a factor of 2, by identifying near-optimal schemes that combine data
caching, folding and retrospective precoding, to efficiently utilize caching
and feedback resources. The key lies in exploiting the retrospective nature
shared by both coded caching and (communicating with) non-timely feedback (the
schemes are supported by obsolete CSIT), where in both cases the transmitter
must retroactively compensate for not knowing the `destination' (channel and
user identity) of this content. The work offers a first study of the
synergistic and competing interplay between caching and feedback quality, and
it applies towards striking the right balance between cache-induced
multi-casting gains, and CSIT-induced broadcasting gains.
  In some asymptotic cases of large K, the designed schemes are optimal. The
derived limits interestingly reveal that now, the gains from coded caching no
longer scale linearly with the (normalized) cache size as they did in the
original single-stream case, but rather have an exponential element --- for
smaller caches --- in the sense that any linear decrease in the required
performance, allows for an exponential reduction in the required cache size.
This means that now, a very small fraction of the library at each cache, can
cover substantial ground toward approaching the optimal scaling laws of even
large BC systems without any timely CSIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03962</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03962</id><created>2015-11-12</created><updated>2016-02-21</updated><authors><author><keyname>Ji</keyname><forenames>Yangfeng</forenames></author><author><keyname>Cohn</keyname><forenames>Trevor</forenames></author><author><keyname>Kong</keyname><forenames>Lingpeng</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>Document Context Language Models</title><categories>cs.CL cs.LG stat.ML</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text documents are structured on multiple levels of detail: individual words
are related by syntax, but larger units of text are related by discourse
structure. Existing language models generally fail to account for discourse
structure, but it is crucial if we are to have language models that reward
coherence and generate coherent texts. We present and empirically evaluate a
set of multi-level recurrent neural network language models, called
Document-Context Language Models (DCLM), which incorporate contextual
information both within and beyond the sentence. In comparison with word-level
recurrent neural network language models, the DCLM models obtain slightly
better predictive likelihoods, and considerably better assessments of document
coherence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03979</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03979</id><created>2015-11-12</created><updated>2015-12-04</updated><authors><author><keyname>McClure</keyname><forenames>Patrick</forenames></author><author><keyname>Kriegeskorte</keyname><forenames>Nikolaus</forenames></author></authors><title>Representational Distance Learning for Deep Neural Networks</title><categories>cs.NE cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose representational distance learning (RDL), a technique that allows
transferring knowledge from an arbitrary model with task related information to
a deep neural network (DNN). This method seeks to maximize the similarity
between the representational distance matrices (RDMs) of a model with desired
knowledge, the teacher, and a DNN currently being trained, the student. The
knowledge contained in the information transformations performed by the teacher
are transferred to a student using auxiliary error functions. This allows a DNN
to simultaneously learn from a teacher model and learn to perform some task
within the framework of backpropagation. We test the use of RDL for knowledge
distillation, also known as model compression, from a large teacher DNN to a
small student DNN using the MNIST and CIFAR-10 datasets. Also, we test the use
of RDL for knowledge transfer between tasks using the CIFAR-10 and CIFAR-100
datasets. For each test, RDL significantly improves performance when compared
to traditional backpropagation alone and performs similarly to, or better than,
recently proposed methods for model compression and knowledge transfer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03981</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03981</id><created>2015-11-12</created><updated>2015-11-12</updated><authors><author><keyname>Hidalgo</keyname><forenames>Cesar A.</forenames></author></authors><title>Disconnected! The parallel streams of network literature in the natural
  and social sciences</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During decades the study of networks has been divided between the efforts of
social scientists and natural scientists, two groups of scholars who often do
not see eye to eye. In this review I present an effort to mutually translate
the work conducted by scholars from both of these academic fronts hoping to
unify what has become a diverging body of literature. I argue that social and
natural scientists fail to see eye to eye because they have diverging academic
goals. Social scientists focus on explaining how context specific social and
economic mechanisms drive the structure of networks and on how networks shape
social and economic outcomes. By contrast, natural scientists focus primarily
on modeling network characteristics that are independent of context, since
their focus is to identify universal characteristics of systems instead of
context specific mechanisms. In the following pages I discuss the differences
between both of these literatures by summarizing the parallel theories advanced
to explain link formation and the applications used by scholars in each field
to justify the study of networks. I conclude by briefly reviewing the
historical sources of these differences and by providing an outlook on how
these two literatures may come closer together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03984</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03984</id><created>2015-11-12</created><authors><author><keyname>Wang</keyname><forenames>Run</forenames></author><author><keyname>Mo</keyname><forenames>Qiaoli</forenames></author><author><keyname>Zhang</keyname><forenames>Qian</forenames></author><author><keyname>Chen</keyname><forenames>Fudi</forenames></author><author><keyname>Yang</keyname><forenames>Dazuo</forenames></author></authors><title>Prediction of the Yield of Enzymatic Synthesis of Betulinic Acid Ester
  Using Artificial Neural Networks and Support Vector Machine</title><categories>cs.LG cs.NE</categories><comments>32 pages, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3\b{eta}-O-phthalic ester of betulinic acid is of great importance in
anticancer studies. However, the optimization of its reaction conditions
requires a large number of experimental works. To simplify the number of times
of optimization in experimental works, here, we use artificial neural network
(ANN) and support vector machine (SVM) models for the prediction of yields of
3\b{eta}-O-phthalic ester of betulinic acid synthesized by betulinic acid and
phthalic anhydride using lipase as biocatalyst. General regression neural
network (GRNN), multilayer feed-forward neural network (MLFN) and the SVM
models were trained based on experimental data. Four indicators were set as
independent variables, including time (h), temperature (C), amount of enzyme
(mg) and molar ratio, while the yield of the 3\b{eta}-O-phthalic ester of
betulinic acid was set as the dependent variable. Results show that the GRNN
and SVM models have the best prediction results during the testing process,
with comparatively low RMS errors (4.01 and 4.23respectively) and short
training times (both 1s). The prediction accuracy of the GRNN and SVM are both
100% in testing process, under the tolerance of 30%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03995</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03995</id><created>2015-11-12</created><authors><author><keyname>Lore</keyname><forenames>Kin Gwn</forenames></author><author><keyname>Akintayo</keyname><forenames>Adedotun</forenames></author><author><keyname>Sarkar</keyname><forenames>Soumik</forenames></author></authors><title>LLNet: A Deep Autoencoder Approach to Natural Low-light Image
  Enhancement</title><categories>cs.CV</categories><comments>10 pages (including 2 pages for references). Submitted for review for
  CVPR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In surveillance, monitoring and tactical reconnaissance, gathering the right
visual information from a dynamic environment and accurately processing such
data are essential ingredients to making informed decisions which determines
the success of an operation. Camera sensors are often cost-limited in ability
to clearly capture objects without defects from images or videos taken in a
poorly-lit environment. The goal in many applications is to enhance the
brightness, contrast and reduce noise content of such images in an on-board
real-time manner. We propose a deep autoencoder-based approach to identify
signal features from low-light images handcrafting and adaptively brighten
images without over-amplifying the lighter parts in images (i.e., without
saturation of image pixels) in high dynamic range. We show that a variant of
the recently proposed stacked-sparse denoising autoencoder can learn to
adaptively enhance and denoise from synthetically darkened and noisy training
examples. The network can then be successfully applied to naturally low-light
environment and/or hardware degraded images. Results show significant
credibility of deep learning based approaches both visually and by quantitative
comparison with various popular enhancing, state-of-the-art denoising and
hybrid enhancing-denoising techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03996</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03996</id><created>2015-11-12</created><authors><author><keyname>Vu</keyname><forenames>Thanh Long</forenames></author><author><keyname>Chiang</keyname><forenames>Hsiao-Dong</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Smart Transmission Network Emergency Control</title><categories>cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1510.07325</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power systems normally operate at their stable operating conditions where the
power supply and demand are balanced. In emergency situations, the operators
proceed to cut a suitable amount of loads to rebalance the supply-demand and
hopefully stabilize the system. This traditional emergency control scheme
results in interrupted service with severely economic damages to customers. In
order to provide seamless electricity service to customers, this paper proposes
a viable alternative for traditional remedial controls of power grids by
exploiting the plentiful transmission facilities. In particular, we consider
two emergency control schemes involving adjustment of the susceptance of a
number of selected transmission lines to drive either fault-on dynamics or
post-fault dynamics, and thereby stabilize the system under emergency
situations. The corresponding emergency control problems will be formulated and
partly solved in some specific cases. Simple numerical simulation will be used
to illustrate the concept of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.03999</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.03999</id><created>2015-11-12</created><updated>2015-11-12</updated><authors><author><keyname>He</keyname><forenames>Liang</forenames></author><author><keyname>Pan</keyname><forenames>Jia</forenames></author><author><keyname>Li</keyname><forenames>Danwei</forenames></author><author><keyname>Manocha</keyname><forenames>Dinesh</forenames></author></authors><title>Efficient Penetration Depth Computation between Rigid Models using
  Contact Space Propagation Sampling</title><categories>cs.RO</categories><comments>10 pages. add the acknowledgement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method to compute the approximate global penetration depth
(PD) between two non-convex geometric models. Our approach consists of two
phases: offline precomputation and run-time queries. In the first phase, our
formulation uses a novel sampling algorithm to precompute an approximation of
the high-dimensional contact space between the pair of models. As compared with
prior random sampling algorithms for contact space approximation, our
propagation sampling considerably speeds up the precomputation and yields a
high quality approximation. At run-time, we perform a nearest-neighbor query
and local projection to efficiently compute the translational or generalized
PD. We demonstrate the performance of our approach on complex 3D benchmarks
with tens or hundreds of thousands of triangles, and we observe significant
improvement over previous methods in terms of accuracy, with a modest
improvement in the run-time performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04001</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04001</id><created>2015-11-12</created><authors><author><keyname>Hahn</keyname><forenames>Lars</forenames></author><author><keyname>Leimeister</keyname><forenames>Chris-Andr&#xe9;</forenames></author><author><keyname>Morgenstern</keyname><forenames>Burkhard</forenames></author></authors><title>RasBhari: optimizing spaced seeds for database searching, read mapping
  and alignment-free sequence comparison</title><categories>q-bio.GN cs.DS q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many algorithms for sequence analysis use patterns or spaced seeds consisting
of match and don't-care positions, such that only characters at the match
positions are considered when sub-words of the sequences are counted or
compared. The performance of these approaches depends on the underlying
patterns. Herein, we show that the overlap complexity of a pattern set that was
introduced by Ilie and Ilie is closely related to the variance of the number of
spaced-word matches between two evolutionarily related sequences, with respect
to this pattern set. We propose an improved hill-climbing algorithm to optimize
sets of patterns or multiple seeds for database searching, read mapping and
alignment-free sequence comparison. Experimental results show that our approach
generates seeds with higher sensitivity than existing approaches. In our
spaced-words approach to alignment-free sequence comparison, pattern sets
calculated with RasBhari led to more accurate estimates of phylogenetic
distances than the randomly generated pattern sets that we previously used. Our
software is available at http://spaced.gobics.de/content/RasBhari.tar.gz
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04003</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04003</id><created>2015-11-12</created><authors><author><keyname>Kislyuk</keyname><forenames>Dmitry</forenames></author><author><keyname>Liu</keyname><forenames>Yuchen</forenames></author><author><keyname>Liu</keyname><forenames>David</forenames></author><author><keyname>Tzeng</keyname><forenames>Eric</forenames></author><author><keyname>Jing</keyname><forenames>Yushi</forenames></author></authors><title>Human Curation and Convnets: Powering Item-to-Item Recommendations on
  Pinterest</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Pinterest Related Pins, an item-to-item recommendation
system that combines collaborative filtering with content-based ranking. We
demonstrate that signals derived from user curation, the activity of users
organizing content, are highly effective when used in conjunction with
content-based ranking. This paper also demonstrates the effectiveness of visual
features, such as image or object representations learned from convnets, in
improving the user engagement rate of our item-to-item recommendation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04012</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04012</id><created>2015-11-06</created><authors><author><keyname>Chen</keyname><forenames>Zhixiong</forenames></author></authors><title>Linear complexity and trace representation of quaternary sequences over
  $\mathbb{Z}_4$ based on generalized cyclotomic classes modulo $pq$</title><categories>math.NT cs.CR</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a family of quaternary sequences over the residue class ring modulo
$4$ of length $pq$, a product of two distinct odd primes, using the generalized
cyclotomic classes modulo $pq$ and calculate the discrete Fourier transform
(DFT) of the sequences. The DFT helps us to determine the exact values of
linear complexity and the trace representation of the sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04023</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04023</id><created>2015-11-12</created><authors><author><keyname>Ma</keyname><forenames>Qian</forenames></author><author><keyname>Liu</keyname><forenames>Ya-Feng</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Time and Location Aware Mobile Data Pricing</title><categories>cs.NI</categories><comments>This manuscript serves as the online technical report of the article
  accepted by IEEE Transactions on Mobile Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile users' correlated mobility and data consumption patterns often lead to
severe cellular network congestion in peak hours and hot spots. This paper
presents an optimal design of time and location aware mobile data pricing,
which incentivizes users to smooth traffic and reduce network congestion. We
derive the optimal pricing scheme through analyzing a two-stage decision
process, where the operator determines the time and location aware prices by
minimizing his total cost in Stage I, and each mobile user schedules his mobile
traffic by maximizing his payoff (i.e., utility minus payment) in Stage II. We
formulate the two-stage decision problem as a bilevel optimization problem, and
propose a derivative-free algorithm to solve the problem for any increasing
concave user utility functions. We further develop low complexity algorithms
for the commonly used logarithmic and linear utility functions. The optimal
pricing scheme ensures a win-win situation for the operator and users.
Simulations show that the operator can reduce the cost by up to 97.52% in the
logarithmic utility case and 98.70% in the linear utility case, and users can
increase their payoff by up to 79.69% and 106.10% for the two types of
utilities, respectively, comparing with a time and location independent pricing
benchmark. Our study suggests that the operator should provide price discounts
at less crowded time slots and locations, and the discounts need to be
significant when the operator's cost of provisioning excessive traffic is high
or users' willingness to delay traffic is low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04024</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04024</id><created>2015-11-12</created><updated>2015-11-29</updated><authors><author><keyname>Seymour</keyname><forenames>Zachary</forenames></author><author><keyname>Li</keyname><forenames>Yingming</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongfei</forenames></author></authors><title>Multimodal Skip-gram Using Convolutional Pseudowords</title><categories>cs.CL cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the representational mapping across multimodal data such
that given a piece of the raw data in one modality the corresponding semantic
description in terms of the raw data in another modality is immediately
obtained. Such a representational mapping can be found in a wide spectrum of
real-world applications including image/video retrieval, object recognition,
action/behavior recognition, and event understanding and prediction. To that
end, we introduce a simplified training objective for learning multimodal
embeddings using the skip-gram architecture by introducing convolutional
&quot;pseudowords:&quot; embeddings composed of the additive combination of distributed
word representations and image features from convolutional neural networks
projected into the multimodal space. We present extensive results of the
representational properties of these embeddings on various word similarity
benchmarks to show the promise of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04026</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04026</id><created>2015-11-10</created><authors><author><keyname>Al-Sarem</keyname><forenames>Mohammed</forenames></author></authors><title>Building a Decision Tree Model for Academic Advising Affairs Based on
  the Algorithm C 4-5</title><categories>cs.OH</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The ability to recognize students weakness and solve any problem that may
confront them in timely fashion is always a target for all educational
institutions. Thus, colleges and universities implement the so-called academic
advising affairs. On the academic advisor relies the responsibility of solving
any problem that may confront students learning progress. This paper shows how
the adviser can benefit from data mining techniques, namely decision trees
techniques. The C 4.5 algorithm is used as a method for building such trees.
The output is evaluated based on the accuracy measure, Kappa measure, and ROC
area. The difference between the registered and gained credit hours is
considered as the main attribute on which advisor can rely
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04028</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04028</id><created>2015-11-12</created><authors><author><keyname>Lavery</keyname><forenames>Domanic</forenames></author><author><keyname>Ives</keyname><forenames>David</forenames></author><author><keyname>Liga</keyname><forenames>Gabriele</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Savory</keyname><forenames>Seb J.</forenames></author><author><keyname>Bayvel</keyname><forenames>Polina</forenames></author></authors><title>The Benefit of Split Nonlinearity Compensation for Optical Fiber
  Communications</title><categories>physics.optics cs.IT math.IT</categories><comments>4 pages, 2 figures, 1 tables, Letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this Letter we analyze the benefit of digital compensation of fiber
nonlinearity, where the digital signal processing is divided between the
transmitter and receiver. The application of the Gaussian noise model indicates
that, where there are two or more spans, it is always beneficial to split the
nonlinearity compensation. The theory is verified via numerical simulations,
investigating transmission of single channel 50 GBd polarization division
multiplexed 256-ary quadrature amplitude modulation over 100 km standard single
mode fiber spans, using lumped amplification. For this case, the additional
increase in mutual information achieved over transmitter- or receiver-side
nonlinearity compensation is approximately 1 bit for distances greater than
2000 km. Further, it is shown, theoretically, that the SNR gain for long
distances and high bandwidth transmission is 1.5 dB versus transmitter- or
receiver-based nonlinearity compensation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04031</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04031</id><created>2015-11-12</created><authors><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Hassner</keyname><forenames>Tal</forenames></author></authors><title>Facial Landmark Detection with Tweaked Convolutional Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel convolutional neural network (CNN) design for facial
landmark coordinate regression. We examine the intermediate features of a
standard CNN trained for landmark detection and show that features extracted
from later, more specialized layers capture rough landmark locations. This
provides a natural means of applying differential treatment midway through the
network, tweaking processing based on facial alignment. The resulting Tweaked
CNN model (TCNN) harnesses the robustness of CNNs for landmark detection, in an
appearance-sensitive manner without training multi-part or multi-scale models.
Our results on standard face landmark detection and face verification
benchmarks show TCNN to surpasses previously published performances by wide
margins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04032</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04032</id><created>2015-11-12</created><authors><author><keyname>Leme</keyname><forenames>Renato Paes</forenames></author><author><keyname>Wong</keyname><forenames>Sam Chiu-wai</forenames></author></authors><title>Computing Walrasian Equilibria: Fast Algorithms and Economic Insights</title><categories>cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first polynomial algorithm to compute a Walrasian equilibrium in
an economy with indivisible goods and general valuations having only access to
an aggregate demand oracle, i.e., an oracle that given a price vector, returns
the aggregated demand over the entire population of buyers. Our algorithm
queries the aggregate demand oracle $\tilde{O}(n)$ times and takes
$\tilde{O}(n^3)$ time, where n is the number of items.
  We also give the fastest known algorithm for computing Walrasian equilibrium
for gross substitute valuations in the value oracle model. Our algorithm has
running time $\tilde{O}((mn+n^3 )T_V)$ where $T_V$ is the cost of querying the
value oracle. En route, we give necessary and sufficient conditions for the
existence of robust Walrasian prices, i.e., price vectors such that each agent
has a unique demanded bundle and the demanded bundles clear the market. When
such prices exist, the market can be perfectly coordinated by solely using
prices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04033</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04033</id><created>2015-11-12</created><authors><author><keyname>Devijver</keyname><forenames>Emilie</forenames></author><author><keyname>Gallopin</keyname><forenames>M&#xe9;lina</forenames></author></authors><title>Block-diagonal covariance selection for high-dimensional Gaussian
  graphical models</title><categories>math.ST cs.LG stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian graphical models are widely utilized to infer and visualize networks
of dependencies between continuous variables. However, inferring the graph is
difficult when the sample size is small compared to the number of variables. To
reduce the number of parameters to estimate in the model, we propose a
non-asymptotic model selection procedure supported by strong theoretical
guarantees based on an oracle inequality and a minimax lower bound. The
covariance matrix of the model is approximated by a block-diagonal matrix. The
structure of this matrix is detected by thresholding the sample covariance
matrix, where the threshold is selected using the slope heuristic. Based on the
block-diagonal structure of the covariance matrix, the estimation problem is
divided into several independent problems: subsequently, the network of
dependencies between variables is inferred using the graphical lasso algorithm
in each block. The performance of the procedure is illustrated on simulated
data. An application to a real gene expression dataset with a limited sample
size is also presented: the dimension reduction allows attention to be
objectively focused on interactions among smaller subsets of genes, leading to
a more parsimonious and interpretable modular network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04036</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04036</id><created>2015-11-12</created><authors><author><keyname>Abrahamsen</keyname><forenames>Mikkel</forenames></author></authors><title>An Optimal Algorithm for the Separating Common Tangents of two Polygons</title><categories>cs.CG</categories><comments>12 pages, 6 figures. A preliminary version of this paper appeared at
  SoCG 2015</comments><msc-class>68U05, 65D18</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm for computing the separating common tangents of two
simple polygons using linear time and only constant workspace. A tangent of a
polygon is a line touching the polygon such that all of the polygon lies to the
same side of the line. A separating common tangent of two polygons is a tangent
of both polygons where the polygons are lying on different sides of the
tangent. Each polygon is given as a read-only array of its corners. If a
separating common tangent does not exist, the algorithm reports that.
Otherwise, two corners defining a separating common tangent are returned. The
algorithm is simple and implies an optimal algorithm for deciding if the convex
hulls of two polygons are disjoint or not. This was not known to be possible in
linear time and constant workspace prior to this paper.
  An outer common tangent is a tangent of both polygons where the polygons are
on the same side of the tangent. In the case where the convex hulls of the
polygons are disjoint, we give an algorithm for computing the outer common
tangents in linear time using constant workspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04045</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04045</id><created>2015-11-10</created><authors><author><keyname>Savic</keyname><forenames>Vladimir</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Ferrer-Coll</keyname><forenames>Javier</forenames></author><author><keyname>Stenumgaard</keyname><forenames>Peter</forenames></author></authors><title>Kernel Methods for Accurate UWB-Based Ranging with Reduced Complexity</title><categories>cs.LG cs.IT math.IT</categories><comments>published in IEEE Transactions on Wireless Communication</comments><doi>10.1109/TWC.2015.2496584</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate and robust positioning in multipath environments can enable many
applications, such as search-and-rescue and asset tracking. For this problem,
ultra-wideband (UWB) technology can provide the most accurate range estimates,
which are required for range-based positioning. However, UWB still faces a
problem with non-line-of-sight (NLOS) measurements, in which the range
estimates based on time-of-arrival (TOA) will typically be positively biased.
There are many techniques that address this problem, mainly based on NLOS
identification and NLOS error mitigation algorithms. However, these techniques
do not exploit all available information in the UWB channel impulse response.
Kernel-based machine learning methods, such as Gaussian Process Regression
(GPR), are able to make use of all information, but they may be too complex in
their original form. In this paper, we propose novel ranging methods based on
kernel principal component analysis (kPCA), in which the selected channel
parameters are projected onto a nonlinear orthogonal high-dimensional space,
and a subset of these projections is then used as an input for ranging. We
evaluate the proposed methods using real UWB measurements obtained in a
basement tunnel, and found that one of the proposed methods is able to
outperform state-of-the-art, even if little training samples are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04048</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04048</id><created>2015-11-12</created><authors><author><keyname>Mottaghi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Bagherinezhad</keyname><forenames>Hessam</forenames></author><author><keyname>Rastegari</keyname><forenames>Mohammad</forenames></author><author><keyname>Farhadi</keyname><forenames>Ali</forenames></author></authors><title>Newtonian Image Understanding: Unfolding the Dynamics of Objects in
  Static Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the challenging problem of predicting the dynamics of
objects in static images. Given a query object in an image, our goal is to
provide a physical understanding of the object in terms of the forces acting
upon it and its long term motion as response to those forces. Direct and
explicit estimation of the forces and the motion of objects from a single image
is extremely challenging. We define intermediate physical abstractions called
Newtonian scenarios and introduce Newtonian Neural Network ($N^3$) that learns
to map a single image to a state in a Newtonian scenario. Our experimental
evaluations show that our method can reliably predict dynamics of a query
object from a single image. In addition, our approach can provide physical
reasoning that supports the predicted dynamics in terms of velocity and force
vectors. To spur research in this direction we compiled Visual Newtonian
Dynamics (VIND) dataset that includes 6806 videos aligned with Newtonian
scenarios represented using game engines, and 4516 still images with their
ground truth dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04052</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04052</id><created>2015-11-11</created><authors><author><keyname>Claes</keyname><forenames>Jan</forenames></author><author><keyname>Vanderfeesten</keyname><forenames>Irene</forenames></author><author><keyname>Reijers</keyname><forenames>Hajo A.</forenames></author><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author><author><keyname>Weidlich</keyname><forenames>Matthias</forenames></author><author><keyname>Zugal</keyname><forenames>Stefan</forenames></author><author><keyname>Fahland</keyname><forenames>Dirk</forenames></author><author><keyname>Weber</keyname><forenames>Barbara</forenames></author><author><keyname>Mendling</keyname><forenames>Jan</forenames></author><author><keyname>Poels</keyname><forenames>Geert</forenames></author></authors><title>Tying Process Model Quality to the Modeling Process: The Impact of
  Structuring, Movement, and Speed</title><categories>cs.SE</categories><doi>10.1007/978-3-642-32885-5_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an investigation into the process of process modeling, we examined how
modeling behavior relates to the quality of the process model that emerges from
that. Specifically, we considered whether (i) a modeler's structured modeling
style, (ii) the frequency of moving existing objects over the modeling canvas,
and (iii) the overall modeling speed is in any way connected to the ease with
which the resulting process model can be understood. In this paper, we describe
the exploratory study to build these three conjectures, clarify the
experimental set-up and infrastructure that was used to collect data, and
explain the used metrics for the various concepts to test the conjectures
empirically. We discuss various implications for research and practice from the
conjectures, all of which were confirmed by the experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04053</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04053</id><created>2015-11-11</created><authors><author><keyname>Claes</keyname><forenames>Jan</forenames></author><author><keyname>Vanderfeesten</keyname><forenames>Irene</forenames></author><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author><author><keyname>Reijers</keyname><forenames>Hajo A.</forenames></author><author><keyname>Weber</keyname><forenames>Barbara</forenames></author><author><keyname>Poels</keyname><forenames>Geert</forenames></author></authors><title>Visualizing the Process of Process Modeling with PPMCharts</title><categories>cs.SE</categories><journal-ref>Proc. TAProViz'12, pp. 744-755, 2013</journal-ref><doi>10.1007/978-3-642-36285-9_75</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the quest for knowledge about how to make good process models, recent
research focus is shifting from studying the quality of process models to
studying the process of process modeling (often abbreviated as PPM) itself.
This paper reports on our efforts to visualize this specific process in such a
way that relevant characteristics of the modeling process can be observed
graphically. By recording each modeling operation in a modeling process, one
can build an event log that can be used as input for the PPMChart Analysis
plug-in we implemented in ProM. The graphical representation this plug-in
generates allows for the discovery of different patterns of the process of
process modeling. It also provides different views on the process of process
modeling (by configuring and filtering the charts).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04055</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04055</id><created>2015-11-11</created><authors><author><keyname>Claes</keyname><forenames>Jan</forenames></author><author><keyname>Vanderfeesten</keyname><forenames>Irene</forenames></author><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author><author><keyname>Reijers</keyname><forenames>Hajo A.</forenames></author><author><keyname>Weber</keyname><forenames>Barbara</forenames></author><author><keyname>Poels</keyname><forenames>Geert</forenames></author></authors><title>A visual analysis of the process of process modeling</title><categories>cs.SE</categories><doi>10.1007/s10257-014-0245-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The construction of business process models has become an important requisite
in the analysis and optimization of processes. The success of the analysis and
optimization efforts heavily depends on the quality of the models. Therefore, a
research domain emerged that studies the process of process modeling. This
paper contributes to this research by presenting a way of visualizing the
different steps a modeler undertakes to construct a process model, in a
so-called process of process modeling Chart. The graphical representation
lowers the cognitive efforts to discover properties of the modeling process,
which facilitates the research and the development of theory, training and tool
support for improving model quality. The paper contains an extensive overview
of applications of the tool that demonstrate its usefulness for research and
practice and discusses the observations from the visualization in relation to
other work. The visualization was evaluated through a qualitative study that
confirmed its usefulness and added value compared to the Dotted Chart on which
the visualization was inspired.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04056</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04056</id><created>2015-11-12</created><authors><author><keyname>Norouzi</keyname><forenames>Mohammad</forenames></author><author><keyname>Collins</keyname><forenames>Maxwell D.</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew</forenames></author><author><keyname>Fleet</keyname><forenames>David J.</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author></authors><title>Efficient non-greedy optimization of decision trees</title><categories>cs.LG cs.CV</categories><comments>in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision trees and randomized forests are widely used in computer vision and
machine learning. Standard algorithms for decision tree induction optimize the
split functions one node at a time according to some splitting criteria. This
greedy procedure often leads to suboptimal trees. In this paper, we present an
algorithm for optimizing the split functions at all levels of the tree jointly
with the leaf parameters, based on a global objective. We show that the problem
of finding optimal linear-combination (oblique) splits for decision trees is
related to structured prediction with latent variables, and we formulate a
convex-concave upper bound on the tree's empirical loss. The run-time of
computing the gradient of the proposed surrogate objective with respect to each
training exemplar is quadratic in the the tree depth, and thus training deep
trees is feasible. The use of stochastic gradient descent for optimization
enables effective training with large datasets. Experiments on several
classification benchmarks demonstrate that the resulting non-greedy decision
trees outperform greedy decision tree baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04057</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04057</id><created>2015-11-11</created><authors><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author><author><keyname>Soffer</keyname><forenames>Pnina</forenames></author><author><keyname>Zugal</keyname><forenames>Stefan</forenames></author><author><keyname>Weber</keyname><forenames>Barbara</forenames></author><author><keyname>Weidlich</keyname><forenames>Matthias</forenames></author><author><keyname>Fahland</keyname><forenames>Dirk</forenames></author><author><keyname>Reijers</keyname><forenames>Hajo A.</forenames></author><author><keyname>Mendling</keyname><forenames>Jan</forenames></author></authors><title>Modeling Styles in Business Process Modeling</title><categories>cs.SE</categories><journal-ref>Proc. BPMDS'12, pp. 151-166, 2012</journal-ref><doi>10.1007/978-3-642-31072-0_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on quality issues of business process models has recently begun to
explore the process of creating process models. As a consequence, the question
arises whether different ways of creating process models exist. In this vein,
we observed 115 students engaged in the act of modeling, recording all their
interactions with the modeling environment using a specialized tool. The
recordings of process modeling were subsequently clustered. Results presented
in this paper suggest the existence of three distinct modeling styles,
exhibiting significantly different characteristics. We believe that this
finding constitutes another building block toward a more comprehensive
understanding of the process of process modeling that will ultimately enable us
to support modelers in creating better business process models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04058</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04058</id><created>2015-11-11</created><authors><author><keyname>Zugal</keyname><forenames>Stefan</forenames></author><author><keyname>Soffer</keyname><forenames>Pnina</forenames></author><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author><author><keyname>Weber</keyname><forenames>Barbara</forenames></author></authors><title>Expressiveness and Understandability Considerations of Hierarchy in
  Declarative Business Process Models</title><categories>cs.SE</categories><journal-ref>Proc. BPMDS'12, pp. 167-181, 2012</journal-ref><doi>10.1007/978-3-642-31072-0_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchy has widely been recognized as a viable approach to deal with the
complexity of conceptual models. For instance, in declarative business process
models, hierarchy is realized by sub-processes. While technical implementations
of declarative sub-processes exist, their application, semantics, and the
resulting impact on understandability are less understood yet-this research gap
is addressed in this work. In particular, we discuss the semantics and the
application of hierarchy and show how sub-processes enhance the expressiveness
of declarative modeling languages. Then, we turn to the impact on the
understandability of hierarchy on a declarative process model. To
systematically assess this impact, we present a cognitive-psychology based
framework that allows to assess the possible impact of hierarchy on the
understandability of the process model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04059</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04059</id><created>2015-11-11</created><authors><author><keyname>Weber</keyname><forenames>Barbara</forenames></author><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author><author><keyname>Torres</keyname><forenames>Victoria</forenames></author><author><keyname>Reichert</keyname><forenames>Manfred</forenames></author></authors><title>Change Patterns in Use: A Critical Evaluation</title><categories>cs.SE</categories><journal-ref>Proc. BPMDS'13, pp. 261-276, 2013</journal-ref><doi>10.1007/978-3-642-38484-4_19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process model quality has been an area of considerable research efforts. In
this context, the correctness-by-construction principle of change patterns
provides promising perspectives. However, using change patterns for model
creation imposes a more structured way of modeling. While the process of
process modeling (PPM) based on change primitives has been investigated, little
is known about this process based on change patterns. To obtain a better
understanding of the PPM when using change patterns, the arising challenges,
and the subjective perceptions of process designers, we conduct an exploratory
study. The results indicate that process designers face little problems as long
as control-flow is simple, but have considerable problems with the usage of
change patterns when complex, nested models have to be created. Finally, we
outline how effective tool support for change patterns should be realized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04060</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04060</id><created>2015-11-11</created><authors><author><keyname>Weber</keyname><forenames>Barbara</forenames></author><author><keyname>Zeitelhofer</keyname><forenames>Sarah</forenames></author><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author><author><keyname>Torres</keyname><forenames>Victoria</forenames></author><author><keyname>Reichert</keyname><forenames>Manfred</forenames></author></authors><title>How Advanced Change Patterns Impact the Process of Process Modeling</title><categories>cs.SE</categories><journal-ref>Proc. BPMDS'14, pp. 17-32, 2014</journal-ref><doi>10.1007/978-3-662-43745-2_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process model quality has been an area of considerable research efforts. In
this context, correctness-by-construction as enabled by change patterns
provides promising perspectives. While the process of process modeling (PPM)
based on change primitives has been thoroughly investigated, only little is
known about the PPM based on change patterns. In particular, it is unclear what
set of change patterns should be provided and how the available change pattern
set impacts the PPM. To obtain a better understanding of the latter as well as
the (subjective) perceptions of process modelers, the arising challenges, and
the pros and cons of different change pattern sets we conduct a controlled
experiment. Our results indicate that process modelers face similar challenges
irrespective of the used change pattern set (core pattern set versus extended
pattern set, which adds two advanced change patterns to the core patterns set).
An extended change pattern set, however, is perceived as more difficult to use,
yielding a higher mental effort. Moreover, our results indicate that more
advanced patterns were only used to a limited extent and frequently applied
incorrectly, thus, lowering the potential benefits of an extended pattern set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04063</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04063</id><created>2015-11-12</created><authors><author><keyname>Loellmann</keyname><forenames>Heinrich</forenames></author><author><keyname>Brendel</keyname><forenames>Andreas</forenames></author><author><keyname>Vary</keyname><forenames>Peter</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author></authors><title>Single-Channel Maximum-Likelihood T60 Estimation Exploiting Subband
  Information</title><categories>cs.SD</categories><comments>In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</comments><report-no>ACEChallenge/2015/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution presents four algorithms developed by the authors for
single-channel fullband and subband T60 estimation within the ACE challenge.
The blind estimation of the fullband reverberation time (RT) by
maximum-likelihood (ML) estimation based on [15] is considered as baseline
approach. An improvement of this algorithm is devised where an energy-weighted
averaging of the upper subband RT estimates is performed using either a DCT or
1/3-octave filter-bank. The evaluation results show that this approach leads to
a lower variance for the estimation error in comparison to the baseline
approach at the price of an increased computational complexity. Moreover, a new
algorithm to estimate the subband RT is devised, where the RT estimates for the
lower octave subbands are extrapolated from the RT estimates of the upper
subbands by means of a simple model for the frequency-dependency of the subband
RT. The evaluation results of the ACE challenge reveal that this approach
allows to estimate the subband RT with an estimation error which is in a
similar range as for the presented fullband RT estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04066</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04066</id><created>2015-11-12</created><authors><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Kane</keyname><forenames>Daniel M.</forenames></author><author><keyname>Stewart</keyname><forenames>Alistair</forenames></author></authors><title>Properly Learning Poisson Binomial Distributions in Almost Polynomial
  Time</title><categories>cs.DS cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm for properly learning Poisson binomial distributions. A
Poisson binomial distribution (PBD) of order $n$ is the discrete probability
distribution of the sum of $n$ mutually independent Bernoulli random variables.
Given $\widetilde{O}(1/\epsilon^2)$ samples from an unknown PBD $\mathbf{p}$,
our algorithm runs in time $(1/\epsilon)^{O(\log \log (1/\epsilon))}$, and
outputs a hypothesis PBD that is $\epsilon$-close to $\mathbf{p}$ in total
variation distance. The previously best known running time for properly
learning PBDs was $(1/\epsilon)^{O(\log(1/\epsilon))}$.
  As one of our main contributions, we provide a novel structural
characterization of PBDs. We prove that, for all $\epsilon &gt;0,$ there exists an
explicit collection $\cal{M}$ of $(1/\epsilon)^{O(\log \log (1/\epsilon))}$
vectors of multiplicities, such that for any PBD $\mathbf{p}$ there exists a
PBD $\mathbf{q}$ with $O(\log(1/\epsilon))$ distinct parameters whose
multiplicities are given by some element of ${\cal M}$, such that $\mathbf{q}$
is $\epsilon$-close to $\mathbf{p}$. Our proof combines tools from Fourier
analysis and algebraic geometry.
  Our approach to the proper learning problem is as follows: Starting with an
accurate non-proper hypothesis, we fit a PBD to this hypothesis. More
specifically, we essentially start with the hypothesis computed by the
computationally efficient non-proper learning algorithm in our recent
work~\cite{DKS15}. Our aforementioned structural characterization allows us to
reduce the corresponding fitting problem to a collection of
$(1/\epsilon)^{O(\log \log(1/\epsilon))}$ systems of low-degree polynomial
inequalities. We show that each such system can be solved in time
$(1/\epsilon)^{O(\log \log(1/\epsilon))}$, which yields the overall running
time of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04067</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04067</id><created>2015-11-12</created><authors><author><keyname>Vemulapalli</keyname><forenames>Raviteja</forenames></author><author><keyname>Tuzel</keyname><forenames>Oncel</forenames></author><author><keyname>Liu</keyname><forenames>Ming-Yu</forenames></author></authors><title>Deep Gaussian Conditional Random Field Network: A Model-based Deep
  Network for Discriminative Denoising</title><categories>cs.CV</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel deep network architecture for image\\ denoising based on a
Gaussian Conditional Random Field (GCRF) model. In contrast to the existing
discriminative denoising methods that train a separate model for each noise
level, the proposed deep network explicitly models the input noise variance and
hence is capable of handling a range of noise levels. Our deep network, which
we refer to as deep GCRF network, consists of two sub-networks: (i) a parameter
generation network that generates the pairwise potential parameters based on
the noisy input image, and (ii) an inference network whose layers perform the
computations involved in an iterative GCRF inference procedure.\ We train the
entire deep GCRF network (both parameter generation and inference networks)
discriminatively in an end-to-end fashion by maximizing the peak
signal-to-noise ratio measure. Experiments on Berkeley segmentation and
PASCALVOC datasets show that the proposed deep GCRF network outperforms
state-of-the-art image denoising approaches for several noise levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04094</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04094</id><created>2015-11-12</created><authors><author><keyname>Taneja</keyname><forenames>Harsh</forenames></author><author><keyname>Webster</keyname><forenames>James</forenames></author></authors><title>How Do Global Audiences Take Shape? The Role of Institutions and Culture
  in Patterns of Web Use</title><categories>cs.CY cs.SI</categories><comments>Suggested Citation: Taneja. H &amp; Webster. J.G. (In Press). How Do
  Global Audiences Take Shape? The Role of Institutions and Culture in Shaping
  Patterns of Web Use. Journal of Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates the role of both cultural and technological factors
in determining audience formation on a global scale. It integrates theories of
media choice with theories of global cultural consumption and tests them by
analyzing shared audience traffic between the world's 1000 most popular
Websites. We find that language and geographic similarities are more powerful
predictors of audience overlap than hyperlinks and genre similarity,
highlighting the role of cultural structures in shaping global media use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04103</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04103</id><created>2015-11-12</created><updated>2016-01-07</updated><authors><author><keyname>Wang</keyname><forenames>Panqu</forenames></author><author><keyname>Cottrell</keyname><forenames>Garrison W.</forenames></author></authors><title>Basic Level Categorization Facilitates Visual Object Recognition</title><categories>cs.CV</categories><comments>ICLR 2016 submission R1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in deep learning have led to significant progress in the
computer vision field, especially for visual object recognition tasks. The
features useful for object classification are learned by feed-forward deep
convolutional neural networks (CNNs) automatically, and they are shown to be
able to predict and decode neural representations in the ventral visual pathway
of humans and monkeys. However, despite the huge amount of work on optimizing
CNNs, there has not been much research focused on linking CNNs with guiding
principles from the human visual cortex. In this work, we propose a network
optimization strategy inspired by both of the developmental trajectory of
children's visual object recognition capabilities, and Bar (2003), who
hypothesized that basic level information is carried in the fast magnocellular
pathway through the prefrontal cortex (PFC) and then projected back to inferior
temporal cortex (IT), where subordinate level categorization is achieved. We
instantiate this idea by training a deep CNN to perform basic level object
categorization first, and then train it on subordinate level categorization. We
apply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC
2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%,
demonstrating the effectiveness of the method. We also show that subsequent
transfer learning on smaller datasets gives superior results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04108</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04108</id><created>2015-11-12</created><updated>2016-01-07</updated><authors><author><keyname>Tan</keyname><forenames>Ming</forenames></author><author><keyname>Santos</keyname><forenames>Cicero dos</forenames></author><author><keyname>Xiang</keyname><forenames>Bing</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>LSTM-based Deep Learning Models for Non-factoid Answer Selection</title><categories>cs.CL cs.LG</categories><comments>added new experiments on TREC-QA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply a general deep learning (DL) framework for the answer
selection task, which does not depend on manually defined features or
linguistic tools. The basic framework is to build the embeddings of questions
and answers based on bidirectional long short-term memory (biLSTM) models, and
measure their closeness by cosine similarity. We further extend this basic
model in two directions. One direction is to define a more composite
representation for questions and answers by combining convolutional neural
network with the basic framework. The other direction is to utilize a simple
but efficient attention mechanism in order to generate the answer
representation according to the question context. Several variations of models
are provided. The models are examined by two datasets, including TREC-QA and
InsuranceQA. Experimental results demonstrate that the proposed models
substantially outperform several strong baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04109</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04109</id><created>2015-11-12</created><authors><author><keyname>Abdelltif</keyname><forenames>Tamer Mohamed</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author></authors><title>Software Analytics to Software Domains: A Systematic Literature Review</title><categories>cs.SE</categories><comments>pp. 30-36</comments><journal-ref>37th IEEE International Conference on Software Engineering -
  Workshop on BIGDSE, 2015</journal-ref><doi>10.1109/BIGDSE.2015.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Analytics (SA) is a new branch of big data analytics that has
recently emerged (2011). What distinguishes SA from direct software analysis is
that it links data mined from many different software artifacts to obtain
valuable insights. These insights are useful for the decision-making process
throughout the different phases of the software lifecycle. Since SA is
currently a hot and promising topic, we have conducted a systematic literature
review, presented in this paper, to identify gaps in knowledge and open
research areas in SA. Because many researchers are still confused about the
true potential of SA, we had to filter out available research papers to obtain
the most SA-relevant work for our review. This filtration yielded 19 studies
out of 135. We have based our systematic review on four main factors: which
software practitioners SA targets, which domains are covered by SA, which
artifacts are extracted by SA, and whether these artifacts are linked or not.
The results of our review have shown that much of the available SA research
only serves the needs of developers. Also, much of the available research uses
only one artifact which, in turn, means fewer links between artifacts and fewer
insights. This shows that the available SA research work is still embryonic
leaving plenty of room for future research in the SA field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04110</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04110</id><created>2015-11-12</created><authors><author><keyname>Mollahosseini</keyname><forenames>Ali</forenames></author><author><keyname>Chan</keyname><forenames>David</forenames></author><author><keyname>Mahoor</keyname><forenames>Mohammad H.</forenames></author></authors><title>Going Deeper in Facial Expression Recognition using Deep Neural Networks</title><categories>cs.NE cs.CV</categories><comments>To be appear in IEEE Winter Conference on Applications of Computer
  Vision (WACV), 2016 {Accepted in first round submission}</comments><journal-ref>IEEE Winter Conference on Applications of Computer Vision (WACV),
  2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated Facial Expression Recognition (FER) has remained a challenging and
interesting problem. Despite efforts made in developing various methods for
FER, existing approaches traditionally lack generalizability when applied to
unseen images or those that are captured in wild setting. Most of the existing
approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where
the classifier's hyperparameters are tuned to give best recognition accuracies
across a single database, or a small collection of similar databases.
Nevertheless, the results are not significant when they are applied to novel
data. This paper proposes a deep neural network architecture to address the FER
problem across multiple well-known standard face datasets. Specifically, our
network consists of two convolutional layers each followed by max pooling and
then four Inception layers. The network is a single component architecture that
takes registered facial images as the input and classifies them into either of
the six basic or the neutral expressions. We conducted comprehensive
experiments on seven publically available facial expression databases, viz.
MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed
architecture are comparable to or better than the state-of-the-art methods and
better than traditional convolutional neural networks and in both accuracy and
training time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04115</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04115</id><created>2015-11-12</created><authors><author><keyname>Mahmoodi</keyname><forenames>S. Eman</forenames></author><author><keyname>Subbalakshmi</keyname><forenames>K. P.</forenames></author><author><keyname>Chandramouli</keyname><forenames>R.</forenames></author><author><keyname>Abolhassani</keyname><forenames>Bahman</forenames></author></authors><title>Joint Spectrum Sensing and Resource Allocation for OFDM-based
  Transmission with a Cognitive Relay</title><categories>cs.IT math.IT</categories><comments>EAI Endorsed Transactions on Wireless Spectrum 14(1): e4 Published
  13th Apr 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the joint spectrum sensing and resource
allocation problem to maximize throughput capacity of an OFDM-based cognitive
radio link with a cognitive relay. By applying a cognitive relay that uses
decode and forward (D&amp;F), we achieve more reliable communications, generating
less interference (by needing less transmit power) and more diversity gain. In
order to account for imperfections in spectrum sensing, the proposed schemes
jointly modify energy detector thresholds and allocates transmit powers to all
cognitive radio (CR) subcarriers, while simultaneously assigning subcarrier
pairs for secondary users (SU) and the cognitive relay. This problem is cast as
a constrained optimization problem with constraints on (1) interference
introduced by the SU and the cognitive relay to the PUs; (2) miss-detection and
false alarm probabilities and (3) subcarrier pairing for transmission on the SU
transmitter and the cognitive relay and (4) minimum Quality of Service (QoS)
for each CR subcarrier. We propose one optimal and two sub-optimal schemes all
of which are compared to other schemes in the literature. Simulation results
show that the proposed schemes achieve significantly higher throughput than
other schemes in the literature for different relay situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04119</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04119</id><created>2015-11-12</created><updated>2016-02-14</updated><authors><author><keyname>Sharma</keyname><forenames>Shikhar</forenames></author><author><keyname>Kiros</keyname><forenames>Ryan</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Action Recognition using Visual Attention</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a soft attention based model for the task of action recognition in
videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long
Short-Term Memory (LSTM) units which are deep both spatially and temporally.
Our model learns to focus selectively on parts of the video frames and
classifies videos after taking a few glimpses. The model essentially learns
which parts in the frames are relevant for the task at hand and attaches higher
importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51
and Hollywood2 datasets and analyze how the model focuses its attention
depending on the scene and the action being performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04120</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04120</id><created>2015-11-11</created><authors><author><keyname>Weber</keyname><forenames>Barbara</forenames></author><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author><author><keyname>Torres</keyname><forenames>Victoria</forenames></author><author><keyname>Reichert</keyname><forenames>Manfred</forenames></author></authors><title>Change Patterns for Model Creation: Investigating the Role of Nesting
  Depth</title><categories>cs.SE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1511.04059</comments><journal-ref>Proc. Cognise'13, pp. 198-204, 2013</journal-ref><doi>10.1007/978-3-642-38490-5_19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process model quality has been an area of considerable research efforts. In
this context, the correctness-by-construction principle of change patterns
offers a promising perspective. However, using change patterns for model
creation imposes a more structured way of modeling. While the process of
process modeling (PPM) based on change primitives has been investigated, little
is known about this process based on change patterns and factors that impact
the cognitive complexity of pattern usage. Insights from the field of cognitive
psychology as well as observations from a pilot study suggest that the nesting
depth of the model to be created has a significant impact on cognitive
complexity. This paper proposes a research design to test the impact of nesting
depth on the cognitive complexity of change pattern usage in an experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04121</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04121</id><created>2015-11-11</created><authors><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author><author><keyname>Furtner</keyname><forenames>Marco</forenames></author><author><keyname>Martini</keyname><forenames>Markus</forenames></author><author><keyname>Sachse</keyname><forenames>Pierre</forenames></author><author><keyname>Reiter</keyname><forenames>Katharina</forenames></author><author><keyname>Zugal</keyname><forenames>Stefan</forenames></author><author><keyname>Weber</keyname><forenames>Barbara</forenames></author></authors><title>Investigating the Process of Process Modeling with Eye Movement Analysis</title><categories>cs.SE</categories><comments>arXiv admin note: text overlap with arXiv:1511.04057</comments><journal-ref>Proc. ER-BPM'12, pp. 438-450, 2013</journal-ref><doi>10.1007/978-3-642-36285-9_46</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on quality issues of business process models has recently begun to
explore the process of creating process models by analyzing the modeler's
interactions with the modeling environment. In this paper we aim to complement
previous insights on the modeler's modeling behavior with data gathered by
tracking the modeler's eye movements when engaged in the act of modeling. We
present preliminary results and outline directions for future research to
triangulate toward a more comprehensive understanding of the process of process
modeling. We believe that combining different views on the process of process
modeling constitutes another building block in understanding this process that
will ultimately enable us to support modelers in creating better process
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04123</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04123</id><created>2015-11-12</created><updated>2015-12-18</updated><authors><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Cheong</keyname><forenames>Otfried</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean Lou</forenames></author><author><keyname>Dobbins</keyname><forenames>Michael Gene</forenames></author><author><keyname>Fleischer</keyname><forenames>Rudolf</forenames></author><author><keyname>Kawamura</keyname><forenames>Akitoshi</forenames></author><author><keyname>Korman</keyname><forenames>Matias</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author><author><keyname>Pach</keyname><forenames>Janos</forenames></author><author><keyname>Tang</keyname><forenames>Yuan</forenames></author><author><keyname>Tokuyama</keyname><forenames>Takeshi</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author><author><keyname>Wang</keyname><forenames>Tianhao</forenames></author></authors><title>Weight Balancing on Boundaries and Skeletons</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a polygonal region containing a target point (which we assume is the
origin), it is not hard to see that there are two points on the perimeter that
are antipodal, i.e., whose midpoint is the origin. We prove three
generalizations of this fact. (1) For any polygon (or any bounded closed region
with connected boundary) containing the origin, it is possible to place a given
set of weights on the boundary so that their barycenter (center of mass)
coincides with the origin, provided that the largest weight does not exceed the
sum of the other weights. (2) On the boundary of any $3$-dimensional bounded
polyhedron containing the origin, there exist three points that form an
equilateral triangle centered at the origin. (3) On the $1$-skeleton of any
$3$-dimensional bounded convex polyhedron containing the origin, there exist
three points whose center of mass coincides with the origin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04126</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04126</id><created>2015-11-12</created><authors><author><keyname>Brandt</keyname><forenames>Rasmus</forenames></author><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>Interference Alignment-Aided Base Station Clustering using Coalition
  Formation</title><categories>cs.IT math.IT</categories><comments>2nd Prize, Student Paper Contest. Copyright 2015 SS&amp;C. Published in
  the Proceedings of the 49th Asilomar Conference on Signals, Systems and
  Computers, Nov 8-11, 2015, Pacific Grove, CA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Base station clustering is necessary in large interference networks, where
the channel state information (CSI) acquisition overhead otherwise would be
overwhelming. In this paper, we propose a novel long-term throughput model for
the clustered users which addresses the balance between interference mitigation
capability and CSI acquisition overhead. The model only depends on statistical
CSI, thus enabling long-term clustering. Based on notions from coalitional game
theory, we propose a low-complexity distributed clustering method. The
algorithm converges in a couple of iterations, and only requires limited
communication between base stations. Numerical simulations show the viability
of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04134</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04134</id><created>2015-11-12</created><authors><author><keyname>An</keyname><forenames>Jisun</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>Whom Should We Sense in &quot;Social Sensing&quot; -- Analyzing Which Users Work
  Best for Social Media Now-Casting</title><categories>cs.SI cs.CY</categories><comments>This is a pre-print of a forthcoming EPJ Data Science paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the ever increasing amount of publicly available social media data,
there is growing interest in using online data to study and quantify phenomena
in the offline &quot;real&quot; world. As social media data can be obtained in near
real-time and at low cost, it is often used for &quot;now-casting&quot; indices such as
levels of flu activity or unemployment. The term &quot;social sensing&quot; is often used
in this context to describe the idea that users act as &quot;sensors&quot;, publicly
reporting their health status or job losses. Sensor activity during a time
period is then typically aggregated in a &quot;one tweet, one vote&quot; fashion by
simply counting. At the same time, researchers readily admit that social media
users are not a perfect representation of the actual population. Additionally,
users differ in the amount of details of their personal lives that they reveal.
Intuitively, it should be possible to improve now-casting by assigning
different weights to different user groups.
  In this paper, we ask &quot;How does social sensing actually work?&quot; or, more
precisely, &quot;Whom should we sense--and whom not--for optimal results?&quot;. We
investigate how different sampling strategies affect the performance of
now-casting of two common offline indices: flu activity and unemployment rate.
We show that now-casting can be improved by 1) applying user filtering
techniques and 2) selecting users with complete profiles. We also find that,
using the right type of user groups, now-casting performance does not degrade,
even when drastically reducing the size of the dataset. More fundamentally, we
describe which type of users contribute most to the accuracy by asking if
&quot;babblers are better&quot;. We conclude the paper by providing guidance on how to
select better user groups for more accurate now-casting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04136</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04136</id><created>2015-11-12</created><updated>2015-12-09</updated><authors><author><keyname>Wen</keyname><forenames>Longyin</forenames></author><author><keyname>Du</keyname><forenames>Dawei</forenames></author><author><keyname>Cai</keyname><forenames>Zhaowei</forenames></author><author><keyname>Lei</keyname><forenames>Zhen</forenames></author><author><keyname>Chang</keyname><forenames>Ming-Ching</forenames></author><author><keyname>Qi</keyname><forenames>Honggang</forenames></author><author><keyname>Lim</keyname><forenames>Jongwoo</forenames></author><author><keyname>Yang</keyname><forenames>Ming-Hsuan</forenames></author><author><keyname>Lyu</keyname><forenames>Siwei</forenames></author></authors><title>DETRAC: A New Benchmark and Protocol for Multi-Object Tracking</title><categories>cs.CV</categories><comments>12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, most effective multi-object tracking (MOT) methods are based
on the tracking-by-detection framework. Existing performance evaluations of MOT
methods usually separate the target association step from the object detection
step by using the same object detection results for comparisons. In this work,
we perform a comprehensive quantitative study on the effect of object detection
accuracy to the overall MOT performance. This is based on a new large-scale
DETection and tRACking (DETRAC) benchmark dataset. The DETRAC benchmark dataset
consists of 100 challenging video sequences captured from real-world traffic
scenes (over 140 thousand frames and 1.2 million labeled bounding boxes of
objects) for both object detection and MOT. We evaluate complete MOT systems
constructed from combinations of state-of-the-art target association methods
and object detection schemes. Our analysis shows the complex effects of object
detection accuracy on MOT performance. Based on these observations, we propose
new evaluation tools and metrics for MOT systems that consider both object
detection and target association for comprehensive analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04137</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04137</id><created>2015-11-12</created><updated>2015-12-01</updated><authors><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Crawford</keyname><forenames>Forrest W.</forenames></author><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author></authors><title>Seeing the Unseen Network: Inferring Hidden Social Ties from
  Respondent-Driven Sampling</title><categories>cs.SI cs.AI cs.LG</categories><comments>A full version with technical proofs. Accepted by AAAI-16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning about the social structure of hidden and hard-to-reach populations
--- such as drug users and sex workers --- is a major goal of epidemiological
and public health research on risk behaviors and disease prevention.
Respondent-driven sampling (RDS) is a peer-referral process widely used by many
health organizations, where research subjects recruit other subjects from their
social network. In such surveys, researchers observe who recruited whom, along
with the time of recruitment and the total number of acquaintances (network
degree) of respondents. However, due to privacy concerns, the identities of
acquaintances are not disclosed. In this work, we show how to reconstruct the
underlying network structure through which the subjects are recruited. We
formulate the dynamics of RDS as a continuous-time diffusion process over the
underlying graph and derive the likelihood for the recruitment time series
under an arbitrary recruitment time distribution. We develop an efficient
stochastic optimization algorithm called RENDER (REspoNdent-Driven nEtwork
Reconstruction) that finds the network that best explains the collected data.
We support our analytical results through an exhaustive set of experiments on
both synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04143</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04143</id><created>2015-11-12</created><updated>2016-02-16</updated><authors><author><keyname>Hausknecht</keyname><forenames>Matthew</forenames></author><author><keyname>Stone</keyname><forenames>Peter</forenames></author></authors><title>Deep Reinforcement Learning in Parameterized Action Space</title><categories>cs.AI cs.LG cs.MA cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown that deep neural networks are capable of approximating
both value functions and policies in reinforcement learning domains featuring
continuous state and action spaces. However, to the best of our knowledge no
previous work has succeeded at using deep neural networks in structured
(parameterized) continuous action spaces. To fill this gap, this paper focuses
on learning within the domain of simulated RoboCup soccer, which features a
small set of discrete action types, each of which is parameterized with
continuous variables. The best learned agent can score goals more reliably than
the 2012 RoboCup champion agent. As such, this paper represents a successful
extension of deep reinforcement learning to the class of parameterized action
space MDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04145</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04145</id><created>2015-11-12</created><authors><author><keyname>Farajtabar</keyname><forenames>Mehrdad</forenames></author><author><keyname>Yousefi</keyname><forenames>Safoora</forenames></author><author><keyname>Tran</keyname><forenames>Long Q.</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author></authors><title>A Continuous-time Mutually-Exciting Point Process Framework for
  Prioritizing Events in Social Media</title><categories>cs.SI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The overwhelming amount and rate of information update in online social media
is making it increasingly difficult for users to allocate their attention to
their topics of interest, thus there is a strong need for prioritizing news
feeds. The attractiveness of a post to a user depends on many complex
contextual and temporal features of the post. For instance, the contents of the
post, the responsiveness of a third user, and the age of the post may all have
impact. So far, these static and dynamic features has not been incorporated in
a unified framework to tackle the post prioritization problem. In this paper,
we propose a novel approach for prioritizing posts based on a feature modulated
multi-dimensional point process. Our model is able to simultaneously capture
textual and sentiment features, and temporal features such as self-excitation,
mutual-excitation and bursty nature of social interaction. As an evaluation, we
also curated a real-world conversational benchmark dataset crawled from
Facebook. In our experiments, we demonstrate that our algorithm is able to
achieve the-state-of-the-art performance in terms of analyzing, predicting, and
prioritizing events. In terms of interpretability of our method, we observe
that features indicating individual user profile and linguistic characteristics
of the events work best for prediction and prioritization of new events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04150</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04150</id><created>2015-11-12</created><authors><author><keyname>Oliva</keyname><forenames>Junier B.</forenames></author><author><keyname>Sutherland</keyname><forenames>Dougal J.</forenames></author><author><keyname>P&#xf3;czos</keyname><forenames>Barnab&#xe1;s</forenames></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames></author></authors><title>Deep Mean Maps</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of distributions and high-level features from deep architecture has
become commonplace in modern computer vision. Both of these methodologies have
separately achieved a great deal of success in many computer vision tasks.
However, there has been little work attempting to leverage the power of these
to methodologies jointly. To this end, this paper presents the Deep Mean Maps
(DMMs) framework, a novel family of methods to non-parametrically represent
distributions of features in convolutional neural network models.
  DMMs are able to both classify images using the distribution of top-level
features, and to tune the top-level features for performing this task. We show
how to implement DMMs using a special mean map layer composed of typical CNN
operations, making both forward and backward propagation simple.
  We illustrate the efficacy of DMMs at analyzing distributional patterns in
image data in a synthetic data experiment. We also show that we extending
existing deep architectures with DMMs improves the performance of existing CNNs
on several challenging real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04153</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04153</id><created>2015-11-12</created><authors><author><keyname>Li</keyname><forenames>Yaoyi</forenames></author><author><keyname>Chen</keyname><forenames>Junxuan</forenames></author><author><keyname>Lu</keyname><forenames>Hongtao</forenames></author></authors><title>Adaptive Affinity Matrix for Unsupervised Metric Learning</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering is one of the most popular clustering approaches with the
capability to handle some challenging clustering problems. Most spectral
clustering methods provide a nonlinear map from the data manifold to a
subspace. Only a little work focuses on the explicit linear map which can be
viewed as the unsupervised distance metric learning. In practice, the selection
of the affinity matrix exhibits a tremendous impact on the unsupervised
learning. While much success of affinity learning has been achieved in recent
years, some issues such as noise reduction remain to be addressed. In this
paper, we propose a novel method, dubbed Adaptive Affinity Matrix (AdaAM), to
learn an adaptive affinity matrix and derive a distance metric from the
affinity. We assume the affinity matrix to be positive semidefinite with
ability to quantify the pairwise dissimilarity. Our method is based on posing
the optimization of objective function as a spectral decomposition problem. We
yield the affinity from both the original data distribution and the widely-used
heat kernel. The provided matrix can be regarded as the optimal representation
of pairwise relationship on the manifold. Extensive experiments on a number of
real-world data sets show the effectiveness and efficiency of AdaAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04156</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04156</id><created>2015-11-12</created><authors><author><keyname>Merel</keyname><forenames>Josh</forenames></author><author><keyname>Carlson</keyname><forenames>David</forenames></author><author><keyname>Paninski</keyname><forenames>Liam</forenames></author><author><keyname>Cunningham</keyname><forenames>John P.</forenames></author></authors><title>Neuroprosthetic decoder training as imitation learning</title><categories>stat.ML cs.LG q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuroprosthetic brain-computer interfaces function via an algorithm which
decodes neural activity of the user into movements of an end effector, such as
a cursor or robotic arm. In practice, the decoder is often learned by updating
its parameters while the user performs a task. When the user's intention is not
directly observable, recent methods have demonstrated value in training the
decoder against a surrogate for the user's intended movement. We describe how
training a decoder in this way can be seen as an imitation learning problem,
where an oracle or expert is employed for supervised training in lieu of direct
observations, which are not available. Specifically, we adapt a generic
imitation learning meta-algorithm, dataset aggregation (DAgger, [1]), to train
a brain-computer interface in arbitrary dimensions. By interpreting existing
learning algorithms for brain-computer interfaces in this framework, we provide
analysis of regret, a standard metric of learning efficacy. Using this
interpretation, we characterize the space of algorithmic variants and bounds on
their regret rates. Existing approaches for decoder learning have been
performed in the cursor control setting, but it has not been clear how these
approaches will scale to naturalistic complexity settings, such as controlling
a high degree-of-freedom robotic arm. Leveraging our proposed imitation
learning interpretation, we couple parameter updating with an oracle produced
by optimal control software, yielding rapid decoder learning in simulations of
neuroprosthetic control of an arm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04158</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04158</id><created>2015-11-12</created><authors><author><keyname>Bagaria</keyname><forenames>Sankalp</forenames></author></authors><title>Aadhaar-Based Unified Payment Solution</title><categories>cs.CR cs.CY</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to build an Aadhaar Based Unified Payment Solution.
The key idea is that a virtual wallet will be linked to the Aadhaar card number
of the customer. After that, any identification unique to the person and linked
with the Aadhaar card, be it something the person knows like secret
Internet-banking password, be it something s/he carries like debit card/ credit
card, something s/he owns like fingerprints, voice, email-id or somewhere s/he
is like house or office address, can be used for money transfer from the
sender's Aadhaar card linked virtual wallet to the receiver's Aadhaar card
linked virtual wallet, whose any unique ID is known to the sender. If the
sender knows the receiver's email-id, s/he can transfer money to his/ her
Aadhaar card linked virtual wallet using the email-id. And, if the sender knows
receiver's mobile number but not email-id, s/he can use the mobile number to
transfer the money to his/ her Aadhaar card linked virtual wallet. And so on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04160</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04160</id><created>2015-11-12</created><authors><author><keyname>Majmudar</keyname><forenames>Jimit</forenames></author><author><keyname>Krone</keyname><forenames>Stephen M.</forenames></author><author><keyname>Baumgaertner</keyname><forenames>Bert O.</forenames></author><author><keyname>Tyson</keyname><forenames>Rebecca C.</forenames></author></authors><title>The Voter Model and Jump Diffusion</title><categories>physics.soc-ph cs.SI math.PR</categories><comments>13 pages, 5 figures, preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opinions, and subsequently opinion dynamics, depend not just on interactions
among individuals, but also on external influences such as the mass media. The
dependence on local interactions, however, has received considerably more
attention. In this paper, we use the classical voter model as a basis, and
extend it to include external influences. We show that this new model can be
understood using the theory of jump diffusion processes. We derive results
pertaining to fixation probability and expected consensus time of the process,
and find that the contribution of an external influence significantly dwarfs
the contribution of the node-to-node interactions in terms of driving the
social network to eventual consensus. This result suggests the potential
importance of ``macro-level'' phenomena such as the media influence as compared
to the ``micro-level'' local interactions, in modelling opinion dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04164</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04164</id><created>2015-11-13</created><authors><author><keyname>Hu</keyname><forenames>Ronghang</forenames></author><author><keyname>Xu</keyname><forenames>Huazhe</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Feng</keyname><forenames>Jiashi</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Natural Language Object Retrieval</title><categories>cs.CV cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the task of natural language object retrieval, to
localize a target object within a given image based on a natural language query
of the object. Natural language object retrieval differs from text-based image
retrieval task as it involves spatial information about objects within the
scene and global scene context. To address this issue, we propose a novel
Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate
boxes for object retrieval, integrating spatial configurations and global
scene-level contextual information into the network. Our model processes query
text, local image descriptors, spatial configurations and global context
features through a recurrent network, outputs the probability of the query text
conditioned on each candidate box as a score for the box, and can transfer
visual-linguistic knowledge from image captioning domain to our task.
Experimental results demonstrate that our method effectively utilizes both
local and global information, outperforming previous baseline methods
significantly on different datasets and scenarios, and can exploit large scale
vision and language datasets for knowledge transfer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04166</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04166</id><created>2015-11-13</created><authors><author><keyname>Li</keyname><forenames>Yin</forenames></author><author><keyname>Paluri</keyname><forenames>Manohar</forenames></author><author><keyname>Rehg</keyname><forenames>James M.</forenames></author><author><keyname>Doll&#xe1;r</keyname><forenames>Piotr</forenames></author></authors><title>Unsupervised Learning of Edges</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-driven approaches for edge detection have proven effective and achieve
top results on modern benchmarks. However, all current data-driven edge
detectors require manual supervision for training in the form of hand-labeled
region segments or object boundaries. Specifically, human annotators mark
semantically meaningful edges which are subsequently used for training. Is this
form of strong, high-level supervision actually necessary to learn to
accurately detect edges? In this work we present a simple yet effective
approach for training edge detectors without human supervision. To this end we
utilize motion, and more specifically, the only input to our method is noisy
semi-dense matches between frames. We begin with only a rudimentary knowledge
of edges (in the form of image gradients), and alternate between improving
motion estimation and edge detection in turn. Using a large corpus of video
data, we show that edge detectors trained using our unsupervised scheme
approach the performance of the same methods trained with full supervision
(within 3-5%). Finally, we show that when using a deep network for the edge
detector, our approach provides a novel pre-training scheme for object
detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04169</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04169</id><created>2015-11-13</created><authors><author><keyname>Amani</keyname><forenames>Sidney</forenames><affiliation>NICTA and University of New South Wales, Australia</affiliation></author><author><keyname>Murray</keyname><forenames>Toby</forenames><affiliation>NICTA and University of New South Wales, Australia</affiliation></author></authors><title>Specifying a Realistic File System</title><categories>cs.LO cs.OS</categories><comments>In Proceedings MARS 2015, arXiv:1511.02528</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 196, 2015, pp. 1-9</journal-ref><doi>10.4204/EPTCS.196.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the most interesting elements of the correctness specification of
BilbyFs, a performant Linux flash file system. The BilbyFs specification
supports asynchronous writes, a feature that has been overlooked by several
file system verification projects, and has been used to verify the correctness
of BilbyFs's fsync() C implementation. It makes use of nondeterminism to be
concise and is shallowly-embedded in higher-order logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04170</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04170</id><created>2015-11-13</created><authors><author><keyname>Andronick</keyname><forenames>June</forenames><affiliation>NICTA and UNSW</affiliation></author><author><keyname>Lewis</keyname><forenames>Corey</forenames><affiliation>NICTA</affiliation></author><author><keyname>Morgan</keyname><forenames>Carroll</forenames><affiliation>NICTA and UNSW</affiliation></author></authors><title>Controlled Owicki-Gries Concurrency: Reasoning about the Preemptible
  eChronos Embedded Operating System</title><categories>cs.LO cs.OS</categories><comments>In Proceedings MARS 2015, arXiv:1511.02528</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 196, 2015, pp. 10-24</journal-ref><doi>10.4204/EPTCS.196.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a controlled concurrency framework, derived from the
Owicki-Gries method, for describing a hardware interface in detail sufficient
to support the modelling and verification of small, embedded operating systems
(OS's) whose run-time responsiveness is paramount. Such real-time systems run
with interrupts mostly enabled, including during scheduling. That differs from
many other successfully modelled and verified OS's that typically reduce the
complexity of concurrency by running on uniprocessor platforms and by switching
interrupts off as much as possible. Our framework builds on the traditional
Owicki-Gries method, for its fine-grained concurrency is needed for
high-performance system code. We adapt it to support explicit concurrency
control, by providing a simple, faithful representation of the hardware
interface that allows software to control the degree of interleaving between
user code, OS code, interrupt handlers and a scheduler that controls context
switching. We then apply this framework to model the interleaving behavior of
the eChronos OS, a preemptible real-time OS for embedded micro-controllers. We
discuss the accuracy and usability of our approach when instantiated to model
the eChronos OS. Both our framework and the eChronos model are formalised in
the Isabelle/HOL theorem prover, taking advantage of the high level of
automation in modern reasoning tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04171</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04171</id><created>2015-11-13</created><authors><author><keyname>Freiberger</keyname><forenames>Felix</forenames><affiliation>Saarland University - Computer Science</affiliation></author><author><keyname>Hermanns</keyname><forenames>Holger</forenames><affiliation>Saarland University - Computer Science</affiliation></author></authors><title>On the Control of Self-Balancing Unicycles</title><categories>cs.SY</categories><comments>In Proceedings MARS 2015, arXiv:1511.02528</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 196, 2015, pp. 25-36</journal-ref><doi>10.4204/EPTCS.196.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the problem of designing a self-balancing unicycle where
pedals are used for both power generation and speed control. After developing
the principal physical aspects (in the longitudinal dimension), we describe an
abstract model in the form of a collection of hybrid automata, together with
design requirements to be met by an ideal controller. We discuss
simplifications and assumptions that make this model amenable to verification
and validation tools such as SpaceEx. To enable experimentation with different
prototypical controllers and user behaviours in concrete scenarios, we also
develop a simple simulation framework using digital time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04172</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04172</id><created>2015-11-13</created><authors><author><keyname>Cassez</keyname><forenames>Franck</forenames><affiliation>Macquarie University, Sydney, Australia</affiliation></author><author><keyname>Marug&#xe1;n</keyname><forenames>Pablo Gonz&#xe1;lez de Aledo</forenames><affiliation>University of Cantabria, Santander, Spain</affiliation></author></authors><title>Timed Automata for Modelling Caches and Pipelines</title><categories>cs.FL cs.LO</categories><comments>In Proceedings MARS 2015, arXiv:1511.02528</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 196, 2015, pp. 37-45</journal-ref><doi>10.4204/EPTCS.196.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on modelling the timing aspects of binary programs
running on architectures featuring caches and pipelines. The objective is to
obtain a timed automaton model to compute tight bounds for the worst-case
execution time (WCET) of the programs using model-checking techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04173</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04173</id><created>2015-11-13</created><authors><author><keyname>Chaudhary</keyname><forenames>Kaylash</forenames><affiliation>University of the South Pacific</affiliation></author><author><keyname>Fehnker</keyname><forenames>Ansgar</forenames><affiliation>University of the South Pacific</affiliation></author><author><keyname>van de Pol</keyname><forenames>Jaco</forenames><affiliation>University of Twente</affiliation></author><author><keyname>Stoelinga</keyname><forenames>Marielle</forenames><affiliation>University of Twente</affiliation></author></authors><title>Modeling and Verification of the Bitcoin Protocol</title><categories>cs.LO cs.CR</categories><comments>In Proceedings MARS 2015, arXiv:1511.02528</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 196, 2015, pp. 46-60</journal-ref><doi>10.4204/EPTCS.196.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bitcoin is a popular digital currency for online payments, realized as a
decentralized peer-to-peer electronic cash system. Bitcoin keeps a ledger of
all transactions; the majority of the participants decides on the correct
ledger. Since there is no trusted third party to guard against double spending,
and inspired by its popularity, we would like to investigate the correctness of
the Bitcoin protocol. Double spending is an important threat to electronic
payment systems. Double spending would happen if one user could force a
majority to believe that a ledger without his previous payment is the correct
one. We are interested in the probability of success of such a double spending
attack, which is linked to the computational power of the attacker. This paper
examines the Bitcoin protocol and provides its formalization as an UPPAAL
model. The model will be used to show how double spending can be done if the
parties in the Bitcoin protocol behave maliciously, and with what probability
double spending occurs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04174</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04174</id><created>2015-11-13</created><authors><author><keyname>Serwe</keyname><forenames>Wendelin</forenames><affiliation>Inria</affiliation></author></authors><title>Formal Specification and Verification of Fully Asynchronous
  Implementations of the Data Encryption Standard</title><categories>cs.LO cs.CR</categories><comments>In Proceedings MARS 2015, arXiv:1511.02528</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 196, 2015, pp. 61-147</journal-ref><doi>10.4204/EPTCS.196.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents two formal models of the Data Encryption Standard (DES),
a first using the international standard LOTOS, and a second using the more
recent process calculus LNT. Both models encode the DES in the style of
asynchronous circuits, i.e., the data-flow blocks of the DES algorithm are
represented by processes communicating via rendezvous. To ensure correctness of
the models, several techniques have been applied, including model checking,
equivalence checking, and comparing the results produced by a prototype
automatically generated from the formal model with those of existing
implementations of the DES. The complete code of the models is provided as
appendices and also available on the website of the CADP verification toolbox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04176</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04176</id><created>2015-11-13</created><updated>2015-12-27</updated><authors><author><keyname>Sahu</keyname><forenames>Devendra Kumar</forenames></author><author><keyname>Sukhwani</keyname><forenames>Mohak</forenames></author></authors><title>Sequence to Sequence Learning for Optical Character Recognition</title><categories>cs.CV</categories><comments>9 pages (including reference), 6 figures (including subfigures), 5
  tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an end-to-end recurrent encoder-decoder based sequence learning
approach for printed text Optical Character Recognition (OCR). In contrast to
present day existing state-of-art OCR solution which uses connectionist
temporal classification (CTC) output layer, our approach makes minimalistic
assumptions on the structure and length of the sequence. We use a two step
encoder-decoder approach -- (a) A recurrent encoder reads a variable length
printed text word image and encodes it to a fixed dimensional embedding. (b)
This fixed dimensional embedding is subsequently comprehended by decoder
structure which converts it into a variable length text output. Our
architecture gives competitive performance relative to connectionist temporal
classification (CTC) output layer while being executed in more natural
settings. The learnt deep word image embedding from encoder can be used for
printed text based retrieval systems. The expressive fixed dimensional
embedding for any variable length input expedites the task of retrieval and
makes it more efficient which is not possible with other recurrent neural
network architectures. We empirically investigate the expressiveness and the
learnability of long short term memory (LSTMs) in the sequence to sequence
learning regime by training our network for prediction tasks in segmentation
free printed text OCR. The utility of the proposed architecture for printed
text is demonstrated by quantitative and qualitative evaluation of two tasks --
word prediction and retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04177</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04177</id><created>2015-11-13</created><authors><author><keyname>Nigam</keyname><forenames>Vivek</forenames></author><author><keyname>Reis</keyname><forenames>Giselle</forenames></author><author><keyname>Lima</keyname><forenames>Leonardo</forenames></author></authors><title>Towards the Automated Generation of Focused Proof Systems</title><categories>cs.LO</categories><comments>In Proceedings WoF'15, arXiv:1511.02529</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 197, 2015, pp. 1-6</journal-ref><doi>10.4204/EPTCS.197.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper tackles the problem of formulating and proving the completeness of
focused-like proof systems in an automated fashion. Focusing is a discipline on
proofs which structures them into phases in order to reduce proof search
non-determinism. We demonstrate that it is possible to construct a complete
focused proof system from a given un-focused proof system if it satisfies some
conditions. Our key idea is to generalize the completeness proof based on
permutation lemmas given by Miller and Saurin for the focused linear logic
proof system. This is done by building a graph from the rule permutation
relation of a proof system, called permutation graph. We then show that from
the permutation graph of a given proof system, it is possible to construct a
complete focused proof system, and additionally infer for which formulas
contraction is admissible. An implementation for building the permutation graph
of a system is provided. We apply our technique to generate the focused proof
systems MALLF, LJF and LKF for linear, intuitionistic and classical logics,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04178</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04178</id><created>2015-11-13</created><authors><author><keyname>Blanco</keyname><forenames>Roberto</forenames><affiliation>Inria and LIX</affiliation></author><author><keyname>Miller</keyname><forenames>Dale</forenames><affiliation>Inria and LIX</affiliation></author></authors><title>Proof Outlines as Proof Certificates: A System Description</title><categories>cs.LO</categories><comments>In Proceedings WoF'15, arXiv:1511.02529</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 197, 2015, pp. 7-14</journal-ref><doi>10.4204/EPTCS.197.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the foundational proof certificate (FPC) framework to the problem of
designing high-level outlines of proofs. The FPC framework provides a means to
formally define and check a wide range of proof evidence. A focused proof
system is central to this framework and such a proof system provides an
interesting approach to proof reconstruction during the process of proof
checking (relying on an underlying logic programming implementation). Here, we
illustrate how the FPC framework can be used to design proof outlines and then
to exploit proof checkers as a means for expanding outlines into fully detailed
proofs. In order to validate this approach to proof outlines, we have built the
ACheck system that allows us to take a sequence of theorems and apply the proof
outline &quot;do the obvious induction and close the proof using previously proved
lemmas&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04179</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04179</id><created>2015-11-13</created><authors><author><keyname>Graham-Lengrand</keyname><forenames>St&#xe9;phane</forenames><affiliation>CNRS - Ecole Polytechnique - INRIA - SRI International</affiliation></author></authors><title>Realisability semantics of abstract focussing, formalised</title><categories>cs.LO</categories><comments>In Proceedings WoF'15, arXiv:1511.02529</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 197, 2015, pp. 15-28</journal-ref><doi>10.4204/EPTCS.197.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a sequent calculus for abstract focussing, equipped with
proof-terms: in the tradition of Zeilberger's work, logical connectives and
their introduction rules are left as a parameter of the system, which collapses
the synchronous and asynchronous phases of focussing as macro rules. We go
further by leaving as a parameter the operation that extends a context of
hypotheses with new ones, which allows us to capture both classical and
intuitionistic focussed sequent calculi. We then define the realisability
semantics of (the proofs of) the system, on the basis of Munch-Maccagnoni's
orthogonality models for the classical focussed sequent calculus, but now
operating at the higher level of abstraction mentioned above. We prove, at that
level, the Adequacy Lemma, namely that if a term is of type A, then in the
model its denotation is in the (set-theoretic) interpretation of A. This
exhibits the fact that the universal quantification involved when taking the
orthogonal of a set, reflects in the semantics Zeilberger's universal
quantification in the macro rule for the asynchronous phase. The system and its
semantics are all formalised in Coq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04180</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04180</id><created>2015-11-13</created><authors><author><keyname>Morrill</keyname><forenames>Glyn</forenames><affiliation>Universitat Polit&#xe8;cnica de Catalunya</affiliation></author><author><keyname>Valent&#xed;n</keyname><forenames>Oriol</forenames><affiliation>Universitat Polit&#xe8;cnica de Catalunya</affiliation></author></authors><title>Multiplicative-Additive Focusing for Parsing as Deduction</title><categories>cs.LO</categories><comments>In Proceedings WoF'15, arXiv:1511.02529</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 197, 2015, pp. 29-54</journal-ref><doi>10.4204/EPTCS.197.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spurious ambiguity is the phenomenon whereby distinct derivations in grammar
may assign the same structural reading, resulting in redundancy in the parse
search space and inefficiency in parsing. Understanding the problem depends on
identifying the essential mathematical structure of derivations. This is
trivial in the case of context free grammar, where the parse structures are
ordered trees; in the case of categorial grammar, the parse structures are
proof nets. However, with respect to multiplicatives intrinsic proof nets have
not yet been given for displacement calculus, and proof nets for additives,
which have applications to polymorphism, are involved. Here we approach
multiplicative-additive spurious ambiguity by means of the proof-theoretic
technique of focalisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04190</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04190</id><created>2015-11-13</created><authors><author><keyname>Dey</keyname><forenames>Palash</forenames></author><author><keyname>Misra</keyname><forenames>Neeldhara</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>On Choosing Committees Based on Approval Votes in the Presence of
  Outliers</title><categories>cs.MA cs.AI cs.CY cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of committee selection problem for
several approval-based voting rules in the presence of outliers. Our first
result shows that outlier consideration makes committee selection problem
intractable for approval, net approval, and minisum approval voting rules. We
then study parameterized complexity of this problem with five natural
parameters, namely the target score, the size of the committee (and its dual
parameter, the number of candidates outside the committee), the number of
outliers (and its dual parameter, the number of non-outliers). For net approval
and minisum approval voting rules, we provide a dichotomous result, resolving
the parameterized complexity of this problem for all subsets of five natural
parameters considered (by showing either FPT or W[1]-hardness for all subsets
of parameters). For the approval voting rule, we resolve the parameterized
complexity of this problem for all subsets of parameters except one.
  We also study approximation algorithms for this problem. We show that there
does not exist any alpha(.) factor approximation algorithm for approval and net
approval voting rules, for any computable function alpha(.), unless P=NP. For
the minisum voting rule, we provide a pseudopolynomial (1+eps) factor
approximation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04191</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04191</id><created>2015-11-13</created><authors><author><keyname>Etesami</keyname><forenames>Omid</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>Maximal Rank Correlation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the notion of maximal correlation, we introduce a new measure of
correlation between two different rankings of the same group of items. Our
measure captures various types of correlation detected in previous measures of
rank correlation like the Spearman correlation and the Kendall tau correlation.
We show that the maximal rank correlation satisfies the data processing and
tensorization properties (that make ordinary maximal correlation applicable to
problems in information theory). Furthermore, MRC is shown to be intimately
related to the FKG inequality. Finally, we pose the problem of the complexity
of the computation of this new measure. We make partial progress by giving a
simple but exponential-time algorithm for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04192</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04192</id><created>2015-11-13</created><updated>2015-12-10</updated><authors><author><keyname>Chen</keyname><forenames>Tianshui</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Liu</keyname><forenames>Lingbo</forenames></author><author><keyname>Luo</keyname><forenames>Xiaonan</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>DISC: Deep Image Saliency Computing via Progressive Representation
  Learning</title><categories>cs.CV</categories><comments>This manuscript is the accepted version for IEEE Transactions on
  Neural Networks and Learning Systems (T-NNLS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Salient object detection increasingly receives attention as an important
component or step in several pattern recognition and image processing tasks.
Although a variety of powerful saliency models have been intensively proposed,
they usually involve heavy feature (or model) engineering based on priors (or
assumptions) about the properties of objects and backgrounds. Inspired by the
effectiveness of recently developed feature learning, we provide a novel Deep
Image Saliency Computing (DISC) framework for fine-grained image saliency
computing. In particular, we model the image saliency from both the coarse- and
fine-level observations, and utilize the deep convolutional neural network
(CNN) to learn the saliency representation in a progressive manner.
Specifically, our saliency model is built upon two stacked CNNs. The first CNN
generates a coarse-level saliency map by taking the overall image as the input,
roughly identifying saliency regions in the global context. Furthermore, we
integrate superpixel-based local context information in the first CNN to refine
the coarse-level saliency map. Guided by the coarse saliency map, the second
CNN focuses on the local context to produce fine-grained and accurate saliency
map while preserving object details. For a testing image, the two CNNs
collaboratively conduct the saliency computing in one shot. Our DISC framework
is capable of uniformly highlighting the objects-of-interest from complex
background while preserving well object details. Extensive experiments on
several standard benchmarks suggest that DISC outperforms other
state-of-the-art methods and it also generalizes well across datasets without
additional training. The executable version of DISC is available online:
http://vision.sysu.edu.cn/projects/DISC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04196</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04196</id><created>2015-11-13</created><authors><author><keyname>Deng</keyname><forenames>Zhiwei</forenames></author><author><keyname>Vahdat</keyname><forenames>Arash</forenames></author><author><keyname>Hu</keyname><forenames>Hexiang</forenames></author><author><keyname>Mori</keyname><forenames>Greg</forenames></author></authors><title>Structure Inference Machines: Recurrent Neural Networks for Analyzing
  Relations in Group Activity Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rich semantic relations are important in a variety of visual recognition
problems. As a concrete example, group activity recognition involves the
interactions and relative spatial relations of a set of people in a scene.
State of the art recognition methods center on deep learning approaches for
training highly effective, complex classifiers for interpreting images.
However, bridging the relatively low-level concepts output by these methods to
interpret higher-level compositional scenes remains a challenge. Graphical
models are a standard tool for this task. In this paper, we propose a method to
integrate graphical models and deep neural networks into a joint framework.
Instead of using a traditional inference method, we instead use a sequential
prediction approximation, modeled by a recurrent neural network. Beyond this,
the appropriate structure for inference can be learned by imposing gates on
edges between connections of nodes. Empirical results on group activity
recognition demonstrate the potential of this model to handle highly structured
learning tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04197</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04197</id><created>2015-11-13</created><authors><author><keyname>Kapetanakis</keyname><forenames>Kostas</forenames></author><author><keyname>Andrioti</keyname><forenames>Haroula</forenames></author><author><keyname>Vonorta</keyname><forenames>Helen</forenames></author><author><keyname>Zotos</keyname><forenames>Marios</forenames></author><author><keyname>Tsigkos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Pachoulakis</keyname><forenames>Ioannis</forenames></author></authors><title>Collaboration Framework in the EViE-m Platform</title><categories>cs.CY</categories><comments>6 pages, 10 figures</comments><acm-class>H.5.1; I.3.7; I.3.8</acm-class><journal-ref>Proceedings of the 24th EAEEIE Annual Conference (EAEEIE'13),
  Chania, Greece, 30 - 31 of May, 2013 pgs 178-183</journal-ref><doi>10.1109/EAEEIE.2013.6576525</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the context of a 3D interactive strategy game, the EViE platform
allows participants to unlock game features using their knowledge and skills in
various thematic areas such as physics, mathematics, etc. By answering
questions organized by Educational Objective in stratified levels of
difficulty, users gather points which grant them access to desired world
elements. Richer world components become increasingly more difficult to access,
so that a players' individual (or cumulative if in a group) knowledge, ability
and / or dexterity is directly reflected by the level of complication of their
virtual world. In the present article we report on the communication
architecture of the platform and focus on framework components that allow group
activities such as cooperation (within the group to facilitate e.g.,
collaboration on more difficult problems), (inter-group) competition as well as
practice and skill honing activities (in single or in multi-player mode).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04203</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04203</id><created>2015-11-13</created><authors><author><keyname>Rampp</keyname><forenames>Markus</forenames><affiliation>MPCDF</affiliation></author><author><keyname>Preuss</keyname><forenames>Roland</forenames><affiliation>IPP</affiliation></author><author><keyname>Fischer</keyname><forenames>Rainer</forenames><affiliation>IPP</affiliation></author><author><keyname>Team</keyname><forenames>the ASDEX Upgrade</forenames><affiliation>IPP</affiliation></author></authors><title>GPEC, a real-time capable Tokamak equilibrium code</title><categories>physics.plasm-ph cs.CE cs.DC physics.comp-ph</categories><comments>accepted for publication in Fusion Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new parallel equilibrium reconstruction code for tokamak plasmas is
presented. GPEC allows to compute equilibrium flux distributions sufficiently
accurate to derive parameters for plasma control within 1 ms of runtime which
enables real-time applications at the ASDEX Upgrade experiment (AUG) and other
machines with a control cycle of at least this size. The underlying algorithms
are based on the well-established offline-analysis code CLISTE, following the
classical concept of iteratively solving the Grad-Shafranov equation and
feeding in diagnostic signals from the experiment. The new code adopts a hybrid
parallelization scheme for computing the equilibrium flux distribution and
extends the fast, shared-memory-parallel Poisson solver which we have described
previously by a distributed computation of the individual Poisson problems
corresponding to different basis functions. The code is based entirely on
open-source software components and runs on standard server hardware and
software environments. The real-time capability of GPEC is demonstrated by
performing an offline-computation of a sequence of 1000 flux distributions
which are taken from one second of operation of a typical AUG discharge and
deriving the relevant control parameters with a time resolution of a
millisecond. On current server hardware the new code allows employing a grid
size of 32x64 zones for the spatial discretization and up to 15 basis
functions. It takes into account about 90 diagnostic signals while using up to
4 equilibrium iterations and computing more than 20 plasma-control parameters,
including the computationally expensive safety-factor q on at least 4 different
levels of the normalized flux.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04207</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04207</id><created>2015-11-13</created><authors><author><keyname>Fiedorowicz</keyname><forenames>Anna</forenames></author><author><keyname>Sidorowicz</keyname><forenames>El&#x17c;bieta</forenames></author></authors><title>Acyclic colourings of graphs with bounded degree</title><categories>cs.DM math.CO</categories><comments>14 pages</comments><msc-class>05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $k$-colouring (not necessarily proper) of vertices of a graph is called
{\it acyclic}, if for every pair of distinct colours $i$ and $j$ the subgraph
induced by the edges whose endpoints have colours $i$ and $j$ is acyclic. In
the paper we consider some generalised acyclic $k$-colourings, namely, we
require that each colour class induces an acyclic or bounded degree graph.
Mainly we focus on graphs with maximum degree 5. We prove that any such graph
has an acyclic $5$-colouring such that each colour class induces an acyclic
graph with maximum degree at most 4. We prove that the problem of deciding
whether a graph $G$ has an acyclic 2-colouring in which each colour class
induces a graph with maximum degree at most 3 is NP-complete, even for graphs
with maximum degree 5. We also give a linear-time algorithm for an acyclic
$t$-improper colouring of any graph with maximum degree $d$ assuming that the
number of colors is large enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04210</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04210</id><created>2015-11-13</created><updated>2016-02-09</updated><authors><author><keyname>Safran</keyname><forenames>Itay</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>On the Quality of the Initial Basin in Overspecified Neural Networks</title><categories>cs.LG stat.ML</categories><comments>Significantly different version, with more general results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning, in the form of artificial neural networks, has achieved
remarkable practical success in recent years, for a variety of difficult
machine learning applications. However, a theoretical explanation for this
remains a major open problem, since training neural networks involves
optimizing a highly non-convex objective function, and is known to be
computationally hard in the worst case. In this work, we study the
\emph{geometric} structure of the associated non-convex objective function, in
the context of ReLU networks and starting from a random initialization of the
network parameters. We identify some conditions under which it becomes more
favorable to optimization, in the sense of (i) High probability of initializing
at a point from which there is a monotonically decreasing path to a global
minimum; and (ii) High probability of initializing at a basin (suitably
defined) with a small minimal objective value. A common theme in our results is
that such properties are more likely to hold for larger (&quot;overspecified&quot;)
networks, which accords with some recent empirical and theoretical
observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04211</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04211</id><created>2015-11-13</created><updated>2015-11-16</updated><authors><author><keyname>Metzen</keyname><forenames>Jan Hendrik</forenames></author></authors><title>Active Contextual Entropy Search</title><categories>stat.ML cs.LG</categories><comments>Corrected title of reference #19</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contextual policy search allows adapting robotic movement primitives to
different situations. For instance, a locomotion primitive might be adapted to
different terrain inclinations or desired walking speeds. Such an adaptation is
often achievable by modifying a small number of hyperparameters. However,
learning, when performed on real robotic systems, is typically restricted to a
small number of trials. Bayesian optimization has recently been proposed as a
sample-efficient means for contextual policy search that is well suited under
these conditions. In this work, we extend entropy search, a variant of Bayesian
optimization, such that it can be used for active contextual policy search
where the agent selects those tasks during training in which it expects to
learn the most. Empirical results in simulation suggest that this allows
learning successful behavior with less trials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04217</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04217</id><created>2015-11-13</created><authors><author><keyname>Hunold</keyname><forenames>Sascha</forenames></author></authors><title>A Survey on Reproducibility in Parallel Computing</title><categories>cs.DC</categories><comments>15 pages, 24 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We summarize the results of a survey on reproducibility in parallel
computing, which was conducted during the Euro-Par conference in August 2015.
The survey form was handed out to all participants of the conference and the
workshops. The questionnaire, which specifically targeted the parallel
computing community, contained questions in four different categories: general
questions on reproducibility, the current state of reproducibility, the
reproducibility of the participants' own papers, and questions about the
participants' familiarity with tools, software, or open-source software
licenses used for reproducible research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04224</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04224</id><created>2015-11-13</created><updated>2015-11-17</updated><authors><author><keyname>Liu</keyname><forenames>Albert J.</forenames></author><author><keyname>Marschner</keyname><forenames>Stephen R.</forenames></author><author><keyname>Dye</keyname><forenames>Victoria E.</forenames></author></authors><title>Procedural wood textures</title><categories>cs.GR</categories><comments>This version: Increased resolution of images and added YouTube link
  to video</comments><acm-class>I.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing bidirectional reflectance distribution function (BRDF) models are
capable of capturing the distinctive highlights produced by the fibrous nature
of wood. However, capturing parameter textures for even a single specimen
remains a laborious process requiring specialized equipment. In this paper we
take a procedural approach to generating parameters for the wood BSDF. We
characterize the elements of trees that are important for the appearance of
wood, discuss techniques appropriate for representing those features, and
present a complete procedural wood shader capable of reproducing the growth
patterns responsible for the distinctive appearance of highly prized
``figured'' wood. Our procedural wood shader is random-access, 3D, modular, and
is fast enough to generate a preview for design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04240</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04240</id><created>2015-11-13</created><authors><author><keyname>Campbell</keyname><forenames>Dylan</forenames></author><author><keyname>Petersson</keyname><forenames>Lars</forenames></author></authors><title>An Adaptive Data Representation for Robust Point-Set Registration and
  Merging</title><categories>cs.CV</categories><comments>Manuscript in press 2015 IEEE International Conference on Computer
  Vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a framework for rigid point-set registration and merging
using a robust continuous data representation. Our point-set representation is
constructed by training a one-class support vector machine with a Gaussian
radial basis function kernel and subsequently approximating the output function
with a Gaussian mixture model. We leverage the representation's sparse
parametrisation and robustness to noise, outliers and occlusions in an
efficient registration algorithm that minimises the L2 distance between our
support vector--parametrised Gaussian mixtures. In contrast, existing
techniques, such as Iterative Closest Point and Gaussian mixture approaches,
manifest a narrower region of convergence and are less robust to occlusions and
missing data, as demonstrated in the evaluation on a range of 2D and 3D
datasets. Finally, we present a novel algorithm, GMMerge, that parsimoniously
and equitably merges aligned mixture models, allowing the framework to be used
for reconstruction and mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04242</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04242</id><created>2015-11-13</created><authors><author><keyname>Cavallari</keyname><forenames>Tommaso</forenames></author><author><keyname>Di Stefano</keyname><forenames>Luigi</forenames></author></authors><title>Volume-based Semantic Labeling with Signed Distance Functions</title><categories>cs.CV</categories><comments>Submitted to PSIVT2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research works on the two topics of Semantic Segmentation and SLAM
(Simultaneous Localization and Mapping) have been following separate tracks.
Here, we link them quite tightly by delineating a category label fusion
technique that allows for embedding semantic information into the dense map
created by a volume-based SLAM algorithm such as KinectFusion. Accordingly, our
approach is the first to provide a semantically labeled dense reconstruction of
the environment from a stream of RGB-D images. We validate our proposal using a
publicly available semantically annotated RGB-D dataset and a) employing ground
truth labels, b) corrupting such annotations with synthetic noise, c) deploying
a state of the art semantic segmentation algorithm based on Convolutional
Neural Networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04244</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04244</id><created>2015-11-13</created><authors><author><keyname>Yue</keyname><forenames>Man-Chung</forenames></author><author><keyname>Wu</keyname><forenames>Sissi Xiaoxiao</forenames></author><author><keyname>So</keyname><forenames>Anthony Man-Cho</forenames></author></authors><title>A Robust Design for MISO Physical-Layer Multicasting over Line-of-Sight
  Channels</title><categories>cs.IT math.IT</categories><comments>This manuscript is submitted for possible journal publication on
  13-Nov-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a robust design problem for far-field line-of-sight (LOS)
channels where phase errors are present. Compared with the commonly used
additive error model, the phase error model is more suitable for capturing the
uncertainty in an LOS channel, as the dominant source of uncertainty lies in
the phase. We consider a multiple-input single-output (MISO) multicast
scenario, in which our goal is to design a beamformer that minimizes the
transmit power while satisfying probabilistic signal-to-noise ratio (SNR)
constraints. The probabilistic constraints give rise to a new computational
challenge, as they involve random trigonometric forms. In this work, we propose
to first approximate the random trigonometric form by its second-order Taylor
expansion and then tackle the resulting random quadratic form using a
Bernstein-type inequality. The advantage of such an approach is that an
approximately optimal beamformer can be obtained using the standard
semidefinite relaxation technique. In the simulations, we first show that if a
non-robust design (i.e., one that does not take phase errors into account) is
used, then the whole system may collapse. We then show that our proposed method
is less conservative than the existing robust design based on Gaussian
approximation and thus requires a lower power budget.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04245</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04245</id><created>2015-11-13</created><authors><author><keyname>Yuan</keyname><forenames>Pu</forenames></author><author><keyname>Xiao</keyname><forenames>Yong</forenames></author><author><keyname>Bi</keyname><forenames>Guoan</forenames></author><author><keyname>Zhang</keyname><forenames>Liren</forenames></author></authors><title>Towards Cooperation by Carrier Aggregation in Heterogeneous Networks: A
  Hierarchical Game Approach</title><categories>cs.NI</categories><comments>13 pages journal papaer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the resource allocation problem for a heterogeneous
network (HetNet) in which the spectrum owned by a macro-cell operator (MCO) can
be shared by both unlicensed users (UUs) and licensed users (LUs). We formulate
a novel hierarchical game theoretic framework to jointly optimize the transmit
powers and sub-band allocations of the UUs as well as the pricing strategies of
the MCO. In our framework, an overlapping coalition formation (OCF) game has
been introduced to model the cooperative behaviors of the UUs. We then
integrate this OCF game into a Stackelberg game-based hierarchical framework.
We prove that the core of our proposed OCF game is non-empty and introduce an
optimal sub-band allocation scheme for UUs. A simple distributed algorithm is
proposed for UUs to autonomously form optimal coalition formation structure.
The Stackelberg Equilibrium (SE) of the proposed hierarchical game is derived
and its uniqueness and optimality have been proved. A distributed joint
optimization algorithm is also proposed to approach the SE of the game with
limited information exchanges between the MCO and the UU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04271</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04271</id><created>2015-11-13</created><authors><author><keyname>Conradie</keyname><forenames>Willem</forenames></author><author><keyname>Palmigiano</keyname><forenames>Alessandra</forenames></author><author><keyname>Sourabh</keyname><forenames>Sumit</forenames></author><author><keyname>Zhao</keyname><forenames>Zhiguang</forenames></author></authors><title>Canonicity and Relativized Canonicity via Pseudo-Correspondence: an
  Application of ALBA</title><categories>cs.LO</categories><msc-class>03B45, 06D50, 06D10, 03G10, 06E15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize Venema's result on the canonicity of the additivity of positive
terms, from classical modal logic to a vast class of logics the algebraic
semantics of which is given by varieties of normal distributive lattice
expansions (normal DLEs), aka `distributive lattices with operators'. We
provide two contrasting proofs for this result: the first is along the lines of
Venema's pseudo-correspondence argument but using the insights and tools of
unified correspondence theory, and in particular the algorithm ALBA; the second
closer to the style of J\'onsson. Using insights gleaned from the second proof,
we define a suitable enhancement of the algorithm ALBA, which we use prove the
canonicity of certain syntactically defined classes of DLE-inequalities (called
the meta-inductive inequalities), relative to the structures in which the
formulas asserting the additivity of some given terms are valid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04273</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04273</id><created>2015-11-13</created><authors><author><keyname>Yi</keyname><forenames>Kwang Moo</forenames></author><author><keyname>Verdie</keyname><forenames>Yannick</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author><author><keyname>Lepetit</keyname><forenames>Vincent</forenames></author></authors><title>Learning to Assign Orientations to Feature Points</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to train a Convolutional Neural Network to assign a canonical
orientation to feature points given an image patch centered on the feature
point. Our method improves feature point matching upon the state-of-the art and
can be used in conjunction with any existing rotation sensitive descriptors. To
avoid the tedious and almost impossible task of finding a target orientation to
learn, we propose to use Siamese networks which implicitly find the optimal
orientations during training. We also propose a new type of activation function
for Neural Networks that generalizes the popular ReLU, maxout, and PReLU
activation functions. This novel activation performs better for our task. We
validate the effectiveness of our method extensively with four existing
datasets, including two non-planar datasets, as well as our own dataset. We
show that we outperform the state-of-the-art without the need of retraining for
each dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04275</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04275</id><created>2015-11-13</created><updated>2015-11-30</updated><authors><author><keyname>Kakushadze</keyname><forenames>Zura</forenames></author></authors><title>An Index for SSRN Downloads</title><categories>cs.DL</categories><comments>41 pages; minor misprints corrected, hyperlinks fixed; to appear in
  Journal of Informetrics</comments><journal-ref>Journal of Informetrics 10(1) (2016) 9-28</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new index to quantify SSRN downloads. Unlike the SSRN downloads
rank, which is based on the total number of an author's SSRN downloads, our
index also reflects the author's productivity by taking into account the
download numbers for the papers. Our index is inspired by - but is not the same
as - Hirsch's h-index for citations, which cannot be directly applied to SSRN
downloads. We analyze data for about 30,000 authors and 367,000 papers. We find
a simple empirical formula for the SSRN author rank via a Gaussian function of
the log of the number of downloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04278</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04278</id><created>2015-11-13</created><authors><author><keyname>Steinbring</keyname><forenames>Jannik</forenames></author><author><keyname>Mandery</keyname><forenames>Christian</forenames></author><author><keyname>Vahrenkamp</keyname><forenames>Nikolaus</forenames></author><author><keyname>Asfour</keyname><forenames>Tamim</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>High-Accuracy Real-Time Whole-Body Human Motion Tracking Based on
  Constrained Nonlinear Kalman Filtering</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new online approach to track human whole-body motion from motion
capture data, i.e., positions of labeled markers attached to the human body.
Tracking in noisy data can be effectively performed with the aid of
well-established recursive state estimation techniques. This allows us to
systematically take noise of the marker measurements into account. However, as
joint limits imposed by the human body have to be satisfied during estimation,
first we transform this constrained estimation problem into an unconstrained
one by using periodic functions. Then, we apply the Smart Sampling Kalman
Filter to solve this unconstrained estimation problem. The proposed recursive
state estimation approach makes the human motion tracking very robust to
partial occlusion of markers and avoids any special treatment or reconstruction
of the missed markers. A concrete implementation built on the kinematic human
reference model of the Master Motor Map framework and a Vicon motion capture
system is evaluated. Different captured motions show that our implementation
can accurately estimate whole-body human motion in real-time and outperforms
existing gradient-based approaches. In addition, we demonstrate its ability to
smoothly handle incomplete marker data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04285</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04285</id><created>2015-11-13</created><authors><author><keyname>Jansson</keyname><forenames>Fredrik</forenames></author><author><keyname>Hartley</keyname><forenames>Matthew</forenames></author><author><keyname>Hinsch</keyname><forenames>Martin</forenames></author><author><keyname>Slavkov</keyname><forenames>Ivica</forenames></author><author><keyname>Carranza</keyname><forenames>Noem&#xed;</forenames></author><author><keyname>Olsson</keyname><forenames>Tjelvar S. G.</forenames></author><author><keyname>Dries</keyname><forenames>Roland M.</forenames></author><author><keyname>Gr&#xf6;nqvist</keyname><forenames>Johanna H.</forenames></author><author><keyname>Mar&#xe9;e</keyname><forenames>Athanasius F. M.</forenames></author><author><keyname>Sharpe</keyname><forenames>James</forenames></author><author><keyname>Kaandorp</keyname><forenames>Jaap A.</forenames></author><author><keyname>Grieneisen</keyname><forenames>Ver&#xf4;nica A.</forenames></author></authors><title>Kilombo: a Kilobot simulator to enable effective research in swarm
  robotics</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Kilobot is a widely used platform for investigation of swarm robotics.
Physical Kilobots are slow moving and require frequent recalibration and
charging, which significantly slows down the development cycle. Simulators can
speed up the process of testing, exploring and hypothesis generation, but
usually require time consuming and error-prone translation of code between
simulator and robot. Moreover, code of different nature often obfuscates direct
comparison, as well as determination of the cause of deviation, between
simulator and actual robot swarm behaviour. To tackle these issues we have
developed a C-based simulator that allows those working with Kilobots to use
the same programme code in both the simulator and the physical robots. Use of
our simulator, coined Kilombo, significantly simplifies and speeds up
development, given that a simulation of 1000 robots can be run at a speed 100
times faster than real time on a desktop computer, making high-throughput
pre-screening possible of potential algorithms that could lead to desired
emergent behaviour. We argue that this strategy, here specifically developed
for Kilobots, is of general importance for effective robot swarm research. The
source code is freely available under the MIT license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04303</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04303</id><created>2015-11-13</created><authors><author><keyname>Fuchs</keyname><forenames>Fabian</forenames></author></authors><title>Experimental Evaluation of Distributed Node Coloring Algorithms for
  Wireless Networks</title><categories>cs.DS</categories><comments>Full version of paper accepted to ALENEX'16; 19 pages plus 12 pages
  appendix,</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we evaluate distributed node coloring algorithms for wireless
networks using the network simulator Sinalgo [by DCG@ETHZ]. All considered
algorithms operate in the realistic signal-to-interference-and-noise-ratio
(SINR) model of interference. We evaluate two recent coloring algorithms,
Rand4DColor and ColorReduction (in the following ColorRed), proposed by Fuchs
and Prutkin in [SIROCCO'15], the MW-Coloring algorithm introduced by Moscibroda
and Wattenhofer [DC'08] and transferred to the SINR model by Derbel and Talbi
[ICDCS'10], and a variant of the coloring algorithm of Yu et al. [TCS'14]. We
additionally consider several practical improvements to the algorithms and
evaluate their performance in both static and dynamic scenarios. Our
experiments show that Rand4DColor is very fast, computing a valid
(4Degree)-coloring in less than one third of the time slots required for local
broadcasting, where Degree is the maximum node degree in the network. Regarding
other O(Degree)-coloring algorithms Rand4DColor is at least 4 to 5 times
faster. Additionally, the algorithm is robust even in networks with mobile
nodes and an additional listening phase at the start of the algorithm makes
Rand4DColor robust against the late wake-up of large parts of the network.
Regarding (Degree+1)-coloring algorithms, we observe that ColorRed it is
significantly faster than the considered variant of the Yu et al. coloring
algorithm, which is the only other (Degree+1)-coloring algorithm for the SINR
model. Further improvement can be made with an error-correcting variant that
increases the runtime by allowing some uncertainty in the communication and
afterwards correcting the introduced conflicts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04306</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04306</id><created>2015-11-13</created><updated>2016-01-07</updated><authors><author><keyname>Stober</keyname><forenames>Sebastian</forenames></author><author><keyname>Sternin</keyname><forenames>Avital</forenames></author><author><keyname>Owen</keyname><forenames>Adrian M.</forenames></author><author><keyname>Grahn</keyname><forenames>Jessica A.</forenames></author></authors><title>Deep Feature Learning for EEG Recordings</title><categories>cs.NE cs.LG</categories><comments>submitted as conference paper for ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and compare several strategies for learning discriminative
features from electroencephalography (EEG) recordings using deep learning
techniques. EEG data are generally only available in small quantities, they are
high-dimensional with a poor signal-to-noise ratio, and there is considerable
variability between individual subjects and recording sessions. Our proposed
techniques specifically address these challenges for feature learning.
Cross-trial encoding forces auto-encoders to focus on features that are stable
across trials. Similarity-constraint encoders learn features that allow to
distinguish between classes by demanding that two trials from the same class
are more similar to each other than to trials from other classes. This
tuple-based training approach is especially suitable for small datasets.
Hydra-nets allow for separate processing pathways adapting to subsets of a
dataset and thus combine the advantages of individual feature learning (better
adaptation of early, low-level processing) with group model training (better
generalization of higher-level processing in deeper layers). This way, models
can, for instance, adapt to each subject individually to compensate for
differences in spatial patterns due to anatomical differences or variance in
electrode positions. The different techniques are evaluated using the publicly
available OpenMIIR dataset of EEG recordings taken while participants listened
to and imagined music.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04308</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04308</id><created>2015-11-11</created><authors><author><keyname>Zugal</keyname><forenames>Stefan</forenames></author><author><keyname>Pinggera</keyname><forenames>Jakob</forenames></author></authors><title>Low-Cost Eye-Trackers: Useful for Information Systems Research?</title><categories>cs.HC cs.CY</categories><journal-ref>S. Zugal and J. Pinggera: Low-Cost Eye-Trackers: Useful for
  Information Systems Research? In: Proc. Cognise'14, pp. 159-170, 2014</journal-ref><doi>10.1007/978-3-319-07869-4_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research investigating cognitive aspects of information systems is often
dependent on detail-rich data. Eye-trackers promise to provide respective data,
but the associated costs are often beyond the researchers' budget. Recently,
eye-trackers have entered the market that promise eye-tracking support at a
reasonable price. In this work, we explore whether such eye-trackers are of use
for information systems research and explore the accuracy of a low-cost
eye-tracker (Gazepoint GP3) in an empirical study. The results show that
Gazepoint GP3 is well suited for respective research, given that experimental
material acknowledges the limits of the eye-tracker. To foster replication and
comparison of results, all data, experimental material as well as the source
code developed for this study are made available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04317</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04317</id><created>2015-11-13</created><authors><author><keyname>Ahmadi</keyname><forenames>Mansour</forenames></author><author><keyname>Giacinto</keyname><forenames>Giorgio</forenames></author><author><keyname>Ulyanov</keyname><forenames>Dmitry</forenames></author><author><keyname>Semenov</keyname><forenames>Stanislav</forenames></author><author><keyname>Trofimov</keyname><forenames>Mikhail</forenames></author></authors><title>Novel feature extraction, selection and fusion for effective malware
  family classification</title><categories>cs.CR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern malware is designed with mutation characteristics, namely polymorphism
and metamorphism, which causes an enormous growth in the number of variants of
malware samples. Categorization of malware samples on the basis of their
behaviors is essential for the computer security community in order to group
samples belonging to same family. Microsoft released a malware classification
challenge in 2015 with a huge dataset of near 0.5 terabytes of data, containing
more than 20K malware samples. The analysis of this dataset inspired the
development of a novel paradigm that is effective in categorizing malware
variants into their actual family groups. This paradigm is presented and
discussed in the present paper, where emphasis has been given to the phases
related to the extraction, and selection of a set of novel features for the
effective representation of malware samples. Features can be grouped according
to different characteristics of malware behavior, and their fusion is performed
according to a per-class weighting paradigm. The proposed method achieved a
very high accuracy ($\approx$ 0.998) on the Microsoft Malware Challenge
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04320</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04320</id><created>2015-11-13</created><authors><author><keyname>Chica</keyname><forenames>Manuel</forenames></author><author><keyname>Campoy</keyname><forenames>Pascual</forenames></author></authors><title>Standard methods for inexpensive pollen loads authentication by means of
  computer vision and machine learning</title><categories>cs.CV</categories><comments>24 pages. Book chapter to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a complete methodology for authenticating local bee pollen against
fraudulent samples using image processing and machine learning techniques. The
proposed standard methods do not need expensive equipment such as advanced
microscopes and can be used for a preliminary fast rejection of unknown pollen
types. The system is able to rapidly reject the non-local pollen samples with
inexpensive hardware and without the need to send the product to the
laboratory. Methods are based on the color properties of bee pollen loads
images and the use of one-class classifiers which are appropriate to reject
unknown pollen samples when there is limited data about them. The validation of
the method is carried out by authenticating Spanish bee pollen types.
Experimentation shows that the proposed methods can obtain an overall
authentication accuracy of 94%. We finally illustrate the user interaction with
the software in some practical cases by showing the developed application
prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04326</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04326</id><created>2015-11-12</created><authors><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author></authors><title>ICON Challenge on Algorithm Selection</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the results of the ICON Challenge on Algorithm Selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04348</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04348</id><created>2015-11-13</created><authors><author><keyname>Wang</keyname><forenames>Linnan</forenames></author><author><keyname>Wu</keyname><forenames>Wei</forenames></author><author><keyname>Xiao</keyname><forenames>Jianxiong</forenames></author><author><keyname>Yi</keyname><forenames>Yang</forenames></author></authors><title>Large Scale Artificial Neural Network Training Using Multi-GPUs</title><categories>cs.DC cs.NE</categories><comments>SC 15 Poster</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a method for accelerating large scale Artificial Neural
Networks (ANN) training using multi-GPUs by reducing the forward and backward
passes to matrix multiplication. We propose an out-of-core multi-GPU matrix
multiplication and integrate the algorithm with the ANN training. The
experiments demonstrate that our matrix multiplication algorithm achieves
linear speedup on multiple inhomogeneous GPUs. The full paper of this project
can be found at [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04352</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04352</id><created>2015-11-13</created><authors><author><keyname>Riguzzi</keyname><forenames>Fabrizio</forenames></author></authors><title>Introduzione all'Intelligenza Artificiale</title><categories>cs.AI</categories><comments>27 pages, in Italian</comments><journal-ref>Fabrizio Riguzzi, Introduzione all'Intelligenza Artificiale, Terre
  di Confine, 2(1), January 2006</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper presents an introduction to Artificial Intelligence (AI) in an
accessible and informal but precise form. The paper focuses on the algorithmic
aspects of the discipline, presenting the main techniques used in AI systems
groped in symbolic and subsymbolic. The last part of the paper is devoted to
the discussion ongoing among experts in the field and the public at large about
on the advantages and disadvantages of AI and in particular on the possible
dangers. The personal opinion of the author on this subject concludes the
paper.
  -----
  L'articolo presenta un'introduzione all'Intelligenza Artificiale (IA) in
forma divulgativa e informale ma precisa. L'articolo affronta prevalentemente
gli aspetti informatici della disciplina, presentando le principali tecniche
usate nei sistemi di IA divise in simboliche e subsimboliche. L'ultima parte
dell'articolo presenta il dibattito in corso tra gli esperi e il pubblico su
vantaggi e svantaggi dell'IA e in particolare sui possibili pericoli.
L'articolo termina con l'opinione dell'autore al riguardo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04359</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04359</id><created>2015-11-12</created><authors><author><keyname>La Guardia</keyname><forenames>Giuliano Gadioli</forenames></author><author><keyname>Alves</keyname><forenames>Marcelo Muniz Silva</forenames></author></authors><title>On cyclotomic cosets and code constructions</title><categories>cs.IT math.CO math.IT</categories><comments>Accepted for publication in Linear Algebra and its Applications</comments><journal-ref>Linear Algebra and its Applications, v. 488, p. 302-319, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New properties of $q$-ary cyclotomic cosets modulo $n = q^{m} - 1$, where $q
\geq 3$ is a prime power, are investigated in this paper. Based on these
properties, the dimension as well as bounds for the designed distance of some
families of classical cyclic codes can be computed. As an application, new
families of nonbinary Calderbank-Shor-Steane (CSS) quantum codes as well as new
families of convolutional codes are constructed in this work. These new CSS
codes have parameters better than the ones available in the literature. The
convolutional codes constructed here have free distance greater than the ones
available in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.04376</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.04376</id><created>2015-11-13</created><authors><author><keyname>Mercian</keyname><forenames>Anu</forenames></author><author><keyname>McGarry</keyname><forenames>Michael P.</forenames></author><author><keyname>Reisslein</keyname><forenames>Martin</forenames></author><author><keyname>Kellerer</keyname><forenames>Wolfgang</forenames></author></authors><title>Software Defined Optical Access Networks (SDOANs): A Comprehensive
  Survey</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emerging software defined networking (SDN) paradigm separates the data
plane from the control plane and centralizes network control in an SDN
controller. SDN facilitates the virtualization of network functions so that
multiple virtual networks can operate over a given installed physical network.
Optical network segments, such as passive optical networks (PONs), have become
important for providing network access. In this article, we comprehensively
survey studies that examine the concepts of SDN and network virtualization in
optical access networks; in brief, we survey the area of software defined
optical access networks (SDOANs). We organize the SDOAN studies into
architecture and protocol focused studies. We then subclassify the studies
according to the network layers. In particular, for protocol-focused studies,
we consider subclassifications for the physical layer, medium access control
(link) layer, and network layer. In addition, we consider subclassifications
for studies focused on quality of service, multilayer (multitechnology)
networking, and virtualization. Based on the survey, we identify open
challenges for SDOANs and outline future directions.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="86000" completeListSize="102538">1122234|87001</resumptionToken>
</ListRecords>
</OAI-PMH>
